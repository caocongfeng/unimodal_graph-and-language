{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "import random\n",
    "\n",
    "path = osp.dirname(osp.abspath(''))\n",
    "sys.path.append(path)\n",
    "sys.path.append(osp.join(path, \"./open_biomed\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text(path=None):\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        text_list = []\n",
    "        for index, line in enumerate(f):\n",
    "            text = line.strip()\n",
    "            text_list.append(text)\n",
    "    return text_list\n",
    "path='./text_after_openai_chemistry_extracted_cleaned.txt'\n",
    "extracted_text=read_text(path=path)\n",
    "path='./text_after_structure_filter.txt'\n",
    "texts=read_text(path=path)\n",
    "\n",
    "# path='./smis_after_structure_filter.txt'\n",
    "# smis=read_text(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_isomers(path=''):\n",
    "    sim_list=[]\n",
    "    with open(path, 'r') as f:\n",
    "        for index, line in enumerate(f):\n",
    "\n",
    "            isomers_list = line.strip().split(\"\\t\")\n",
    "            sim_list.append(isomers_list[0])\n",
    "    return sim_list\n",
    "structure_isomers=structure_isomers(path='./structure_ismers.txt')\n",
    "\n",
    "smis=structure_isomers\n",
    "print((len(texts),len(smis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(texts),len(smis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性核函数，用 PyTorch 实现\n",
    "def linear_kernel(X):\n",
    "    return torch.mm(X, X.T)  # 使用矩阵乘法\n",
    "\n",
    "# 居中核矩阵\n",
    "def center_kernel(K):\n",
    "    n = K.shape[0]\n",
    "    H = torch.eye(n, device=K.device) - torch.ones((n, n), device=K.device) / n  # 中心化矩阵 H\n",
    "    return torch.mm(H, torch.mm(K, H))\n",
    "\n",
    "# CKA 实现，计算两个张量 X 和 Y 的 CKA\n",
    "def CKA(X, Y):\n",
    "    # 计算线性核矩阵\n",
    "    K = linear_kernel(X)\n",
    "    L = linear_kernel(Y)\n",
    "\n",
    "    # 居中核矩阵\n",
    "    Kc = center_kernel(K)\n",
    "    Lc = center_kernel(L)\n",
    "\n",
    "    # 计算 HSIC 值\n",
    "    hsic_XY = torch.trace(torch.mm(Kc, Lc))\n",
    "    hsic_XX = torch.trace(torch.mm(Kc, Kc))\n",
    "    hsic_YY = torch.trace(torch.mm(Lc, Lc))\n",
    "\n",
    "    # 计算 CKA 值\n",
    "    cka_value = hsic_XY / torch.sqrt(hsic_XX * hsic_YY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return round(cka_value.item(),3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import cka,gram_linear\n",
    "# structure_feats_np = structure_feats[indes].cpu().detach().numpy()\n",
    "# text_feats_np = text_feats[indes].cpu().detach().numpy()\n",
    "# # Calculate Gram matrices for structure_feats and text_feats\n",
    "def de_biased_cka(structure_feats,text_feats):\n",
    "    structure_feats_np = structure_feats.cpu().detach().numpy()\n",
    "    text_feats_np = text_feats.cpu().detach().numpy()\n",
    "    gram_structure = gram_linear(structure_feats_np)\n",
    "    gram_text = gram_linear(text_feats_np)\n",
    "\n",
    "    # Calculate CKA between the two Gram matrices\n",
    "    cka_value = cka(gram_structure, gram_text, debiased=True)\n",
    "\n",
    "    return round(cka_value,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def retrieve(A, B, K=1):\n",
    "    \"\"\"\n",
    "    检索任务函数，输入两个 tensor A 和 B，返回 A 和 B 匹配的索引值、匹配的分数和正确匹配的条目数。\n",
    "\n",
    "    参数：\n",
    "    - A: torch.Tensor，待匹配的 query tensor，形状 (n, d)。\n",
    "    - B: torch.Tensor，待匹配的 target tensor，形状 (n, d)。\n",
    "    - K: int，检索 top K 匹配。\n",
    "\n",
    "    返回：\n",
    "    - indices: 每个 A 对应的最相似 B 的索引值，形状 (n, K)。\n",
    "    - scores: 每个 A 对应的最相似 B 的分数，形状 (n, K)。\n",
    "    - correct_total: 匹配正确的总条目数。\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算 A 和 B 之间的余弦相似度 (n, n)\n",
    "    similarities = F.cosine_similarity(A.unsqueeze(1), B.unsqueeze(0), dim=-1)\n",
    "    # 获取相似度最高的前 K 个条目的索引和分数\n",
    "    topk_scores, topk_indices = torch.topk(similarities, K, dim=1)\n",
    "    # 计算总的正确匹配数量\n",
    "    # 正确的定义：A[i] 和 B[i] 相匹配，意味着第 i 个 A 应该匹配第 i 个 B\n",
    "    correct_total = 0\n",
    "    n = A.size(0)\n",
    "    for i in range(n):\n",
    "        # 检查 top K 的索引是否包含正确匹配\n",
    "        if i in topk_indices[i]:\n",
    "            correct_total += 1\n",
    "    return topk_indices, topk_scores, correct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overleap(indexA,indexB ):\n",
    "    C=[]\n",
    "    D=[]\n",
    "    for a, b in zip(indexA, indexB):\n",
    "        overlap = list(set(a) & set(b))\n",
    "        C.append(overlap)\n",
    "        D.append([len(overlap)])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Overlap C:\", C)\n",
    "    print(\"Number of overlaps D:\", D)\n",
    "\n",
    "    sum=0\n",
    "    for i in C:\n",
    "        sum=sum+len(i)\n",
    "    print(sum)\n",
    "    jaccard_similarity=0\n",
    "    for a, b in zip(indexA, indexB):\n",
    "        jaccard_similarity = jaccard_similarity+len(set(a) & set(b)) / len(set(a) | set(b))\n",
    "\n",
    "    print(\"Mean Jaccard Similarity:\", round(jaccard_similarity/len(indexA),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def KM(embedding=None,num_clusters=5):\n",
    "    num_clusters = num_clusters  # You can set this based on your requirement\n",
    "\n",
    "    # Initialize and fit the KMeans model\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def NN(embedding=None,num=10):\n",
    "    # Initialize the Nearest Neighbors model\n",
    "    num_neighbors = num  # Number of nearest neighbors to find\n",
    "    nn_model = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "\n",
    "    # Fit the model to your data\n",
    "    try:\n",
    "        nn_model.fit(embedding.cpu().numpy())\n",
    "    except:\n",
    "        nn_model.fit(embedding.detach().cpu().numpy())\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(smis),len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoLFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_embeddings=torch.load('./savedpt/Structure_sim_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_embeddings.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/lightonai/pylate.git\n",
    "# !pip install git+https://github.com/UKPLab/sentence-transformers.git\n",
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Define model ID\n",
    "model_id = \"answerdotai/ModernBERT-base\" \n",
    "\n",
    "# Load tokenizer and model, move model to GPU if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize input texts with padding for batch processing\n",
    "inputs = tokenizer(texts,padding=True,truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden states\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the last hidden states\n",
    "print(\"Last hidden states shape:\", last_hidden_states.shape)\n",
    "\n",
    "# 取 [CLS] token 的 embedding（句子嵌入）\n",
    "cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "# 也可以取所有 token 的平均值作为句子嵌入\n",
    "mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "print(cls_embedding.shape,mean_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input texts with padding for batch processing\n",
    "extracted_inputs = tokenizer(extracted_text, padding=True,truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "extracted_inputs = {key: value.to(device) for key, value in extracted_inputs.items()}\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    extracted_outputs = model(**extracted_inputs)\n",
    "\n",
    "# Extract the last hidden states\n",
    "extracted_last_hidden_states = extracted_outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the last hidden states\n",
    "print(\"Last hidden states shape:\", extracted_last_hidden_states.shape)\n",
    "\n",
    "# 取 [CLS] token 的 embedding（句子嵌入）\n",
    "extracted_cls_embedding = extracted_last_hidden_states[:, 0, :]\n",
    "\n",
    "# 也可以取所有 token 的平均值作为句子嵌入\n",
    "extracted_mean_embedding = torch.mean(extracted_last_hidden_states, dim=1)\n",
    "\n",
    "print(f\"CLS token embedding size: {extracted_cls_embedding.shape}\")\n",
    "print(f\"Mean token embedding size: {extracted_mean_embedding.shape}\")\n",
    "\n",
    "print(extracted_cls_embedding.shape,extracted_mean_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_embedding=cls_embedding\n",
    "SCIB_mean_embedding=mean_embedding\n",
    "SCIB_extracted_text_mean_embedding=extracted_mean_embedding\n",
    "SCIB_extracted_text_cls_embedding=extracted_cls_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_mean_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_extracted_text_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_extracted_text_mean_embedding,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(SCIB_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_mean_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_extracted_text_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_extracted_text_mean_embedding.cpu(),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have cls mean embedding of texts and extracted_text\n",
    "cls_embedding,\n",
    "\n",
    "mean_embedding,\n",
    "\n",
    "extracted_text_cls_embedding,\n",
    "\n",
    "extracted_text_mean_embedding,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_cls_NN=NN(embedding=SCIB_cls_embedding)\n",
    "SCIB_texts_mean_NN=NN(embedding=SCIB_mean_embedding)\n",
    "SCIB_extracted_text_cls_NN=NN(embedding=SCIB_extracted_text_cls_embedding)\n",
    "SCIB_extracted_text_mean_NN=NN(embedding=SCIB_extracted_text_mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_cls_NN_distance,SCIB_texts_cls_NN_indes=SCIB_texts_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_texts_mean_NN_distance,SCIB_texts_mean_NN_indes=SCIB_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_extracted_text_cls_NN_distance,SCIB_extracted_text_cls_NN_indes=SCIB_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_extracted_text_mean_NN_distance,SCIB_extracted_text_mean_NN_indes=SCIB_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_match_indices, SCIB_texts_match_scores, SCIB_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_cls_embedding.cpu().detach(), K=10)\n",
    "SCIB_texts_mean_match_indices, SCIB_texts_mean_match_scores, SCIB_texts_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_mean_match_correct_total)\n",
    "SCIB_extracted_text_match_indices, SCIB_extracted_text_match_scores, SCIB_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_extracted_text_cls_embedding.cpu().detach(), K=10)\n",
    "SCIB_extracted_text_mean_match_indices, SCIB_extracted_text_mean_match_scores, SCIB_extracted_text_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_extracted_text_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_k1_match_indices, SCIB_texts_k1_match_scores, SCIB_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_cls_embedding.cpu().detach(), K=1)\n",
    "SCIB_texts_K1_mean_match_indices, SCIB_texts_k1_mean_match_scores, SCIB_texts_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_k1_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_k1_mean_match_correct_total)\n",
    "SCIB_extracted_k1_text_match_indices, SCIB_extracted_k1_text_match_scores, SCIB_extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_extracted_text_cls_embedding.cpu().detach(), K=1)\n",
    "SCIB_extracted_k1_text_mean_match_indices, SCIB_extracted_k1_text_mean_match_scores, SCIB_extracted_text_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_extracted_text_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_k1_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_k1_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIB_texts_cls_NN_indes)\n",
    "\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIB_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIB_texts_mean_NN_indes)\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIB_texts_cls_NN_indes)\n",
    "print('='*20)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIB_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIB_extracted_text_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIB_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIB_extracted_text_mean_NN_indes)\n",
    "# 8562\n",
    "# Mean Jaccard Similarity: 0.146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_texts_cls_NN_distance,SCIBSMIS_texts_cls_NN_indes=SCIBSMIS_texts_cls_NN.kneighbors(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIBSMIS_texts_mean_NN_distance,SCIBSMIS_texts_mean_NN_indes=SCIBSMIS_texts_mean_NN.kneighbors(SCIB_mean_embedding.cpu().numpy())\n",
    "\n",
    "SCIBSMIS_extracted_text_cls_NN_distance,SCIBSMIS_extracted_text_cls_NN_indes=SCIBSMIS_extracted_text_cls_NN.kneighbors(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIBSMIS_extracted_text_mean_NN_distance,SCIBSMIS_extracted_text_mean_NN_indes=SCIBSMIS_extracted_text_mean_NN.kneighbors(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIBSMIS_texts_cls_NN_indes)\n",
    "\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIBSMIS_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIBSMIS_texts_mean_NN_indes)\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIBSMIS_texts_cls_NN_indes)\n",
    "print('='*20)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_mean_NN_indes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_KM=KM(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIB_mean_KM=KM(SCIB_mean_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_cls_KM=KM(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_mean_KM=KM(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_KM_index=SCIB_cls_KM.predict(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIB_mean_KM_index=SCIB_mean_KM.predict(SCIB_mean_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_cls_KM_index=SCIB_extracted_text_cls_KM.predict(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_mean_KM_index=SCIB_extracted_text_mean_KM.predict(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_match_index=SCIB_cls_KM.predict(SCIB_cls_embedding[SCIB_texts_k1_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_mean_match_index=SCIB_mean_KM.predict(SCIB_mean_embedding[SCIB_texts_K1_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_et_cls_match_index=SCIB_extracted_text_cls_KM.predict(SCIB_extracted_text_cls_embedding[SCIB_extracted_k1_text_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_et_mean_match_index=SCIB_extracted_text_mean_KM.predict(SCIB_extracted_text_mean_embedding[SCIB_extracted_k1_text_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_x_cls=SCIB_cls_match_index==SCIB_cls_KM_index\n",
    "SCIB_x_mean=SCIB_mean_KM_index==SCIB_mean_match_index\n",
    "SCIB_x_et_cls=SCIB_extracted_text_cls_KM_index==SCIB_et_cls_match_index\n",
    "SCIB_x_et_mean=SCIB_et_mean_match_index==SCIB_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(SCIB_x_cls),np.sum(SCIB_x_mean),np.sum(SCIB_x_et_cls),np.sum(SCIB_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smis K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_KM_index=SCIBSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_mean_KM_index=SCIBSMIS_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_cls_KM_index=SCIBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_mean_KM_index=SCIBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_match_index=SCIBSMIS_cls_KM.predict(sim_embeddings.pooler_output[SCIB_texts_k1_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_mean_match_index=SCIBSMIS_mean_KM.predict(sim_embeddings.pooler_output[SCIB_texts_K1_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_et_cls_match_index=SCIBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[SCIB_extracted_k1_text_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_et_mean_match_index=SCIBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output[SCIB_extracted_k1_text_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_x_cls=SCIBSMIS_cls_match_index==SCIBSMIS_cls_KM_index\n",
    "SCIBSMIS_x_mean=SCIBSMIS_mean_KM_index==SCIBSMIS_mean_match_index\n",
    "SCIBSMIS_x_et_cls=SCIBSMIS_extracted_text_cls_KM_index==SCIBSMIS_et_cls_match_index\n",
    "SCIBSMIS_x_et_mean=SCIBSMIS_et_mean_match_index==SCIBSMIS_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(SCIBSMIS_x_cls),np.sum(SCIBSMIS_x_mean),np.sum(SCIBSMIS_x_et_cls),np.sum(SCIBSMIS_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MolCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_embeddings=torch.load( './savedpt/ismers_gcn_embeddings.pth')\n",
    "gcn_out_embeddings=torch.load( './savedpt/ismers_gcn_out_embeddings.pth')\n",
    "gin_embeddings=torch.load( './savedpt/ismers_gin_embeddings.pth')\n",
    "gin_out_embeddings=torch.load( './savedpt/ismers_gin_out_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(de_biased_cka(SCIB_cls_embedding, gcn_embeddings),de_biased_cka(SCIB_cls_embedding, gcn_out_embeddings),de_biased_cka(SCIB_cls_embedding,gin_embeddings),de_biased_cka(SCIB_cls_embedding,gin_out_embeddings))\n",
    "print(de_biased_cka(SCIB_mean_embedding,gcn_embeddings),de_biased_cka(SCIB_mean_embedding,gcn_out_embeddings),de_biased_cka(SCIB_mean_embedding,gin_embeddings),de_biased_cka(SCIB_mean_embedding,gin_out_embeddings))\n",
    "print(de_biased_cka(SCIB_extracted_text_cls_embedding, gcn_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding,gcn_out_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding,gin_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding,gin_out_embeddings))\n",
    "print(de_biased_cka(SCIB_extracted_text_mean_embedding, gcn_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding,gcn_out_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding,gin_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding,gin_out_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(SCIB_cls_embedding, gcn_embeddings.to(device)),CKA(SCIB_cls_embedding, gcn_out_embeddings.to(device)),CKA(SCIB_cls_embedding,gin_embeddings.to(device)),CKA(SCIB_cls_embedding,gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_mean_embedding,gcn_embeddings.to(device)),CKA(SCIB_mean_embedding,gcn_out_embeddings.to(device)),CKA(SCIB_mean_embedding,gin_embeddings.to(device)),CKA(SCIB_mean_embedding,gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_extracted_text_cls_embedding, gcn_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding,gcn_out_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding,gin_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding,gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_extracted_text_mean_embedding, gcn_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding,gcn_out_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding,gin_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding,gin_out_embeddings.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenBioMed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
