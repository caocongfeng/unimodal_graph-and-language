{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "import random\n",
    "\n",
    "path = osp.dirname(osp.abspath(''))\n",
    "sys.path.append(path)\n",
    "sys.path.append(osp.join(path, \"./../open_biomed\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text(path=None):\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        text_list = []\n",
    "        for index, line in enumerate(f):\n",
    "            text = line.strip()\n",
    "            text_list.append(text)\n",
    "    return text_list\n",
    "path='./extracted_chemistry_keywords.txt'\n",
    "extracted_text=read_text(path=path)\n",
    "path='./text_after_structure_filter.txt'\n",
    "texts=read_text(path=path)\n",
    "\n",
    "path='./smis_after_structure_filter.txt'\n",
    "smis=read_text(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(texts),len(smis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性核函数，用 PyTorch 实现\n",
    "def linear_kernel(X):\n",
    "    return torch.mm(X, X.T)  # 使用矩阵乘法\n",
    "\n",
    "# 居中核矩阵\n",
    "def center_kernel(K):\n",
    "    n = K.shape[0]\n",
    "    H = torch.eye(n, device=K.device) - torch.ones((n, n), device=K.device) / n  # 中心化矩阵 H\n",
    "    return torch.mm(H, torch.mm(K, H))\n",
    "\n",
    "# CKA 实现，计算两个张量 X 和 Y 的 CKA\n",
    "def CKA(X, Y):\n",
    "    # 计算线性核矩阵\n",
    "    K = linear_kernel(X)\n",
    "    L = linear_kernel(Y)\n",
    "\n",
    "    # 居中核矩阵\n",
    "    Kc = center_kernel(K)\n",
    "    Lc = center_kernel(L)\n",
    "\n",
    "    # 计算 HSIC 值\n",
    "    hsic_XY = torch.trace(torch.mm(Kc, Lc))\n",
    "    hsic_XX = torch.trace(torch.mm(Kc, Kc))\n",
    "    hsic_YY = torch.trace(torch.mm(Lc, Lc))\n",
    "\n",
    "    # 计算 CKA 值\n",
    "    cka_value = hsic_XY / torch.sqrt(hsic_XX * hsic_YY)\n",
    "    return cka_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import cka,gram_linear\n",
    "# structure_feats_np = structure_feats[indes].cpu().detach().numpy()\n",
    "# text_feats_np = text_feats[indes].cpu().detach().numpy()\n",
    "# # Calculate Gram matrices for structure_feats and text_feats\n",
    "def de_biased_cka(structure_feats,text_feats):\n",
    "    structure_feats_np = structure_feats.cpu().detach().numpy()\n",
    "    text_feats_np = text_feats.cpu().detach().numpy()\n",
    "    gram_structure = gram_linear(structure_feats_np)\n",
    "    gram_text = gram_linear(text_feats_np)\n",
    "\n",
    "    # Calculate CKA between the two Gram matrices\n",
    "    cka_value = cka(gram_structure, gram_text, debiased=True)\n",
    "\n",
    "    return cka_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def retrieve(A, B, K=1):\n",
    "    \"\"\"\n",
    "    检索任务函数，输入两个 tensor A 和 B，返回 A 和 B 匹配的索引值、匹配的分数和正确匹配的条目数。\n",
    "\n",
    "    参数：\n",
    "    - A: torch.Tensor，待匹配的 query tensor，形状 (n, d)。\n",
    "    - B: torch.Tensor，待匹配的 target tensor，形状 (n, d)。\n",
    "    - K: int，检索 top K 匹配。\n",
    "\n",
    "    返回：\n",
    "    - indices: 每个 A 对应的最相似 B 的索引值，形状 (n, K)。\n",
    "    - scores: 每个 A 对应的最相似 B 的分数，形状 (n, K)。\n",
    "    - correct_total: 匹配正确的总条目数。\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算 A 和 B 之间的余弦相似度 (n, n)\n",
    "    similarities = F.cosine_similarity(A.unsqueeze(1), B.unsqueeze(0), dim=-1)\n",
    "    # 获取相似度最高的前 K 个条目的索引和分数\n",
    "    topk_scores, topk_indices = torch.topk(similarities, K, dim=1)\n",
    "    # 计算总的正确匹配数量\n",
    "    # 正确的定义：A[i] 和 B[i] 相匹配，意味着第 i 个 A 应该匹配第 i 个 B\n",
    "    correct_total = 0\n",
    "    n = A.size(0)\n",
    "    for i in range(n):\n",
    "        # 检查 top K 的索引是否包含正确匹配\n",
    "        if i in topk_indices[i]:\n",
    "            correct_total += 1\n",
    "    return topk_indices, topk_scores, correct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overleap(indexA,indexB ):\n",
    "    C=[]\n",
    "    D=[]\n",
    "    for a, b in zip(indexA, indexB):\n",
    "        overlap = list(set(a) & set(b))\n",
    "        C.append(overlap)\n",
    "        D.append([len(overlap)])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Overlap C:\", C)\n",
    "    print(\"Number of overlaps D:\", D)\n",
    "\n",
    "    sum=0\n",
    "    for i in C:\n",
    "        sum=sum+len(i)\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def KM(embedding=None,num_clusters=5):\n",
    "    num_clusters = num_clusters  # You can set this based on your requirement\n",
    "\n",
    "    # Initialize and fit the KMeans model\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def NN(embedding=None,num=10):\n",
    "    # Initialize the Nearest Neighbors model\n",
    "    num_neighbors = num  # Number of nearest neighbors to find\n",
    "    nn_model = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "\n",
    "    # Fit the model to your data\n",
    "    try:\n",
    "        nn_model.fit(embedding.cpu().numpy())\n",
    "    except:\n",
    "        nn_model.fit(embedding.detach().cpu().numpy())\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(smis),len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MolCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_embeddings=torch.load('gcn_embeddings.pt')\n",
    "# gcn_out_embeddings=torch.load('gcn_out_embeddings.pt')\n",
    "# gin_embeddings=torch.load('gin_embeddings.pt')\n",
    "# gin_out_embeddings=torch.load('gin_out_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_embeddings.shape,gcn_out_embeddings.shape,gin_embeddings.shape,gin_out_embeddings.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from MolCLR.models.ginet_molclr import GINet\n",
    "from MolCLR.models.gcn_molclr import GCN\n",
    "\n",
    "# Convert a SMILES string into a graph representation\n",
    "def smiles_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    Chem.SanitizeMol(mol)\n",
    "    \n",
    "    # Extract atom features and edge features\n",
    "    atom_features = torch.tensor([[atom.GetAtomicNum(), atom.GetChiralTag()] for atom in mol.GetAtoms()], dtype=torch.long)\n",
    "    bond_features = torch.tensor([[bond.GetBondTypeAsDouble(), bond.GetStereo()] for bond in mol.GetBonds()], dtype=torch.long)\n",
    "    \n",
    "    # Build edge_index (source, target) pairs for the molecular graph\n",
    "    edge_index = torch.tensor([[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()] for bond in mol.GetBonds()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create a batch (for a single molecule, the batch index will be all 0s)\n",
    "    data = Data(x=atom_features, edge_index=edge_index, edge_attr=bond_features, batch=torch.tensor([0] * atom_features.size(0)))\n",
    "    return data\n",
    "\n",
    "# Load the GINet model based on the configuration from config.yaml\n",
    "def load_model(path='',type='gin'):\n",
    "    # Initialize the GINet model with the exact settings from config.yaml\n",
    "    if type=='gin':\n",
    "        model = GINet(num_layer=5, emb_dim=300, feat_dim=512, drop_ratio=0, pool='mean')\n",
    "    elif type=='gcn':\n",
    "        model=GCN(num_layer=5, emb_dim=300, feat_dim=512, drop_ratio=0, pool='mean')\n",
    "    \n",
    "    # Optionally load pre-trained weights from the \"pretrained_gin\" model\n",
    "    pretrained_model_path = path  # Ensure this path exists and is correct\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(pretrained_model_path,map_location='cuda:0'))\n",
    "        print(\"Loaded pre-trained model from:\", pretrained_model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained weights not found, training from scratch.\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "\n",
    "gin_model = load_model(path='./../MolCLR/ckpt/pretrained_gzin/checkpoints/model.pth',type='gin')\n",
    "gcn_model = load_model(path='./../MolCLR/ckpt/pretrained_gcn/checkpoints/model.pth',type='gcn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_h_list=[]\n",
    "gcn_out_list=[]\n",
    "gin_h_list=[]\n",
    "gin_out_list=[]\n",
    "text_list=[]\n",
    "et_list=[]\n",
    "pass_list=[]\n",
    "compute_list=[]\n",
    "for index,i in enumerate(smis) :\n",
    "    # print(index)\n",
    "    # print(i)\n",
    "    graph_data = smiles_to_graph(i)\n",
    "    # print(graph_data)\n",
    "    try:\n",
    "        gin_h, gin_out = gin_model(graph_data)\n",
    "        gin_h_list.append(gin_h)\n",
    "        gin_out_list.append(gin_out)\n",
    "        gcn_h, gcn_out = gcn_model(graph_data)\n",
    "        gcn_h_list.append(gcn_h)\n",
    "        gcn_out_list.append(gcn_out)\n",
    "        text_list.append(texts[index])\n",
    "        et_list.append(extracted_text[index])\n",
    "        compute_list.append(index)\n",
    "    except IndexError:\n",
    "        # print(\"PASS:\",index)\n",
    "        pass_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(compute_list),len(pass_list),len(gin_h_list),len(gin_out_list),len(gcn_h_list),len(gcn_out_list),len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('compute_list.txt','w') as f:\n",
    "#     for i in compute_list:\n",
    "#         f.writelines(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pass_list.txt','w') as f:\n",
    "#     for i in pass_list:\n",
    "#         f.writelines(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_embeddings = torch.cat(gcn_h_list, dim=0)\n",
    "gcn_out_embeddings= torch.cat(gcn_out_list, dim=0)\n",
    "gin_embeddings = torch.cat(gin_h_list, dim=0)\n",
    "gin_out_embeddings= torch.cat(gin_out_list, dim=0)\n",
    "gcn_embeddings.shape,gin_embeddings.shape,gcn_out_embeddings.shape,gin_out_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "def SCIBERT(texts=texts):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCIB_cls_embedding,SCIB_mean_embedding=SCIBERT(text_list)\n",
    "SCIB_cls_embedding1,SCIB_mean_embedding1=SCIBERT(text_list[:1000])\n",
    "SCIB_cls_embedding2,SCIB_mean_embedding2=SCIBERT(text_list[1000:2000])\n",
    "SCIB_cls_embedding3,SCIB_mean_embedding3=SCIBERT(text_list[2000:3000])\n",
    "SCIB_cls_embedding4,SCIB_mean_embedding4=SCIBERT(text_list[3000:])\n",
    "\n",
    "SCIB_cls_embedding=torch.cat((SCIB_cls_embedding1,SCIB_cls_embedding2,SCIB_cls_embedding3,SCIB_cls_embedding4))\n",
    "SCIB_mean_embedding=torch.cat((SCIB_mean_embedding1,SCIB_mean_embedding2,SCIB_mean_embedding3,SCIB_mean_embedding4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_embedding.shape,SCIB_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCIB_extracted_text_cls_embedding,SCIB_extracted_text_mean_embedding=SCIBERT(et_list)\n",
    "SCIB_extracted_text_cls_embedding1,SCIB_extracted_text_mean_embedding1=SCIBERT(et_list[:1000])\n",
    "SCIB_extracted_text_cls_embedding2,SCIB_extracted_text_mean_embedding2=SCIBERT(et_list[1000:2000])\n",
    "SCIB_extracted_text_cls_embedding3,SCIB_extracted_text_mean_embedding3=SCIBERT(et_list[2000:3000])\n",
    "SCIB_extracted_text_cls_embedding4,SCIB_extracted_text_mean_embedding4=SCIBERT(et_list[3000:])\n",
    "\n",
    "SCIB_extracted_text_cls_embedding=torch.cat((SCIB_extracted_text_cls_embedding1,SCIB_extracted_text_cls_embedding2,SCIB_extracted_text_cls_embedding3,SCIB_extracted_text_cls_embedding4))\n",
    "SCIB_extracted_text_mean_embedding=torch.cat((SCIB_extracted_text_mean_embedding1,SCIB_extracted_text_mean_embedding2,SCIB_extracted_text_mean_embedding3,SCIB_extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(SCIB_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(SCIB_cls_embedding[compute_list],gin_embeddings),de_biased_cka(SCIB_cls_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.22608945, 0.21755558, 0.19078067, 0.21649921)\n",
    "# (0.22608945, 0.21755558, 0.22363263, 0.22730517)\n",
    "# (0.22608945, 0.21755558, 0.22283247, 0.21325558)\n",
    "# (0.22608945, 0.21755558, 0.1987281, 0.19862846)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_mean_embedding[compute_list],gcn_embeddings),de_biased_cka(SCIB_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(SCIB_mean_embedding[compute_list],gin_embeddings),de_biased_cka(SCIB_mean_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.24103628, 0.24200632, 0.18887858, 0.2180085)\n",
    "# (0.24103628, 0.24200632, 0.22267848, 0.22766191)\n",
    "# (0.24103628, 0.24200632, 0.22232172, 0.21107413)\n",
    "# (0.24103628, 0.24200632, 0.19761267, 0.19851111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(SCIB_extracted_text_cls_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.2045951, 0.20098218, 0.18605205, 0.18484423)\n",
    "# (0.2045951, 0.20098218, 0.17631763, 0.18533646)\n",
    "# (0.2045951, 0.20098218, 0.17070535, 0.17068122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(SCIB_extracted_text_mean_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.2992024, 0.29717383, 0.2547181, 0.25347224)\n",
    "# (0.2992024, 0.29717383, 0.24284594, 0.2569317)\n",
    "# (0.2992024, 0.29717383, 0.2356572, 0.23578407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(SCIB_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(SCIB_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(SCIB_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(SCIB_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(SCIB_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(SCIB_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(SCIB_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(SCIB_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(SCIB_extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(SCIB_extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "\n",
    "# tensor(0.2303, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2236, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2118, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2206, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.2463, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2491, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2126, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2251, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.2112, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2102, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1844, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1938, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.3025, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.3028, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2488, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2630, device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del SCIB_cls_embedding1,\n",
    "del SCIB_mean_embedding1,\n",
    "del SCIB_cls_embedding2,\n",
    "del SCIB_mean_embedding2,\n",
    "del SCIB_cls_embedding3,\n",
    "del SCIB_mean_embedding3,\n",
    "del SCIB_cls_embedding4,\n",
    "del SCIB_mean_embedding4,\n",
    "del SCIB_cls_embedding,\n",
    "del SCIB_mean_embedding,\n",
    "del SCIB_extracted_text_cls_embedding1,\n",
    "del SCIB_extracted_text_mean_embedding1,\n",
    "del SCIB_extracted_text_cls_embedding2,\n",
    "del SCIB_extracted_text_mean_embedding2,\n",
    "del SCIB_extracted_text_cls_embedding3,\n",
    "del SCIB_extracted_text_mean_embedding3,\n",
    "del SCIB_extracted_text_cls_embedding4,\n",
    "del SCIB_extracted_text_mean_embedding4,\n",
    "del SCIB_extracted_text_cls_embedding,\n",
    "del SCIB_extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\").to(device)\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "SB_embeddings = model.encode(text_list)\n",
    "print('embeddings',SB_embeddings.shape)\n",
    "\n",
    "SB_extracted_embeddings = model.encode(et_list)\n",
    "print('extracted_embeddings',SB_extracted_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(torch.from_numpy(SB_embeddings)[compute_list],gcn_embeddings),de_biased_cka(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_embeddings),de_biased_cka(torch.from_numpy(SB_embeddings)[compute_list],gcn_out_embeddings),de_biased_cka(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_out_embeddings)\n",
    "# (0.2921439, 0.26299226, 0.28667632, 0.25718084)\n",
    "# (0.2921439, 0.26299226, 0.28667632, 0.25718084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(torch.from_numpy(SB_embeddings)[compute_list],gin_embeddings),de_biased_cka(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_embeddings),de_biased_cka(torch.from_numpy(SB_embeddings)[compute_list],gin_out_embeddings),de_biased_cka(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_out_embeddings)\n",
    "# (0.25000453, 0.23379803, 0.2507004, 0.23436297)\n",
    "# (0.26438615, 0.2464931, 0.28694633, 0.26617283)\n",
    "# (0.2528803, 0.23645988, 0.25703993, 0.23996557)\n",
    "# (0.26420167, 0.24588796, 0.2621179, 0.24466914)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(torch.from_numpy(SB_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(SB_embeddings)[compute_list],gcn_out_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_out_embeddings))\n",
    "print(CKA(torch.from_numpy(SB_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(SB_embeddings)[compute_list],gin_out_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_out_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(torch.from_numpy(SB_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(SB_embeddings)[compute_list],gcn_out_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gcn_out_embeddings))\n",
    "print(CKA(torch.from_numpy(SB_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(SB_embeddings)[compute_list],gin_out_embeddings),CKA(torch.from_numpy(SB_extracted_embeddings)[compute_list],gin_out_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "# GCN\n",
    "SB_gcn_texts_match_indices, SB_texts_match_scores, SB_texts_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(SB_embeddings).cpu().detach()[compute_list], K=10)\n",
    "SB_gcn_extracted_match_indices, SB_extracted_match_scores, SB_extracted_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(SB_extracted_embeddings).cpu().detach()[compute_list], K=10)\n",
    "\n",
    "print(SB_texts_match_correct_total,SB_extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "# GIN\n",
    "SB_gin_texts_match_indices, SB_texts_match_scores, SB_texts_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(SB_embeddings).cpu().detach()[compute_list], K=10)\n",
    "SB_gin_extracted_match_indices, SB_extracted_match_scores, SB_extracted_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(SB_extracted_embeddings).cpu().detach()[compute_list], K=10)\n",
    "\n",
    "print(SB_texts_match_correct_total,SB_extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "# GCN\n",
    "SB_gcn_texts_k1_match_indices, SB_texts_k1_match_scores, SB_texts_k1_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(SB_embeddings).cpu().detach()[compute_list], K=K)\n",
    "SB_gcn_extracte_k1_match_indices, SB_extracted_k1_match_scores, SB_extracted_k1_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(SB_extracted_embeddings).cpu().detach()[compute_list], K=K)\n",
    "\n",
    "print(SB_texts_k1_match_correct_total,SB_extracted_k1_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "# GIN\n",
    "SB_gin_texts_k1_match_indices, SB_texts_match_scores, SB_texts_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(SB_embeddings).cpu().detach()[compute_list], K=K)\n",
    "SB_gin_extracted_k1_match_indices, SB_extracted_match_scores, SB_extracted_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(SB_extracted_embeddings).cpu().detach()[compute_list], K=K)\n",
    "\n",
    "print(SB_texts_match_correct_total,SB_extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_texts_mean_NN=NN(embedding=torch.from_numpy(SB_embeddings)[compute_list])\n",
    "SB_extracted_text_mean_NN=NN(embedding=torch.from_numpy(SB_extracted_embeddings)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_emb_distance,SB_gcn_emb_index=SB_texts_mean_NN.kneighbors(SB_embeddings[compute_list])\n",
    "SB_et_distance,SB_gcn_et_index=SB_extracted_text_mean_NN.kneighbors(SB_extracted_embeddings[compute_list])\n",
    "\n",
    "SB_emb_distance,SB_gin_emb_index=SB_texts_mean_NN.kneighbors(SB_embeddings[compute_list])\n",
    "SB_et_distance,SB_gin_et_index=SB_extracted_text_mean_NN.kneighbors(SB_extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SB_gcn_texts_match_indices.cpu().numpy(), SB_gcn_emb_index)\n",
    "\n",
    "overleap(SB_gcn_extracted_match_indices.cpu().numpy(), SB_gcn_et_index)\n",
    "\n",
    "overleap(SB_gin_texts_match_indices.cpu().numpy(), SB_gin_emb_index)\n",
    "\n",
    "overleap(SB_gin_extracted_match_indices.cpu().numpy(), SB_gin_et_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_GCN_NN=NN(embedding=gcn_embeddings[compute_list])\n",
    "SB_GIN_NN=NN(embedding=gin_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_emb_distance,SB_gcn_emb_index=SB_GCN_NN.kneighbors(gcn_embeddings.detach().numpy()[compute_list])\n",
    "SB_et_distance,SB_gcn_et_index=SB_GCN_NN.kneighbors(gcn_embeddings.detach().numpy()[compute_list])\n",
    "\n",
    "SB_emb_distance,SB_gin_emb_index=SB_GIN_NN.kneighbors(gin_embeddings.detach().numpy()[compute_list])\n",
    "SB_et_distance,SB_gin_et_index=SB_GIN_NN.kneighbors(gin_embeddings.detach().numpy()[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SB_gcn_texts_match_indices.cpu().numpy(), SB_gcn_emb_index)\n",
    "\n",
    "overleap(SB_gcn_extracted_match_indices.cpu().numpy(), SB_gcn_et_index)\n",
    "\n",
    "overleap(SB_gin_texts_match_indices.cpu().numpy(), SB_gin_emb_index)\n",
    "\n",
    "overleap(SB_gin_extracted_match_indices.cpu().numpy(), SB_gin_et_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_cls_KM=KM(SB_embeddings[compute_list])\n",
    "\n",
    "SB_extracted_text_KM=KM(SB_extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_cls=SB_cls_KM.predict(SB_embeddings[compute_list])\n",
    "SB_extracted_text_cls=SB_extracted_text_KM.predict(SB_extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_cls_index=SB_cls_KM.predict(SB_embeddings[SB_gcn_texts_k1_match_indices].reshape(SB_embeddings.shape)[compute_list])\n",
    "SB_et_index=SB_extracted_text_KM.predict(SB_extracted_embeddings[SB_gcn_extracte_k1_match_indices].reshape(SB_extracted_embeddings.shape)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_x_cls=SB_cls_index==SB_cls\n",
    "SB_et=SB_et_index==SB_extracted_text_cls\n",
    "np.sum(SB_x_cls),np.sum(SB_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_cls_index=SB_cls_KM.predict(SB_embeddings[SB_gin_texts_k1_match_indices].reshape(SB_embeddings.shape)[compute_list])\n",
    "SB_et_index=SB_extracted_text_KM.predict(SB_extracted_embeddings[SB_gin_extracted_k1_match_indices].reshape(SB_extracted_embeddings.shape)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_x_cls=SB_cls_index==SB_cls\n",
    "SB_et=SB_et_index==SB_extracted_text_cls\n",
    "np.sum(SB_x_cls),np.sum(SB_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIS K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls_KM=KM(gcn_embeddings.detach().numpy())\n",
    "\n",
    "SBSMIS_extracted_text_KM=KM(gcn_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls=SBSMIS_cls_KM.predict(gcn_embeddings.detach().numpy())\n",
    "SBSMIS_extracted_text_cls=SBSMIS_extracted_text_KM.predict(gcn_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls_index=SBSMIS_cls_KM.predict(gcn_embeddings[SB_gcn_texts_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy() )\n",
    "SBSMIS_et_index=SBSMIS_extracted_text_KM.predict(gcn_embeddings[SB_gcn_extracte_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_x_cls=SBSMIS_cls_index==SBSMIS_cls\n",
    "SBSMIS_et=SBSMIS_et_index==SBSMIS_extracted_text_cls\n",
    "np.sum(SBSMIS_x_cls),np.sum(SBSMIS_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls_KM=KM(gin_embeddings.detach().numpy())\n",
    "SBSMIS_extracted_text_KM=KM(gin_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls=SBSMIS_cls_KM.predict(gin_embeddings.detach().numpy())\n",
    "SBSMIS_extracted_text_cls=SBSMIS_extracted_text_KM.predict(gin_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_cls_index=SBSMIS_cls_KM.predict(gin_embeddings[SB_gin_texts_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy())\n",
    "SBSMIS_et_index=SBSMIS_extracted_text_KM.predict(gin_embeddings[SB_gin_extracted_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_x_cls=SBSMIS_cls_index==SBSMIS_cls\n",
    "SBSMIS_et=SBSMIS_et_index==SBSMIS_extracted_text_cls\n",
    "np.sum(SBSMIS_x_cls),np.sum(SBSMIS_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del SB_embeddings,\n",
    "del SB_extracted_embeddings,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "def pureBert(texts=texts):\n",
    "    # 检查是否有可用的 GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cuda:1')\n",
    "    # 加载预训练的BERT模型和tokenizer\n",
    "    model_name = 'bert-base-uncased'  # 你可以使用其他的BERT模型，例如 'bert-large-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # 将模型转移到 GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PB_cls_embedding,PB_mean_embedding=pureBert(text_list)\n",
    "PB_cls_embedding1,PB_mean_embedding1=pureBert(text_list[:1000])\n",
    "PB_cls_embedding2,PB_mean_embedding2=pureBert(text_list[1000:2000])\n",
    "PB_cls_embedding3,PB_mean_embedding3=pureBert(text_list[2000:3000])\n",
    "PB_cls_embedding4,PB_mean_embedding4=pureBert(text_list[3000:])\n",
    "\n",
    "PB_cls_embedding=torch.cat((PB_cls_embedding1,PB_cls_embedding2,PB_cls_embedding3,PB_cls_embedding4))\n",
    "PB_mean_embedding=torch.cat((PB_mean_embedding1,PB_mean_embedding2,PB_mean_embedding3,PB_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_cls_embedding.shape,PB_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PB_extracted_text_cls_embedding,PB_extracted_text_mean_embedding=pureBert(et_list)\n",
    "PB_extracted_text_cls_embedding1,PB_extracted_text_mean_embedding1=pureBert(et_list[:1000])\n",
    "PB_extracted_text_cls_embedding2,PB_extracted_text_mean_embedding2=pureBert(et_list[1000:2000])\n",
    "PB_extracted_text_cls_embedding3,PB_extracted_text_mean_embedding3=pureBert(et_list[2000:3000])\n",
    "PB_extracted_text_cls_embedding4,PB_extracted_text_mean_embedding4=pureBert(et_list[3000:])\n",
    "\n",
    "PB_extracted_text_cls_embedding=torch.cat((PB_extracted_text_cls_embedding1,PB_extracted_text_cls_embedding2,PB_extracted_text_cls_embedding3,PB_extracted_text_cls_embedding4))\n",
    "PB_extracted_text_mean_embedding=torch.cat((PB_extracted_text_mean_embedding1,PB_extracted_text_mean_embedding2,PB_extracted_text_mean_embedding3,PB_extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(PB_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(PB_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(PB_cls_embedding[compute_list],gin_embeddings),de_biased_cka(PB_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(PB_mean_embedding[compute_list],gcn_embeddings),de_biased_cka(PB_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(PB_mean_embedding[compute_list],gin_embeddings),de_biased_cka(PB_mean_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.09211786, 0.089327276, 0.09924333, 0.098099664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(PB_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(PB_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(PB_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(PB_extracted_text_cls_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.14848486, 0.14544165, 0.14571217, 0.14427526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(PB_extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(PB_extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(PB_extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(PB_extracted_text_mean_embedding[compute_list],gin_out_embeddings)\n",
    "# (0.1153796, 0.11176958, 0.11518018, 0.11434773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(PB_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(PB_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(PB_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(PB_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(PB_mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(PB_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(PB_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(PB_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(PB_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(PB_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(PB_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(PB_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(PB_extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(PB_extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(PB_extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(PB_extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "# tensor(0.1656, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1650, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1629, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1613, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.0987, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0975, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1058, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1047, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.1557, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1550, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1533, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1520, device='cuda:0', grad_fn=<DivBackward0>)\n",
    "# tensor(0.1221, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1203, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1222, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1214, device='cuda:0', grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del PB_cls_embedding1,\n",
    "del PB_mean_embedding1,\n",
    "del PB_cls_embedding2,\n",
    "del PB_mean_embedding2,\n",
    "del PB_cls_embedding3,\n",
    "del PB_mean_embedding3,\n",
    "del PB_cls_embedding4,\n",
    "del PB_mean_embedding4,\n",
    "del PB_cls_embedding,\n",
    "del PB_mean_embedding,\n",
    "del PB_extracted_text_cls_embedding1,\n",
    "del PB_extracted_text_mean_embedding1,\n",
    "del PB_extracted_text_cls_embedding2,\n",
    "del PB_extracted_text_mean_embedding2,\n",
    "del PB_extracted_text_cls_embedding3,\n",
    "del PB_extracted_text_mean_embedding3,\n",
    "del PB_extracted_text_cls_embedding4,\n",
    "del PB_extracted_text_mean_embedding4,\n",
    "del PB_extracted_text_cls_embedding,\n",
    "del PB_extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "def get_GPTembedding(texts=texts):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "    # Set the padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Run model to get hidden states\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_cls_embedding,GPT_mean_embedding=get_GPTembedding(text_list)\n",
    "GPT_cls_embedding1,GPT_mean_embedding1=get_GPTembedding(text_list[:1000])\n",
    "GPT_cls_embedding2,GPT_mean_embedding2=get_GPTembedding(text_list[1000:2000])\n",
    "GPT_cls_embedding3,GPT_mean_embedding3=get_GPTembedding(text_list[2000:3000])\n",
    "GPT_cls_embedding4,GPT_mean_embedding4=get_GPTembedding(text_list[3000:])\n",
    "\n",
    "GPT_cls_embedding=torch.cat((GPT_cls_embedding1,GPT_cls_embedding2,GPT_cls_embedding3,GPT_cls_embedding4))\n",
    "GPT_mean_embedding=torch.cat((GPT_mean_embedding1,GPT_mean_embedding2,GPT_mean_embedding3,GPT_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_cls_embedding.shape,GPT_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_extracted_text_cls_embedding,GPT_extracted_text_mean_embedding=get_GPTembedding(et_list)\n",
    "GPT_extracted_text_cls_embedding1,GPT_extracted_text_mean_embedding1=get_GPTembedding(et_list[:1000])\n",
    "GPT_extracted_text_cls_embedding2,GPT_extracted_text_mean_embedding2=get_GPTembedding(et_list[1000:2000])\n",
    "GPT_extracted_text_cls_embedding3,GPT_extracted_text_mean_embedding3=get_GPTembedding(et_list[2000:3000])\n",
    "GPT_extracted_text_cls_embedding4,GPT_extracted_text_mean_embedding4=get_GPTembedding(et_list[3000:])\n",
    "\n",
    "GPT_extracted_text_cls_embedding=torch.cat((GPT_extracted_text_cls_embedding1,GPT_extracted_text_cls_embedding2,GPT_extracted_text_cls_embedding3,GPT_extracted_text_cls_embedding4))\n",
    "GPT_extracted_text_mean_embedding=torch.cat((GPT_extracted_text_mean_embedding1,GPT_extracted_text_mean_embedding2,GPT_extracted_text_mean_embedding3,GPT_extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(GPT_cls_embedding[compute_list],gin_embeddings),de_biased_cka(GPT_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT_mean_embedding[compute_list],gcn_embeddings),de_biased_cka(GPT_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT_mean_embedding[compute_list],gin_embeddings),de_biased_cka(GPT_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(GPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT_extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT_extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT_extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(GPT_extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(GPT_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(GPT_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT_mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(GPT_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT_extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT_extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT_extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT_extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del GPT_cls_embedding1,\n",
    "del GPT_mean_embedding1,\n",
    "del GPT_cls_embedding2,\n",
    "del GPT_mean_embedding2,\n",
    "del GPT_cls_embedding3,\n",
    "del GPT_mean_embedding3,\n",
    "del GPT_cls_embedding4,\n",
    "del GPT_mean_embedding4,\n",
    "del GPT_cls_embedding,\n",
    "del GPT_mean_embedding,\n",
    "del GPT_extracted_text_cls_embedding1,\n",
    "del GPT_extracted_text_mean_embedding1,\n",
    "del GPT_extracted_text_cls_embedding2,\n",
    "del GPT_extracted_text_mean_embedding2,\n",
    "del GPT_extracted_text_cls_embedding3,\n",
    "del GPT_extracted_text_mean_embedding3,\n",
    "del GPT_extracted_text_cls_embedding4,\n",
    "del GPT_extracted_text_mean_embedding4,\n",
    "del GPT_extracted_text_cls_embedding,\n",
    "del GPT_extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def LastGPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_cls_embedding=LastGPTembedding(text_list)\n",
    "\n",
    "LastGPT_cls_embedding1=LastGPTembedding(text_list[:1000])\n",
    "LastGPT_cls_embedding2=LastGPTembedding(text_list[1000:2000])\n",
    "LastGPT_cls_embedding3=LastGPTembedding(text_list[2000:3000])\n",
    "LastGPT_cls_embedding4=LastGPTembedding(text_list[3000:])\n",
    "\n",
    "LastGPT_cls_embedding=torch.cat((LastGPT_cls_embedding1,LastGPT_cls_embedding2,LastGPT_cls_embedding3,LastGPT_cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_extracted_text_cls_embedding=LastGPTembedding(et_list)\n",
    "\n",
    "LastGPT_extracted_text_cls_embedding1=LastGPTembedding(et_list[:1000])\n",
    "LastGPT_extracted_text_cls_embedding2=LastGPTembedding(et_list[1000:2000])\n",
    "LastGPT_extracted_text_cls_embedding3=LastGPTembedding(et_list[2000:3000])\n",
    "LastGPT_extracted_text_cls_embedding4=LastGPTembedding(et_list[3000:])\n",
    "\n",
    "LastGPT_extracted_text_cls_embedding=torch.cat((LastGPT_extracted_text_cls_embedding1,LastGPT_extracted_text_cls_embedding2,LastGPT_extracted_text_cls_embedding3,LastGPT_extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(LastGPT_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(LastGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastGPT_cls_embedding1,\n",
    "del LastGPT_cls_embedding2,\n",
    "del LastGPT_cls_embedding3,\n",
    "del LastGPT_cls_embedding4,\n",
    "del LastGPT_cls_embedding,\n",
    "del LastGPT_extracted_text_cls_embedding1,\n",
    "del LastGPT_extracted_text_cls_embedding2,\n",
    "del LastGPT_extracted_text_cls_embedding3,\n",
    "del LastGPT_extracted_text_cls_embedding4,\n",
    "del LastGPT_extracted_text_cls_embedding,\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def LastSGPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT_cls_embedding=LastSGPTembedding(text_list)\n",
    "LastSGPT_cls_embedding1=LastSGPTembedding(text_list[:1000])\n",
    "LastSGPT_cls_embedding2=LastSGPTembedding(text_list[1000:2000])\n",
    "LastSGPT_cls_embedding3=LastSGPTembedding(text_list[2000:3000])\n",
    "LastSGPT_cls_embedding4=LastSGPTembedding(text_list[3000:])\n",
    "\n",
    "LastSGPT_cls_embedding=torch.cat((LastSGPT_cls_embedding1,LastSGPT_cls_embedding2,LastSGPT_cls_embedding3,LastSGPT_cls_embedding4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT_extracted_text_cls_embedding=LastSGPTembedding(et_list)\n",
    "LastSGPT_extracted_text_cls_embedding1=LastGPTembedding(et_list[:1000])\n",
    "LastSGPT_extracted_text_cls_embedding2=LastGPTembedding(et_list[1000:2000])\n",
    "LastSGPT_extracted_text_cls_embedding3=LastGPTembedding(et_list[2000:3000])\n",
    "LastSGPT_extracted_text_cls_embedding4=LastGPTembedding(et_list[3000:])\n",
    "\n",
    "LastSGPT_extracted_text_cls_embedding=torch.cat((LastSGPT_extracted_text_cls_embedding1,LastSGPT_extracted_text_cls_embedding2,LastSGPT_extracted_text_cls_embedding3,LastSGPT_extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastSGPT_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(LastSGPT_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastSGPT_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastSGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(LastSGPT_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastSGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(LastSGPT_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastSGPT_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(LastSGPT_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastSGPT_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(LastSGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastSGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(LastSGPT_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastSGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastSGPT_cls_embedding1,\n",
    "del LastSGPT_cls_embedding2,\n",
    "del LastSGPT_cls_embedding3,\n",
    "del LastSGPT_cls_embedding4,\n",
    "del LastSGPT_cls_embedding,\n",
    "del LastSGPT_extracted_text_cls_embedding1,\n",
    "del LastSGPT_extracted_text_cls_embedding2,\n",
    "del LastSGPT_extracted_text_cls_embedding3,\n",
    "del LastSGPT_extracted_text_cls_embedding4,\n",
    "del LastSGPT_extracted_text_cls_embedding,\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "    input_ids = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "\n",
    "    # Get the hidden states (outputs from each layer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # # Optionally, move the result back to the CPU if needed\n",
    "    # sentence_embedding = sentence_embedding.cpu().detach()\n",
    "\n",
    "    # print(sentence_embedding.shape)\n",
    "    # return sentence_embedding\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2_cls_embedding,GPT2_mean_embedding=GPT2embedding(text_list)\n",
    "\n",
    "GPT2_cls_embedding1,GPT2_mean_embedding1=GPT2embedding(text_list[:1000])\n",
    "GPT2_cls_embedding2,GPT2_mean_embedding2=GPT2embedding(text_list[1000:2000])\n",
    "GPT2_cls_embedding3,GPT2_mean_embedding3=GPT2embedding(text_list[2000:3000])\n",
    "GPT2_cls_embedding4,GPT2_mean_embedding4=GPT2embedding(text_list[3000:])\n",
    "\n",
    "GPT2_cls_embedding=torch.cat((GPT2_cls_embedding1,GPT2_cls_embedding2,GPT2_cls_embedding3,GPT2_cls_embedding4))\n",
    "GPT2_mean_embedding=torch.cat((GPT2_mean_embedding1,GPT2_mean_embedding2,GPT2_mean_embedding3,GPT2_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2_extracted_text_cls_embedding,GPT2_extracted_text_mean_embedding=GPT2embedding(et_list)\n",
    "\n",
    "GPT2_extracted_text_cls_embedding1,GPT2_extracted_text_mean_embedding1=GPT2embedding(et_list[:1000])\n",
    "GPT2_extracted_text_cls_embedding2,GPT2_extracted_text_mean_embedding2=GPT2embedding(et_list[1000:2000])\n",
    "GPT2_extracted_text_cls_embedding3,GPT2_extracted_text_mean_embedding3=GPT2embedding(et_list[2000:3000])\n",
    "GPT2_extracted_text_cls_embedding4,GPT2_extracted_text_mean_embedding4=GPT2embedding(et_list[3000:])\n",
    "\n",
    "GPT2_extracted_text_cls_embedding=torch.cat((GPT2_extracted_text_cls_embedding1,GPT2_extracted_text_cls_embedding2,GPT2_extracted_text_cls_embedding3,GPT2_extracted_text_cls_embedding4))\n",
    "GPT2_extracted_text_mean_embedding=torch.cat((GPT2_extracted_text_mean_embedding1,GPT2_extracted_text_mean_embedding2,GPT2_extracted_text_mean_embedding3,GPT2_extracted_text_mean_embedding4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT2_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT2_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(GPT2_cls_embedding[compute_list],gin_embeddings),de_biased_cka(GPT2_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT2_mean_embedding[compute_list],gcn_embeddings),de_biased_cka(GPT2_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT2_mean_embedding[compute_list],gin_embeddings),de_biased_cka(GPT2_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT2_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT2_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT2_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(GPT2_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT2_extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(GPT2_extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(GPT2_extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(GPT2_extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(GPT2_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT2_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(GPT2_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT2_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT2_mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(GPT2_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT2_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT2_mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT2_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT2_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT2_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT2_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(GPT2_extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(GPT2_extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(GPT2_extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(GPT2_extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del GPT2_cls_embedding1,\n",
    "del GPT2_mean_embedding1,\n",
    "del GPT2_cls_embedding2,\n",
    "del GPT2_mean_embedding2,\n",
    "del GPT2_cls_embedding3,\n",
    "del GPT2_mean_embedding3,\n",
    "del GPT2_cls_embedding4,\n",
    "del GPT2_mean_embedding4,\n",
    "del GPT2_cls_embedding,\n",
    "del GPT2_mean_embedding,\n",
    "del GPT2_extracted_text_cls_embedding1,\n",
    "del GPT2_extracted_text_mean_embedding1,\n",
    "del GPT2_extracted_text_cls_embedding2,\n",
    "del GPT2_extracted_text_mean_embedding2,\n",
    "del GPT2_extracted_text_cls_embedding3,\n",
    "del GPT2_extracted_text_mean_embedding3,\n",
    "del GPT2_extracted_text_cls_embedding4,\n",
    "del GPT2_extracted_text_mean_embedding4,\n",
    "del GPT2_extracted_text_cls_embedding,\n",
    "del GPT2_extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def LastGPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_cls_embedding=LastGPT2embedding(text_list)\n",
    "\n",
    "LastGPT_cls_embedding1=LastGPT2embedding(text_list[:1000])\n",
    "LastGPT_cls_embedding2=LastGPT2embedding(text_list[1000:2000])\n",
    "LastGPT_cls_embedding3=LastGPT2embedding(text_list[2000:3000])\n",
    "LastGPT_cls_embedding4=LastGPT2embedding(text_list[3000:])\n",
    "\n",
    "LastGPT_cls_embedding=torch.cat((LastGPT_cls_embedding1,LastGPT_cls_embedding2,LastGPT_cls_embedding3,LastGPT_cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_extracted_text_cls_embedding=LastGPT2embedding(et_list)\n",
    "\n",
    "LastGPT_extracted_text_cls_embedding1=LastGPT2embedding(et_list[:1000])\n",
    "LastGPT_extracted_text_cls_embedding2=LastGPT2embedding(et_list[1000:2000])\n",
    "LastGPT_extracted_text_cls_embedding3=LastGPT2embedding(et_list[2000:3000])\n",
    "LastGPT_extracted_text_cls_embedding4=LastGPT2embedding(et_list[3000:])\n",
    "\n",
    "LastGPT_extracted_text_cls_embedding=torch.cat((LastGPT_extracted_text_cls_embedding1,LastGPT_extracted_text_cls_embedding2,LastGPT_extracted_text_cls_embedding3,LastGPT_extracted_text_cls_embedding4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastGPT_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(LastGPT_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastGPT_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(LastGPT_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastGPT_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastGPT_cls_embedding1,\n",
    "del LastGPT_cls_embedding2,\n",
    "del LastGPT_cls_embedding3,\n",
    "del LastGPT_cls_embedding4,\n",
    "del LastGPT_cls_embedding,\n",
    "del LastGPT_extracted_text_cls_embedding1,\n",
    "del LastGPT_extracted_text_cls_embedding2,\n",
    "del LastGPT_extracted_text_cls_embedding3,\n",
    "del LastGPT_extracted_text_cls_embedding4,\n",
    "del LastGPT_extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def LastSGPT2_embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT2_cls_embedding=LastSGPT2_embedding(text_list)\n",
    "LastSGPT2_cls_embedding1=LastSGPT2_embedding(text_list[:1000])\n",
    "LastSGPT2_cls_embedding2=LastSGPT2_embedding(text_list[1000:2000])\n",
    "LastSGPT2_cls_embedding3=LastSGPT2_embedding(text_list[2000:3000])\n",
    "LastSGPT2_cls_embedding4=LastSGPT2_embedding(text_list[3000:])\n",
    "\n",
    "LastSGPT2_cls_embedding=torch.cat((LastSGPT2_cls_embedding1,LastSGPT2_cls_embedding2,LastSGPT2_cls_embedding3,LastSGPT2_cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT2_extracted_text_cls_embedding=LastSGPT2_embedding(et_list)\n",
    "LastSGPT2_extracted_text_cls_embedding1=LastSGPT2_embedding(et_list[:1000])\n",
    "LastSGPT2_extracted_text_cls_embedding2=LastSGPT2_embedding(et_list[1000:2000])\n",
    "LastSGPT2_extracted_text_cls_embedding3=LastSGPT2_embedding(et_list[2000:3000])\n",
    "LastSGPT2_extracted_text_cls_embedding4=LastSGPT2_embedding(et_list[3000:])\n",
    "\n",
    "LastSGPT2_extracted_text_cls_embedding=torch.cat((LastSGPT2_extracted_text_cls_embedding1,LastSGPT2_extracted_text_cls_embedding2,LastSGPT2_extracted_text_cls_embedding3,LastSGPT2_extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT2_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastSGPT2_cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(LastSGPT2_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastSGPT2_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT2_extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(LastSGPT2_extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(LastSGPT2_extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(LastSGPT2_extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(LastSGPT2_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastSGPT2_cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(LastSGPT2_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastSGPT2_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(LastSGPT2_extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(LastSGPT2_extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(LastSGPT2_extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(LastSGPT2_extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastSGPT2_cls_embedding1,\n",
    "del LastSGPT2_cls_embedding2,\n",
    "del LastSGPT2_cls_embedding3,\n",
    "del LastSGPT2_cls_embedding4,\n",
    "del LastSGPT2_cls_embedding,\n",
    "del LastSGPT2_extracted_text_cls_embedding1,\n",
    "del LastSGPT2_extracted_text_cls_embedding2,\n",
    "del LastSGPT2_extracted_text_cls_embedding3,\n",
    "del LastSGPT2_extracted_text_cls_embedding4,\n",
    "del LastSGPT2_extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenBioMed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
