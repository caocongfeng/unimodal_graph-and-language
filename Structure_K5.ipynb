{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "import random\n",
    "\n",
    "path = osp.dirname(osp.abspath(''))\n",
    "sys.path.append(path)\n",
    "sys.path.append(osp.join(path, \"./open_biomed\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text(path=None):\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        text_list = []\n",
    "        for index, line in enumerate(f):\n",
    "            text = line.strip()\n",
    "            text_list.append(text)\n",
    "    return text_list\n",
    "path='./extracted_chemistry_keywords.txt'\n",
    "extracted_text=read_text(path=path)\n",
    "path='./text_after_structure_filter.txt'\n",
    "texts=read_text(path=path)\n",
    "\n",
    "# path='./smis_after_structure_filter.txt'\n",
    "# smis=read_text(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_isomers(path=''):\n",
    "    sim_list=[]\n",
    "    with open(path, 'r') as f:\n",
    "        for index, line in enumerate(f):\n",
    "\n",
    "            isomers_list = line.strip().split(\"\\t\")\n",
    "            sim_list.append(isomers_list[0])\n",
    "    return sim_list\n",
    "structure_isomers=structure_isomers(path='./structure_ismers.txt')\n",
    "\n",
    "smis=structure_isomers\n",
    "print((len(texts),len(smis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(texts),len(smis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性核函数，用 PyTorch 实现\n",
    "def linear_kernel(X):\n",
    "    return torch.mm(X, X.T)  # 使用矩阵乘法\n",
    "\n",
    "# 居中核矩阵\n",
    "def center_kernel(K):\n",
    "    n = K.shape[0]\n",
    "    H = torch.eye(n, device=K.device) - torch.ones((n, n), device=K.device) / n  # 中心化矩阵 H\n",
    "    return torch.mm(H, torch.mm(K, H))\n",
    "\n",
    "# CKA 实现，计算两个张量 X 和 Y 的 CKA\n",
    "def CKA(X, Y):\n",
    "    # 计算线性核矩阵\n",
    "    K = linear_kernel(X)\n",
    "    L = linear_kernel(Y)\n",
    "\n",
    "    # 居中核矩阵\n",
    "    Kc = center_kernel(K)\n",
    "    Lc = center_kernel(L)\n",
    "\n",
    "    # 计算 HSIC 值\n",
    "    hsic_XY = torch.trace(torch.mm(Kc, Lc))\n",
    "    hsic_XX = torch.trace(torch.mm(Kc, Kc))\n",
    "    hsic_YY = torch.trace(torch.mm(Lc, Lc))\n",
    "\n",
    "    # 计算 CKA 值\n",
    "    cka_value = hsic_XY / torch.sqrt(hsic_XX * hsic_YY)\n",
    "    return cka_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import cka,gram_linear\n",
    "# structure_feats_np = structure_feats[indes].cpu().detach().numpy()\n",
    "# text_feats_np = text_feats[indes].cpu().detach().numpy()\n",
    "# # Calculate Gram matrices for structure_feats and text_feats\n",
    "def de_biased_cka(structure_feats,text_feats):\n",
    "    structure_feats_np = structure_feats.cpu().detach().numpy()\n",
    "    text_feats_np = text_feats.cpu().detach().numpy()\n",
    "    gram_structure = gram_linear(structure_feats_np)\n",
    "    gram_text = gram_linear(text_feats_np)\n",
    "\n",
    "    # Calculate CKA between the two Gram matrices\n",
    "    cka_value = cka(gram_structure, gram_text, debiased=True)\n",
    "\n",
    "    return cka_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def retrieve(A, B, K=1):\n",
    "    \"\"\"\n",
    "    检索任务函数，输入两个 tensor A 和 B，返回 A 和 B 匹配的索引值、匹配的分数和正确匹配的条目数。\n",
    "\n",
    "    参数：\n",
    "    - A: torch.Tensor，待匹配的 query tensor，形状 (n, d)。\n",
    "    - B: torch.Tensor，待匹配的 target tensor，形状 (n, d)。\n",
    "    - K: int，检索 top K 匹配。\n",
    "\n",
    "    返回：\n",
    "    - indices: 每个 A 对应的最相似 B 的索引值，形状 (n, K)。\n",
    "    - scores: 每个 A 对应的最相似 B 的分数，形状 (n, K)。\n",
    "    - correct_total: 匹配正确的总条目数。\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算 A 和 B 之间的余弦相似度 (n, n)\n",
    "    similarities = F.cosine_similarity(A.unsqueeze(1), B.unsqueeze(0), dim=-1)\n",
    "    # 获取相似度最高的前 K 个条目的索引和分数\n",
    "    topk_scores, topk_indices = torch.topk(similarities, K, dim=1)\n",
    "    # 计算总的正确匹配数量\n",
    "    # 正确的定义：A[i] 和 B[i] 相匹配，意味着第 i 个 A 应该匹配第 i 个 B\n",
    "    correct_total = 0\n",
    "    n = A.size(0)\n",
    "    for i in range(n):\n",
    "        # 检查 top K 的索引是否包含正确匹配\n",
    "        if i in topk_indices[i]:\n",
    "            correct_total += 1\n",
    "    return topk_indices, topk_scores, correct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overleap(indexA,indexB ):\n",
    "    C=[]\n",
    "    D=[]\n",
    "    for a, b in zip(indexA, indexB):\n",
    "        overlap = list(set(a) & set(b))\n",
    "        C.append(overlap)\n",
    "        D.append([len(overlap)])\n",
    "\n",
    "    # Print results\n",
    "    print(\"Overlap C:\", C)\n",
    "    print(\"Number of overlaps D:\", D)\n",
    "\n",
    "    sum=0\n",
    "    for i in C:\n",
    "        sum=sum+len(i)\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def KM(embedding=None,num_clusters=5):\n",
    "    num_clusters = num_clusters  # You can set this based on your requirement\n",
    "\n",
    "    # Initialize and fit the KMeans model\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def NN(embedding=None,num=10):\n",
    "    # Initialize the Nearest Neighbors model\n",
    "    num_neighbors = num  # Number of nearest neighbors to find\n",
    "    nn_model = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "\n",
    "    # Fit the model to your data\n",
    "    try:\n",
    "        nn_model.fit(embedding.cpu().numpy())\n",
    "    except:\n",
    "        nn_model.fit(embedding.detach().cpu().numpy())\n",
    "\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_text),len(smis),len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_compute_list=[]\n",
    "# with open('compute_list.txt','r') as f:\n",
    "#     for index, line in enumerate(f):\n",
    "#         i = line.strip()\n",
    "#         new_compute_list.append(int(i))\n",
    "\n",
    "# new_pass_list=[]\n",
    "# with open('pass_list.txt','r') as f:\n",
    "#     for index, line in enumerate(f):\n",
    "#         i = line.strip()\n",
    "#         new_pass_list.append(int(i))\n",
    "\n",
    "# len(new_compute_list),len(new_pass_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(extracted_text),len(smis),len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text=[extracted_text[i] for i in new_compute_list]\n",
    "# smis=[smis[i] for i in new_compute_list]\n",
    "# texts=[texts[i] for i in new_compute_list]\n",
    "# len(extracted_text),len(smis),len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aligned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "import json\n",
    "from open_biomed.utils.data_utils import DataProcessorFast\n",
    "from open_biomed.utils import fix_path_in_config\n",
    "\n",
    "config = json.load(open(\"./../configs/mtr/molfm.json\", \"r\"))\n",
    "# config['network']['bert_config_path']='../../configs/encoders/multimodal/molfm_bert_config.json'\n",
    "# config['data']['mol']['featurizer']['text']['model_name_or_path']='../../ckpts/text_ckpts/scibert_scivocab_uncased'\n",
    "fix_path_in_config(config, path)\n",
    "print(\"Config: \", config)\n",
    "processor = DataProcessorFast(entity_type=\"molecule\", config=config[\"data\"][\"mol\"])\n",
    "processor.featurizer.set_mol2text_dict(dict(zip(smis, texts)))\n",
    "mols = processor(smis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from open_biomed.models.multimodal import MolFM\n",
    "model = MolFM(config[\"network\"])\n",
    "state_dict = torch.load(\"./../ckpts/fusion_ckpts/MolFM.pth\", map_location=\"cpu\")[\"model\"]\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    structure_feats = model.encode_mol(mols[\"structure\"].to(device), proj=True)\n",
    "    text_feats = model.encode_text(mols[\"text\"].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_feats.shape,text_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(structure_feats,text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(structure_feats,text_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =structure_feats\n",
    "B = text_feats\n",
    "indices_k1, scores_k1, correct_total_k1 = retrieve(A.to(device), B.to(device), K=1)\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices_k1)\n",
    "# print(\"Top K 分数：\", scores_k1)\n",
    "print(\"正确匹配的条目数：\", correct_total_k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =structure_feats\n",
    "B = text_feats\n",
    "indices, scores, correct_total = retrieve(A.to(device), B.to(device), K=10)\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_NN=NN(embedding=B)\n",
    "texts_NN_distance,texts_NN_indes=texts_NN.kneighbors(B.cpu().detach())\n",
    "overleap(indices.cpu().numpy(), texts_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis_NN=NN(embedding=A)\n",
    "smis_NN_distance,smis_NN_indes=smis_NN.kneighbors(A.cpu().detach())\n",
    "overleap(indices.cpu().numpy(), smis_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "### texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "texts_KM=KM(B.cpu().numpy())\n",
    "cls_index=texts_KM.predict(B.cpu().numpy())\n",
    "\n",
    "C=B[indices_k1.cpu()].reshape(B.shape).cpu().numpy()\n",
    "match_index=texts_KM.predict(C)\n",
    "x=cls_index==match_index\n",
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### smis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A =structure_feats\n",
    "# B = text_feats\n",
    "\n",
    "smis_KM=KM(A.cpu().numpy())\n",
    "\n",
    "smis_index=smis_KM.predict(A.cpu().numpy())\n",
    "\n",
    "C=A[indices_k1.cpu()].reshape(A.shape).cpu().numpy()\n",
    "match_index=smis_KM.predict(C)\n",
    "\n",
    "x=smis_index==match_index\n",
    "\n",
    "np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoLFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # success_sim\n",
    "# # success_text\n",
    "# import torch\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "\n",
    "# # smiles = [\"Cn1c(=O)c2c(ncn2C)n(C)c1=O\", \"CC(=O)Oc1ccccc1C(=O)O\"]\n",
    "# inputs = tokenizer(smis, padding=True, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     sim_embeddings = model(**inputs)\n",
    "# # sim_embeddings.pooler_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_embeddings.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sim_embeddings,'./savedpt/Structure_sim_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_embeddings=torch.load('./savedpt/Structure_sim_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_embeddings.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "def SCIBERT(texts=texts):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_embedding,SCIB_mean_embedding=SCIBERT(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_extracted_text_cls_embedding,SCIB_extracted_text_mean_embedding=SCIBERT(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(SCIB_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_mean_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_extracted_text_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(SCIB_extracted_text_mean_embedding,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(SCIB_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_mean_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_extracted_text_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(SCIB_extracted_text_mean_embedding.cpu(),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have cls mean embedding of texts and extracted_text\n",
    "cls_embedding,\n",
    "\n",
    "mean_embedding,\n",
    "\n",
    "extracted_text_cls_embedding,\n",
    "\n",
    "extracted_text_mean_embedding,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_cls_NN=NN(embedding=SCIB_cls_embedding)\n",
    "SCIB_texts_mean_NN=NN(embedding=SCIB_mean_embedding)\n",
    "SCIB_extracted_text_cls_NN=NN(embedding=SCIB_extracted_text_cls_embedding)\n",
    "SCIB_extracted_text_mean_NN=NN(embedding=SCIB_extracted_text_mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_cls_NN_distance,SCIB_texts_cls_NN_indes=SCIB_texts_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_texts_mean_NN_distance,SCIB_texts_mean_NN_indes=SCIB_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_extracted_text_cls_NN_distance,SCIB_extracted_text_cls_NN_indes=SCIB_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SCIB_extracted_text_mean_NN_distance,SCIB_extracted_text_mean_NN_indes=SCIB_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_match_indices, SCIB_texts_match_scores, SCIB_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_cls_embedding.cpu().detach(), K=10)\n",
    "SCIB_texts_mean_match_indices, SCIB_texts_mean_match_scores, SCIB_texts_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_mean_match_correct_total)\n",
    "SCIB_extracted_text_match_indices, SCIB_extracted_text_match_scores, SCIB_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_extracted_text_cls_embedding.cpu().detach(), K=10)\n",
    "SCIB_extracted_text_mean_match_indices, SCIB_extracted_text_mean_match_scores, SCIB_extracted_text_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, SCIB_extracted_text_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_texts_k1_match_indices, SCIB_texts_k1_match_scores, SCIB_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_cls_embedding.cpu().detach(), K=1)\n",
    "SCIB_texts_K1_mean_match_indices, SCIB_texts_k1_mean_match_scores, SCIB_texts_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_k1_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_texts_k1_mean_match_correct_total)\n",
    "SCIB_extracted_k1_text_match_indices, SCIB_extracted_k1_text_match_scores, SCIB_extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_extracted_text_cls_embedding.cpu().detach(), K=1)\n",
    "SCIB_extracted_k1_text_mean_match_indices, SCIB_extracted_k1_text_mean_match_scores, SCIB_extracted_text_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), SCIB_extracted_text_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_k1_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", SCIB_extracted_text_k1_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIB_texts_cls_NN_indes)\n",
    "\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIB_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIB_texts_mean_NN_indes)\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIB_texts_cls_NN_indes)\n",
    "print('='*20)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIB_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIB_extracted_text_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIB_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIB_extracted_text_mean_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SCIBSMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_texts_cls_NN_distance,SCIBSMIS_texts_cls_NN_indes=SCIBSMIS_texts_cls_NN.kneighbors(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIBSMIS_texts_mean_NN_distance,SCIBSMIS_texts_mean_NN_indes=SCIBSMIS_texts_mean_NN.kneighbors(SCIB_mean_embedding.cpu().numpy())\n",
    "\n",
    "SCIBSMIS_extracted_text_cls_NN_distance,SCIBSMIS_extracted_text_cls_NN_indes=SCIBSMIS_extracted_text_cls_NN.kneighbors(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIBSMIS_extracted_text_mean_NN_distance,SCIBSMIS_extracted_text_mean_NN_indes=SCIBSMIS_extracted_text_mean_NN.kneighbors(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIBSMIS_texts_cls_NN_indes)\n",
    "\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIBSMIS_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(SCIB_texts_match_indices.cpu().numpy(), SCIBSMIS_texts_mean_NN_indes)\n",
    "overleap(SCIB_texts_mean_match_indices.cpu().numpy(), SCIBSMIS_texts_cls_NN_indes)\n",
    "print('='*20)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "overleap(SCIB_extracted_text_mean_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(SCIB_extracted_text_match_indices.cpu().numpy(), SCIBSMIS_extracted_text_mean_NN_indes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_KM=KM(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIB_mean_KM=KM(SCIB_mean_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_cls_KM=KM(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_mean_KM=KM(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_KM_index=SCIB_cls_KM.predict(SCIB_cls_embedding.cpu().numpy())\n",
    "SCIB_mean_KM_index=SCIB_mean_KM.predict(SCIB_mean_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_cls_KM_index=SCIB_extracted_text_cls_KM.predict(SCIB_extracted_text_cls_embedding.cpu().numpy())\n",
    "SCIB_extracted_text_mean_KM_index=SCIB_extracted_text_mean_KM.predict(SCIB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_cls_match_index=SCIB_cls_KM.predict(SCIB_cls_embedding[SCIB_texts_k1_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_mean_match_index=SCIB_mean_KM.predict(SCIB_mean_embedding[SCIB_texts_K1_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_et_cls_match_index=SCIB_extracted_text_cls_KM.predict(SCIB_extracted_text_cls_embedding[SCIB_extracted_k1_text_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIB_et_mean_match_index=SCIB_extracted_text_mean_KM.predict(SCIB_extracted_text_mean_embedding[SCIB_extracted_k1_text_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIB_x_cls=SCIB_cls_match_index==SCIB_cls_KM_index\n",
    "SCIB_x_mean=SCIB_mean_KM_index==SCIB_mean_match_index\n",
    "SCIB_x_et_cls=SCIB_extracted_text_cls_KM_index==SCIB_et_cls_match_index\n",
    "SCIB_x_et_mean=SCIB_et_mean_match_index==SCIB_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(SCIB_x_cls),np.sum(SCIB_x_mean),np.sum(SCIB_x_et_cls),np.sum(SCIB_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smis K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_KM_index=SCIBSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_mean_KM_index=SCIBSMIS_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_cls_KM_index=SCIBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SCIBSMIS_extracted_text_mean_KM_index=SCIBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_cls_match_index=SCIBSMIS_cls_KM.predict(sim_embeddings.pooler_output[SCIB_texts_k1_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_mean_match_index=SCIBSMIS_mean_KM.predict(sim_embeddings.pooler_output[SCIB_texts_K1_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_et_cls_match_index=SCIBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[SCIB_extracted_k1_text_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())\n",
    "SCIBSMIS_et_mean_match_index=SCIBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output[SCIB_extracted_k1_text_mean_match_indices].reshape(SCIB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIBSMIS_x_cls=SCIBSMIS_cls_match_index==SCIBSMIS_cls_KM_index\n",
    "SCIBSMIS_x_mean=SCIBSMIS_mean_KM_index==SCIBSMIS_mean_match_index\n",
    "SCIBSMIS_x_et_cls=SCIBSMIS_extracted_text_cls_KM_index==SCIBSMIS_et_cls_match_index\n",
    "SCIBSMIS_x_et_mean=SCIBSMIS_et_mean_match_index==SCIBSMIS_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(SCIBSMIS_x_cls),np.sum(SCIBSMIS_x_mean),np.sum(SCIBSMIS_x_et_cls),np.sum(SCIBSMIS_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "def pureBert(texts=texts):\n",
    "    # 检查是否有可用的 GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cuda:1')\n",
    "    # 加载预训练的BERT模型和tokenizer\n",
    "    model_name = 'bert-base-uncased'  # 你可以使用其他的BERT模型，例如 'bert-large-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # 将模型转移到 GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_cls_embedding,PB_mean_embedding=pureBert(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_extracted_text_cls_embedding,PB_extracted_text_mean_embedding=pureBert(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(PB_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(PB_mean_embedding,sim_embeddings.pooler_output),de_biased_cka(PB_extracted_text_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(PB_extracted_text_mean_embedding,sim_embeddings.pooler_output)\n",
    "\n",
    "# (0.21898326, 0.15394284, 0.18666576, 0.16397773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(PB_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(PB_mean_embedding.cpu(),sim_embeddings.pooler_output),CKA(PB_extracted_text_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(PB_extracted_text_mean_embedding.cpu(),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have cls mean embedding of texts and extracted_text\n",
    "cls_embedding,\n",
    "\n",
    "mean_embedding,\n",
    "\n",
    "extracted_text_cls_embedding,\n",
    "\n",
    "extracted_text_mean_embedding,\n",
    "\n",
    "#### exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_texts_match_indices, PB_texts_match_scores, PB_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, PB_cls_embedding.cpu().detach(), K=10)\n",
    "PB_texts_mean_match_indices, PB_texts_mean_match_scores, PB_texts_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, PB_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", PB_texts_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", PB_texts_mean_match_correct_total)\n",
    "PB_extracted_text_match_indices, PB_extracted_text_match_scores, PB_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output, PB_extracted_text_cls_embedding.cpu().detach(), K=10)\n",
    "PB_extracted_text_mean_match_indices, PB_extracted_text_mean_match_scores, PB_extracted_text_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, PB_extracted_text_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "print(\"正确匹配的条目数：\", PB_extracted_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", PB_extracted_text_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_texts_k1_match_indices, PB_texts_k1_match_scores, PB_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), PB_cls_embedding.cpu().detach(), K=1)\n",
    "texts_K1_mean_match_indices, texts_k1_mean_match_scores, texts_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), PB_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", PB_texts_k1_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", texts_k1_mean_match_correct_total)\n",
    "PB_extracted_k1_text_match_indices, PB_extracted_k1_text_match_scores, PB_extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), PB_extracted_text_cls_embedding.cpu().detach(), K=1)\n",
    "PB_extracted_k1_text_mean_match_indices, PB_extracted_k1_text_mean_match_scores, PB_extracted_text_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), PB_extracted_text_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "print(\"正确匹配的条目数：\", PB_extracted_k1_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", PB_extracted_text_k1_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_texts_cls_NN=NN(embedding=PB_cls_embedding)\n",
    "PB_texts_mean_NN=NN(embedding=PB_mean_embedding)\n",
    "PB_extracted_text_cls_NN=NN(embedding=PB_extracted_text_cls_embedding)\n",
    "PB_extracted_text_mean_NN=NN(embedding=PB_extracted_text_mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_texts_cls_NN_distance,PB_texts_cls_NN_indes=PB_texts_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "PB_,PB_texts_mean_NN_indes=PB_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "\n",
    "PB_extracted_text_cls_NN_distance,PB_extracted_text_cls_NN_indes=PB_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "PB_extracted_text_mean_NN_distance,PB_extracted_text_mean_NN_indes=PB_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(PB_texts_match_indices.cpu().numpy(), PB_texts_cls_NN_indes)\n",
    "\n",
    "overleap(PB_texts_mean_match_indices.cpu().numpy(), PB_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(PB_texts_match_indices.cpu().numpy(), PB_texts_mean_NN_indes)\n",
    "overleap(PB_texts_mean_match_indices.cpu().numpy(), PB_texts_cls_NN_indes)\n",
    "print('='*30)\n",
    "overleap(PB_extracted_text_match_indices.cpu().numpy(), PB_extracted_text_cls_NN_indes)\n",
    "overleap(PB_extracted_text_mean_match_indices.cpu().numpy(), PB_extracted_text_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(PB_extracted_text_mean_match_indices.cpu().numpy(), PB_extracted_text_cls_NN_indes)\n",
    "overleap(PB_extracted_text_match_indices.cpu().numpy(), PB_extracted_text_mean_NN_indes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "PBSMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "PBSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "PBSMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_texts_cls_NN_distance,PBSMIS_texts_cls_NN_indes=PBSMIS_texts_cls_NN.kneighbors(PB_cls_embedding.cpu().numpy())\n",
    "PBSMIS_texts_mean_NN_distance,PBSMIS_texts_mean_NN_indes=PBSMIS_texts_mean_NN.kneighbors(PB_mean_embedding.cpu().numpy())\n",
    "\n",
    "PBSMIS_extracted_text_cls_NN_distance,PBSMIS_extracted_text_cls_NN_indes=PBSMIS_extracted_text_cls_NN.kneighbors(PB_extracted_text_cls_embedding.cpu().numpy())\n",
    "PBSMIS_extracted_text_mean_NN_distance,PBSMIS_extracted_text_mean_NN_indes=PBSMIS_extracted_text_mean_NN.kneighbors(PB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(PB_texts_match_indices.cpu().numpy(), PBSMIS_texts_cls_NN_indes)\n",
    "\n",
    "overleap(PB_texts_mean_match_indices.cpu().numpy(), PBSMIS_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(PB_texts_match_indices.cpu().numpy(), PBSMIS_texts_mean_NN_indes)\n",
    "overleap(PB_texts_mean_match_indices.cpu().numpy(), PBSMIS_texts_cls_NN_indes)\n",
    "print('='*30)\n",
    "overleap(PB_extracted_text_match_indices.cpu().numpy(), PBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(PB_extracted_text_mean_match_indices.cpu().numpy(), PBSMIS_extracted_text_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "overleap(PB_extracted_text_mean_match_indices.cpu().numpy(), PBSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(PB_extracted_text_match_indices.cpu().numpy(), PBSMIS_extracted_text_mean_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_cls_KM=KM(PB_cls_embedding.cpu().numpy())\n",
    "PB_mean_KM=KM(PB_mean_embedding.cpu().numpy())\n",
    "\n",
    "PB_extracted_text_cls_KM=KM(PB_extracted_text_cls_embedding.cpu().numpy())\n",
    "PB_extracted_text_mean_KM=KM(PB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_cls_KM_index=PB_cls_KM.predict(PB_cls_embedding.cpu().numpy())\n",
    "PB_mean_KM_index=PB_mean_KM.predict(PB_mean_embedding.cpu().numpy())\n",
    "\n",
    "PB_extracted_text_cls_KM_index=PB_extracted_text_cls_KM.predict(PB_extracted_text_cls_embedding.cpu().numpy())\n",
    "PB_extracted_text_mean_KM_index=PB_extracted_text_mean_KM.predict(PB_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_cls_match_index=PB_cls_KM.predict(PB_cls_embedding[PB_texts_k1_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PB_mean_match_index=PB_mean_KM.predict(PB_mean_embedding[texts_K1_mean_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PB_et_cls_match_index=PB_extracted_text_cls_KM.predict(PB_extracted_text_cls_embedding[PB_extracted_k1_text_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PB_et_mean_match_index=PB_extracted_text_mean_KM.predict(PB_extracted_text_mean_embedding[PB_extracted_k1_text_mean_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB_x_cls=PB_cls_match_index==PB_cls_KM_index\n",
    "PB_x_mean=PB_mean_KM_index==PB_mean_match_index\n",
    "PB_x_et_cls=PB_extracted_text_cls_KM_index==PB_et_cls_match_index\n",
    "PB_x_et_mean=PB_et_mean_match_index==PB_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(PB_x_cls),np.sum(PB_x_mean),np.sum(PB_x_et_cls),np.sum(PB_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smis K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "PBSMIS_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "PBSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "PBSMIS_extracted_text_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_cls_KM_index=PBSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "PBSMIS_mean_KM_index=PBSMIS_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "PBSMIS_extracted_text_cls_KM_index=PBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "PBSMIS_extracted_text_mean_KM_index=PBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = sim_embeddings.pooler_output\n",
    "# print(\"Dtype after pooler output:\", c.dtype)  # Should be torch.float32 if using PyTorch\n",
    "# c = c[texts_k1_match_indices].reshape(cls_embedding.shape).cpu().numpy()\n",
    "# print(\"Dtype after conversion to numpy:\", c.dtype)  # Should be float64 initially\n",
    "# c = c.astype(np.float32)\n",
    "# print(\"Dtype after astype(float32):\", c.dtype)  # Confirm it is now float32\n",
    "# cls_match_index = cls_KM.predict(c)\n",
    "# # extracted_k1_text_match_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_cls_match_index=PBSMIS_cls_KM.predict(sim_embeddings.pooler_output[PB_texts_k1_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PBSMIS_mean_match_index=PBSMIS_mean_KM.predict(sim_embeddings.pooler_output[texts_K1_mean_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PBSMIS_et_cls_match_index=PBSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[PB_extracted_k1_text_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())\n",
    "PBSMIS_et_mean_match_index=PBSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output[PB_extracted_k1_text_mean_match_indices].reshape(PB_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBSMIS_x_cls=PBSMIS_cls_match_index==PBSMIS_cls_KM_index\n",
    "PBSMIS_x_mean=PBSMIS_mean_KM_index==PBSMIS_mean_match_index\n",
    "PBSMIS_x_et_cls=PBSMIS_extracted_text_cls_KM_index==PBSMIS_et_cls_match_index\n",
    "PBSMIS_x_et_mean=PBSMIS_et_mean_match_index==PBSMIS_extracted_text_mean_KM_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(PBSMIS_x_cls),np.sum(PBSMIS_x_mean),np.sum(PBSMIS_x_et_cls),np.sum(PBSMIS_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\").to(device)\n",
    "# 2. Calculate SB_embeddings by calling model.encode()\n",
    "SB_embeddings = model.encode(texts)\n",
    "print('SB_embeddings',SB_embeddings.shape)\n",
    "\n",
    "SB_extracted_embeddings = model.encode(extracted_text)\n",
    "print('SB_extracted_embeddings',SB_extracted_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(torch.from_numpy(SB_embeddings),sim_embeddings.pooler_output),de_biased_cka(torch.from_numpy(SB_extracted_embeddings),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(torch.from_numpy(SB_embeddings),sim_embeddings.pooler_output),CKA(torch.from_numpy(SB_extracted_embeddings),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exactly match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "SB_texts_match_indices, SB_texts_match_scores, SB_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, torch.from_numpy(SB_embeddings).cpu().detach(), K=K)\n",
    "SB_extracted_match_indices, SB_extracted_match_scores, SB_extracted_match_correct_total = retrieve(sim_embeddings.pooler_output, torch.from_numpy(SB_extracted_embeddings).cpu().detach(), K=K)\n",
    "\n",
    "print(SB_texts_match_correct_total,SB_extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "\n",
    "SB_texts_k1_match_indices, SB_texts_k1_match_scores, SB_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), torch.from_numpy(SB_embeddings).cpu().detach(), K=1)\n",
    "SB_extracted_k1_match_indices, SB_extracted_k1_match_scores, SB_extracted_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), torch.from_numpy(SB_extracted_embeddings).cpu().detach(), K=1)\n",
    "\n",
    "print(SB_texts_k1_match_correct_total,SB_extracted_k1_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_texts_mean_NN=NN(embedding=torch.from_numpy(SB_embeddings))\n",
    "SB_extracted_text_mean_NN=NN(embedding=torch.from_numpy(SB_extracted_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_emb_distance,SB_emb_index=SB_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "SB_et_distance,SB_et_index=SB_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SB_texts_match_indices.cpu().numpy(), SB_emb_index)\n",
    "\n",
    "overleap(SB_extracted_match_indices.cpu().numpy(), SB_et_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "SBSMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBSMIS_emb_distance,SBSMIS_emb_index=SBSMIS_texts_mean_NN.kneighbors(SB_embeddings)\n",
    "SBSMIS_et_distance,SBSMIS_et_index=SBSMIS_extracted_text_mean_NN.kneighbors(SB_extracted_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(SB_texts_match_indices.cpu().numpy(), SBSMIS_emb_index)\n",
    "\n",
    "overleap(SB_extracted_match_indices.cpu().numpy(), SBSMIS_et_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "#### text K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB_cls_KM=KM(SB_embeddings)\n",
    "\n",
    "SB_extracted_text_KM=KM(SB_extracted_embeddings)\n",
    "SB_cls=SB_cls_KM.predict(SB_embeddings)\n",
    "SB_extracted_text_cls=SB_extracted_text_KM.predict(SB_extracted_embeddings)\n",
    "SB_cls_index=SB_cls_KM.predict(SB_embeddings[SB_texts_k1_match_indices].reshape(SB_embeddings.shape))\n",
    "SB_et_index=SB_extracted_text_KM.predict(SB_extracted_embeddings[SB_extracted_k1_match_indices].reshape(SB_extracted_embeddings.shape))\n",
    "SB_x_cls=SB_cls_index==SB_cls\n",
    "SB_et=SB_et_index==SB_extracted_text_cls\n",
    "np.sum(SB_x_cls),np.sum(SB_et)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SMIS\n",
    "SBSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "SBSMIS_extracted_text_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "SBSMIS_cls=SBSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "SBSMIS_extracted_text_cls=SBSMIS_extracted_text_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "SBSMIS_cls_index=SBSMIS_cls_KM.predict(sim_embeddings.pooler_output[SB_texts_k1_match_indices].reshape(SB_embeddings.shape).cpu().numpy())\n",
    "SBSMIS_et_index=SBSMIS_extracted_text_KM.predict(sim_embeddings.pooler_output[SB_extracted_k1_match_indices].reshape(SB_embeddings.shape).cpu().numpy())\n",
    "\n",
    "SBSMIS_x_cls=SBSMIS_cls_index==SBSMIS_cls\n",
    "SBSMIS_et=SBSMIS_et_index==SBSMIS_extracted_text_cls\n",
    "np.sum(SBSMIS_x_cls),np.sum(SBSMIS_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "def get_GPT_embedding(texts=texts):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "    # Set the padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Run model to get hidden states\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_cls_embedding,GPT_mean_embedding=get_GPT_embedding(texts)\n",
    "\n",
    "GPT_cls_embedding1,GPT_mean_embedding1=get_GPT_embedding(texts[:1000])\n",
    "GPT_cls_embedding2,GPT_mean_embedding2=get_GPT_embedding(texts[1000:2000])\n",
    "GPT_cls_embedding3,GPT_mean_embedding3=get_GPT_embedding(texts[2000:3000])\n",
    "GPT_cls_embedding4,GPT_mean_embedding4=get_GPT_embedding(texts[3000:])\n",
    "GPT_cls_embedding=torch.cat((GPT_cls_embedding1,GPT_cls_embedding2,GPT_cls_embedding3,GPT_cls_embedding4))\n",
    "GPT_mean_embedding=torch.cat((GPT_mean_embedding1,GPT_mean_embedding2,GPT_mean_embedding3,GPT_mean_embedding4))\n",
    "GPT_cls_embedding.shape,GPT_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_extracted_text_cls_embedding1,GPT_extracted_text_mean_embedding1=get_GPT_embedding(extracted_text[:1000])\n",
    "GPT_extracted_text_cls_embedding2,GPT_extracted_text_mean_embedding2=get_GPT_embedding(extracted_text[1000:2000])\n",
    "GPT_extracted_text_cls_embedding3,GPT_extracted_text_mean_embedding3=get_GPT_embedding(extracted_text[2000:3000])\n",
    "GPT_extracted_text_cls_embedding4,GPT_extracted_text_mean_embedding4=get_GPT_embedding(extracted_text[3000:])\n",
    "GPT_extracted_text_cls_embedding=torch.cat((GPT_extracted_text_cls_embedding1,GPT_extracted_text_cls_embedding2,GPT_extracted_text_cls_embedding3,GPT_extracted_text_cls_embedding4))\n",
    "GPT_extracted_text_mean_embedding=torch.cat((GPT_extracted_text_mean_embedding1,GPT_extracted_text_mean_embedding2,GPT_extracted_text_mean_embedding3,GPT_extracted_text_mean_embedding4))\n",
    "GPT_extracted_text_cls_embedding.shape,GPT_extracted_text_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT_mean_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT_extracted_text_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT_extracted_text_mean_embedding,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(GPT_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT_mean_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT_extracted_text_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT_extracted_text_mean_embedding.cpu(),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have cls mean embedding of texts and extracted_text\n",
    "cls_embedding,\n",
    "\n",
    "mean_embedding,\n",
    "\n",
    "extracted_text_cls_embedding,\n",
    "\n",
    "extracted_text_mean_embedding,\n",
    "\n",
    "#### exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_texts_match_indices, GPT_texts_match_scores, GPT_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT_cls_embedding.cpu().detach(), K=10)\n",
    "GPT_texts_mean_match_indices, GPT_texts_mean_match_scores, GPT_texts_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", GPT_texts_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT_texts_mean_match_correct_total)\n",
    "GPT_extracted_text_match_indices, GPT_extracted_text_match_scores, GPTextracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT_extracted_text_cls_embedding.cpu().detach(), K=10)\n",
    "GPT_extracted_text_mean_match_indices, GPT_extracted_text_mean_match_scores, GPT_extracted_text_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT_extracted_text_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "print(\"正确匹配的条目数：\", GPTextracted_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT_extracted_text_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_texts_k1_match_indices, GPT_texts_k1_match_scores, GPT_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT_cls_embedding.cpu().detach(), K=1)\n",
    "GPT_texts_K1_mean_match_indices, GPT_texts_k1_mean_match_scores, GPT_texts_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", GPT_texts_k1_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT_texts_k1_mean_match_correct_total)\n",
    "GPT_extracted_k1_text_match_indices, GPT_extracted_k1_text_match_scores, GPT_extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT_extracted_text_cls_embedding.cpu().detach(), K=1)\n",
    "GPT_extracted_k1_text_mean_match_indices, GPT_extracted_k1_text_mean_match_scores, GPT_extracted_text_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT_extracted_text_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "print(\"正确匹配的条目数：\", GPT_extracted_k1_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT_extracted_text_k1_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_texts_cls_NN=NN(embedding=GPT_cls_embedding)\n",
    "GPT_texts_mean_NN=NN(embedding=GPT_mean_embedding)\n",
    "GPT_extracted_text_cls_NN=NN(embedding=GPT_extracted_text_cls_embedding)\n",
    "GPT_extracted_text_mean_NN=NN(embedding=GPT_extracted_text_mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPT_texts_cls_NN_distance,GPT_texts_cls_NN_indes=GPT_texts_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "GPT_texts_mean_NN_distance,GPT_texts_mean_NN_indes=GPT_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "\n",
    "GPT_extracted_text_cls_NN_distance,GPT_extracted_text_cls_NN_indes=GPT_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "GPT_extracted_text_mean_NN_distance,GPT_extracted_text_mean_NN_indes=GPT_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(GPT_texts_match_indices.cpu().numpy(), GPT_texts_cls_NN_indes)\n",
    "\n",
    "overleap(GPT_texts_mean_match_indices.cpu().numpy(), GPT_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT_texts_match_indices.cpu().numpy(), GPT_texts_mean_NN_indes)\n",
    "overleap(GPT_texts_mean_match_indices.cpu().numpy(), GPT_texts_cls_NN_indes)\n",
    "print('='*10)\n",
    "overleap(GPT_extracted_text_match_indices.cpu().numpy(), GPT_extracted_text_cls_NN_indes)\n",
    "overleap(GPT_extracted_text_mean_match_indices.cpu().numpy(), GPT_extracted_text_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT_extracted_text_mean_match_indices.cpu().numpy(), GPT_extracted_text_cls_NN_indes)\n",
    "overleap(GPT_extracted_text_match_indices.cpu().numpy(), GPT_extracted_text_mean_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPTSMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPTSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPTSMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPTSMIS_texts_cls_NN_distance,GPTSMIS_texts_cls_NN_indes=GPTSMIS_texts_cls_NN.kneighbors(GPT_cls_embedding.cpu().numpy())\n",
    "GPTSMIS_texts_mean_NN_distance,GPTSMIS_texts_mean_NN_indes=GPTSMIS_texts_mean_NN.kneighbors(GPT_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPTSMIS_extracted_text_cls_NN_distance,GPTSMIS_extracted_text_cls_NN_indes=GPTSMIS_extracted_text_cls_NN.kneighbors(GPT_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPTSMIS_extracted_text_mean_NN_distance,GPTSMIS_extracted_text_mean_NN_indes=GPTSMIS_extracted_text_mean_NN.kneighbors(GPT_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(GPT_texts_match_indices.cpu().numpy(), GPTSMIS_texts_cls_NN_indes)\n",
    "\n",
    "overleap(GPT_texts_mean_match_indices.cpu().numpy(), GPTSMIS_texts_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT_texts_match_indices.cpu().numpy(), GPTSMIS_texts_mean_NN_indes)\n",
    "overleap(GPT_texts_mean_match_indices.cpu().numpy(), GPTSMIS_texts_cls_NN_indes)\n",
    "print('='*30)\n",
    "overleap(GPT_extracted_text_match_indices.cpu().numpy(), GPTSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(GPT_extracted_text_mean_match_indices.cpu().numpy(), GPTSMIS_extracted_text_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "overleap(GPT_extracted_text_mean_match_indices.cpu().numpy(), GPTSMIS_extracted_text_cls_NN_indes)\n",
    "overleap(GPT_extracted_text_match_indices.cpu().numpy(), GPTSMIS_extracted_text_mean_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_cls_KM=KM(GPT_cls_embedding.cpu().numpy())\n",
    "GPT_mean_KM=KM(GPT_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPT_extracted_text_cls_KM=KM(GPT_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPT_extracted_text_mean_KM=KM(GPT_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_cls_KM_index=GPT_cls_KM.predict(GPT_cls_embedding.cpu().numpy())\n",
    "GPT_mean_KM_index=GPT_mean_KM.predict(GPT_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPT_extracted_text_cls_KM_index=GPT_extracted_text_cls_KM.predict(GPT_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPT_extracted_text_mean_KM_index=GPT_extracted_text_mean_KM.predict(GPT_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_cls_match_index=GPT_cls_KM.predict(GPT_cls_embedding[GPT_texts_k1_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPT_mean_match_index=GPT_mean_KM.predict(GPT_mean_embedding[GPT_texts_K1_mean_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPT_et_cls_match_index=GPT_extracted_text_cls_KM.predict(GPT_extracted_text_cls_embedding[GPT_extracted_k1_text_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPT_et_mean_match_index=GPT_extracted_text_mean_KM.predict(GPT_extracted_text_mean_embedding[GPT_extracted_k1_text_mean_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_x_cls=GPT_cls_match_index==GPT_cls_KM_index\n",
    "GPT_x_mean=GPT_mean_KM_index==GPT_mean_match_index\n",
    "GPT_x_et_cls=GPT_extracted_text_cls_KM_index==GPT_et_cls_match_index\n",
    "GPT_x_et_mean=GPT_et_mean_match_index==GPT_extracted_text_mean_KM_index\n",
    "np.sum(GPT_x_cls),np.sum(GPT_x_mean),np.sum(GPT_x_et_cls),np.sum(GPT_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smis K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPTSMIS_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "GPTSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPTSMIS_extracted_text_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTSMIS_cls_KM_index=GPTSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPTSMIS_mean_KM_index=GPTSMIS_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "GPTSMIS_extracted_text_cls_KM_index=GPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPTSMIS_extracted_text_mean_KM_index=GPTSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTSMIS_cls_match_index=GPTSMIS_cls_KM.predict(sim_embeddings.pooler_output[GPT_texts_k1_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPTSMIS_mean_match_index=GPTSMIS_mean_KM.predict(sim_embeddings.pooler_output[GPT_texts_K1_mean_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPTSMIS_et_cls_match_index=GPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[GPT_extracted_k1_text_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())\n",
    "GPTSMIS_et_mean_match_index=GPTSMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output[GPT_extracted_k1_text_mean_match_indices].reshape(GPT_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTSMIS_x_cls=GPTSMIS_cls_match_index==GPTSMIS_cls_KM_index\n",
    "GPTSMIS_x_mean=GPTSMIS_mean_KM_index==GPTSMIS_mean_match_index\n",
    "GPTSMIS_x_et_cls=GPTSMIS_extracted_text_cls_KM_index==GPTSMIS_et_cls_match_index\n",
    "GPTSMIS_x_et_mean=GPTSMIS_et_mean_match_index==GPTSMIS_extracted_text_mean_KM_index\n",
    "np.sum(GPTSMIS_x_cls),np.sum(GPTSMIS_x_mean),np.sum(GPTSMIS_x_et_cls),np.sum(GPTSMIS_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def Last_GPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_embeddings=Last_GPTembedding(texts)\n",
    "LastGPT_embeddings1=Last_GPTembedding(texts[:1000])\n",
    "LastGPT_embeddings2=Last_GPTembedding(texts[1000:2000])\n",
    "LastGPT_embeddings3=Last_GPTembedding(texts[2000:3000])\n",
    "LastGPT_embeddings4=Last_GPTembedding(texts[3000:])\n",
    "\n",
    "LastGPT_embeddings=torch.cat((LastGPT_embeddings1,LastGPT_embeddings2,LastGPT_embeddings3,LastGPT_embeddings4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT_et_embeddings=Last_GPTembedding(extracted_text)\n",
    "LastGPT_et_embeddings1=Last_GPTembedding(extracted_text[:1000])\n",
    "LastGPT_et_embeddings2=Last_GPTembedding(extracted_text[1000:2000])\n",
    "LastGPT_et_embeddings3=Last_GPTembedding(extracted_text[2000:3000])\n",
    "LastGPT_et_embeddings4=Last_GPTembedding(extracted_text[3000:])\n",
    "\n",
    "LastGPT_et_embeddings=torch.cat((LastGPT_et_embeddings1,LastGPT_et_embeddings2,LastGPT_et_embeddings3,LastGPT_et_embeddings4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_embeddings.shape,LastGPT_et_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT_embeddings,sim_embeddings.pooler_output),de_biased_cka(LastGPT_et_embeddings,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(LastGPT_embeddings,sim_embeddings.pooler_output.to(device)),CKA(LastGPT_et_embeddings,sim_embeddings.pooler_output.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "\n",
    "LastGPT_texts_match_indices, LastGPT_texts_match_scores, LastGPT_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastGPT_texts_match_correct_total)\n",
    "LastGPT_extracted_text_match_indices, LastGPT_extracted_text_match_scores, LastGPT_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastGPT_extracted_text_match_correct_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "\n",
    "LastGPT_texts_k1_match_indices, LastGPT_texts_match_scores, LastGPT_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastGPT_texts_match_correct_total)\n",
    "LastGPT_extracted_k1_text_match_indices, LastGPT_extracted_text_match_scores, LastGPT_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastGPT_extracted_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_texts_cls_NN=NN(embedding=LastGPT_embeddings)\n",
    "LastGPT_extracted_text_cls_NN=NN(embedding=LastGPT_et_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_texts_cls_NN_distance,LastGPT_texts_cls_NN_indes=LastGPT_texts_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())\n",
    "\n",
    "LastGPT_extracted_text_cls_NN_distance,LastGPT_extracted_text_cls_NN_indes=LastGPT_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastGPT_texts_match_indices.cpu().numpy(), LastGPT_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastGPT_extracted_text_match_indices.cpu().numpy(), LastGPT_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_cls_KM=KM(LastGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastGPT_extracted_text_cls_KM=KM(LastGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_cls_KM_index=LastGPT_cls_KM.predict(LastGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastGPT_extracted_text_cls_KM_index=LastGPT_extracted_text_cls_KM.predict(LastGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_cls_match_index=LastGPT_cls_KM.predict(LastGPT_embeddings[LastGPT_texts_k1_match_indices].reshape(LastGPT_embeddings.shape).cpu().numpy())\n",
    "LastGPT_et_cls_match_index=LastGPT_extracted_text_cls_KM.predict(LastGPT_et_embeddings[LastGPT_extracted_k1_text_match_indices].reshape(LastGPT_et_embeddings.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT_x_cls=LastGPT_cls_match_index==LastGPT_cls_KM_index\n",
    "LastGPT_x_et_cls=LastGPT_extracted_text_cls_KM_index==LastGPT_et_cls_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(LastGPT_x_cls),np.sum(LastGPT_x_et_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPTSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "LastGPTSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPTSMIS_texts_cls_NN_distance,LastGPTSMIS_texts_cls_NN_indes=LastGPTSMIS_texts_cls_NN.kneighbors(LastGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastGPTSMIS_extracted_text_cls_NN_distance,LastGPTSMIS_extracted_text_cls_NN_indes=LastGPTSMIS_extracted_text_cls_NN.kneighbors(LastGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastGPT_texts_match_indices.cpu().numpy(), LastGPTSMIS_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastGPT_extracted_text_match_indices.cpu().numpy(), LastGPTSMIS_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPTSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "LastGPTSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPTSMIS_cls_KM_index=LastGPTSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "LastGPTSMIS_extracted_text_cls_KM_index=LastGPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPTSMIS_cls_match_index=LastGPTSMIS_cls_KM.predict(sim_embeddings.pooler_output[LastGPT_texts_k1_match_indices].reshape(LastGPT_embeddings.shape).cpu().numpy().astype(np.float32))\n",
    "LastGPTSMIS_et_cls_match_index=LastGPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[LastGPT_extracted_k1_text_match_indices].reshape(LastGPT_et_embeddings.shape).cpu().numpy().astype(np.float32))\n",
    "LastGPTSMIS_x_cls=LastGPTSMIS_cls_match_index==LastGPTSMIS_cls_KM_index\n",
    "LastGPTSMIS_x_et_cls=LastGPTSMIS_extracted_text_cls_KM_index==LastGPTSMIS_et_cls_match_index\n",
    "\n",
    "np.sum(LastGPTSMIS_x_cls),np.sum(LastGPTSMIS_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AutoTokenizer,\n",
    "del AutoModel,\n",
    "del SCIBERT,\n",
    "del SCIB_cls_embedding,\n",
    "del SCIB_mean_embedding,\n",
    "del SCIB_extracted_text_cls_embedding,\n",
    "del SCIB_extracted_text_mean_embedding,\n",
    "del SCIB_texts_cls_NN,\n",
    "del SCIB_texts_mean_NN,\n",
    "del SCIB_extracted_text_cls_NN,\n",
    "del SCIB_extracted_text_mean_NN,\n",
    "del SCIB_texts_cls_NN_distance,\n",
    "del SCIB_texts_cls_NN_indes,\n",
    "del SCIB_texts_mean_NN_distance,\n",
    "del SCIB_texts_mean_NN_indes,\n",
    "del SCIB_extracted_text_cls_NN_distance,\n",
    "del SCIB_extracted_text_cls_NN_indes,\n",
    "del SCIB_extracted_text_mean_NN_distance,\n",
    "del SCIB_extracted_text_mean_NN_indes,\n",
    "del SCIB_texts_match_indices,\n",
    "del SCIB_texts_match_scores,\n",
    "del SCIB_texts_match_correct_total,\n",
    "del SCIB_texts_mean_match_indices,\n",
    "del SCIB_texts_mean_match_scores,\n",
    "del SCIB_texts_mean_match_correct_total,\n",
    "del SCIB_extracted_text_match_indices,\n",
    "del SCIB_extracted_text_match_scores,\n",
    "del SCIB_extracted_text_match_correct_total,\n",
    "del SCIB_extracted_text_mean_match_indices,\n",
    "del SCIB_extracted_text_mean_match_scores,\n",
    "del SCIB_extracted_text_mean_match_correct_total,\n",
    "del SCIB_texts_k1_match_indices,\n",
    "del SCIB_texts_k1_match_scores,\n",
    "del SCIB_texts_k1_match_correct_total,\n",
    "del SCIB_texts_K1_mean_match_indices,\n",
    "del SCIB_texts_k1_mean_match_scores,\n",
    "del SCIB_texts_k1_mean_match_correct_total,\n",
    "del SCIB_extracted_k1_text_match_indices,\n",
    "del SCIB_extracted_k1_text_match_scores,\n",
    "del SCIB_extracted_k1_text_match_correct_total,\n",
    "del SCIB_extracted_k1_text_mean_match_indices,\n",
    "del SCIB_extracted_k1_text_mean_match_scores,\n",
    "del SCIB_extracted_text_k1_mean_match_correct_total,\n",
    "del SCIBSMIS_texts_cls_NN,\n",
    "del SCIBSMIS_texts_mean_NN,\n",
    "del SCIBSMIS_extracted_text_cls_NN,\n",
    "del SCIBSMIS_extracted_text_mean_NN,\n",
    "del SCIBSMIS_texts_cls_NN_distance,\n",
    "del SCIBSMIS_texts_cls_NN_indes,\n",
    "del SCIBSMIS_texts_mean_NN_distance,\n",
    "del SCIBSMIS_texts_mean_NN_indes,\n",
    "del SCIBSMIS_extracted_text_cls_NN_distance,\n",
    "del SCIBSMIS_extracted_text_cls_NN_indes,\n",
    "del SCIBSMIS_extracted_text_mean_NN_distance,\n",
    "del SCIBSMIS_extracted_text_mean_NN_indes,\n",
    "del SCIB_cls_KM,\n",
    "del SCIB_mean_KM,\n",
    "del SCIB_extracted_text_cls_KM,\n",
    "del SCIB_extracted_text_mean_KM,\n",
    "del SCIB_cls_KM_index,\n",
    "del SCIB_mean_KM_index,\n",
    "del SCIB_extracted_text_cls_KM_index,\n",
    "del SCIB_extracted_text_mean_KM_index,\n",
    "del SCIB_cls_match_index,\n",
    "del SCIB_mean_match_index,\n",
    "del SCIB_et_cls_match_index,\n",
    "del SCIB_et_mean_match_index,\n",
    "del SCIB_x_cls,\n",
    "del SCIB_x_mean,\n",
    "del SCIB_x_et_cls,\n",
    "del SCIB_x_et_mean,\n",
    "del SCIBSMIS_cls_KM,\n",
    "del SCIBSMIS_mean_KM,\n",
    "del SCIBSMIS_extracted_text_cls_KM,\n",
    "del SCIBSMIS_extracted_text_mean_KM,\n",
    "del SCIBSMIS_cls_KM_index,\n",
    "del SCIBSMIS_mean_KM_index,\n",
    "del SCIBSMIS_extracted_text_cls_KM_index,\n",
    "del SCIBSMIS_extracted_text_mean_KM_index,\n",
    "del SCIBSMIS_cls_match_index,\n",
    "del SCIBSMIS_mean_match_index,\n",
    "del SCIBSMIS_et_cls_match_index,\n",
    "del SCIBSMIS_et_mean_match_index,\n",
    "del SCIBSMIS_x_cls,\n",
    "del SCIBSMIS_x_mean,\n",
    "del SCIBSMIS_x_et_cls,\n",
    "del SCIBSMIS_x_et_mean,\n",
    "del BertTokenizer,\n",
    "del BertModel,\n",
    "del pureBert,\n",
    "del PB_cls_embedding,\n",
    "del PB_mean_embedding,\n",
    "del PB_extracted_text_cls_embedding,\n",
    "del PB_extracted_text_mean_embedding,\n",
    "del PB_texts_match_indices,\n",
    "del PB_texts_match_scores,\n",
    "del PB_texts_match_correct_total,\n",
    "del PB_texts_mean_match_indices,\n",
    "del PB_texts_mean_match_scores,\n",
    "del PB_texts_mean_match_correct_total,\n",
    "del PB_extracted_text_match_indices,\n",
    "del PB_extracted_text_match_scores,\n",
    "del PB_extracted_text_match_correct_total,\n",
    "del PB_extracted_text_mean_match_indices,\n",
    "del PB_extracted_text_mean_match_scores,\n",
    "del PB_extracted_text_mean_match_correct_total,\n",
    "del PB_texts_k1_match_indices,\n",
    "del PB_texts_k1_match_scores,\n",
    "del PB_texts_k1_match_correct_total,\n",
    "del texts_K1_mean_match_indices,\n",
    "del texts_k1_mean_match_scores,\n",
    "del texts_k1_mean_match_correct_total,\n",
    "del PB_extracted_k1_text_match_indices,\n",
    "del PB_extracted_k1_text_match_scores,\n",
    "del PB_extracted_k1_text_match_correct_total,\n",
    "del PB_extracted_k1_text_mean_match_indices,\n",
    "del PB_extracted_k1_text_mean_match_scores,\n",
    "del PB_extracted_text_k1_mean_match_correct_total,\n",
    "del PB_texts_cls_NN,\n",
    "del PB_texts_mean_NN,\n",
    "del PB_extracted_text_cls_NN,\n",
    "del PB_extracted_text_mean_NN,\n",
    "del PB_texts_cls_NN_distance,\n",
    "del PB_texts_cls_NN_indes,\n",
    "del PB_,\n",
    "del PB_texts_mean_NN_indes,\n",
    "del PB_extracted_text_cls_NN_distance,\n",
    "del PB_extracted_text_cls_NN_indes,\n",
    "del PB_extracted_text_mean_NN_distance,\n",
    "del PB_extracted_text_mean_NN_indes,\n",
    "del PBSMIS_texts_cls_NN,\n",
    "del PBSMIS_texts_mean_NN,\n",
    "del PBSMIS_extracted_text_cls_NN,\n",
    "del PBSMIS_extracted_text_mean_NN,\n",
    "del PBSMIS_texts_cls_NN_distance,\n",
    "del PBSMIS_texts_cls_NN_indes,\n",
    "del PBSMIS_texts_mean_NN_distance,\n",
    "del PBSMIS_texts_mean_NN_indes,\n",
    "del PBSMIS_extracted_text_cls_NN_distance,\n",
    "del PBSMIS_extracted_text_cls_NN_indes,\n",
    "del PBSMIS_extracted_text_mean_NN_distance,\n",
    "del PBSMIS_extracted_text_mean_NN_indes,\n",
    "del PB_cls_KM,\n",
    "del PB_mean_KM,\n",
    "del PB_extracted_text_cls_KM,\n",
    "del PB_extracted_text_mean_KM,\n",
    "del PB_cls_KM_index,\n",
    "del PB_mean_KM_index,\n",
    "del PB_extracted_text_cls_KM_index,\n",
    "del PB_extracted_text_mean_KM_index,\n",
    "del PB_cls_match_index,\n",
    "del PB_mean_match_index,\n",
    "del PB_et_cls_match_index,\n",
    "del PB_et_mean_match_index,\n",
    "del PB_x_cls,\n",
    "del PB_x_mean,\n",
    "del PB_x_et_cls,\n",
    "del PB_x_et_mean,\n",
    "del PBSMIS_cls_KM,\n",
    "del PBSMIS_mean_KM,\n",
    "del PBSMIS_extracted_text_cls_KM,\n",
    "del PBSMIS_extracted_text_mean_KM,\n",
    "del PBSMIS_cls_KM_index,\n",
    "del PBSMIS_mean_KM_index,\n",
    "del PBSMIS_extracted_text_cls_KM_index,\n",
    "del PBSMIS_extracted_text_mean_KM_index,\n",
    "del PBSMIS_cls_match_index,\n",
    "del PBSMIS_mean_match_index,\n",
    "del PBSMIS_et_cls_match_index,\n",
    "del PBSMIS_et_mean_match_index,\n",
    "del PBSMIS_x_cls,\n",
    "del PBSMIS_x_mean,\n",
    "del PBSMIS_x_et_cls,\n",
    "del PBSMIS_x_et_mean,\n",
    "del SentenceTransformer,\n",
    "del SB_embeddings,\n",
    "del SB_extracted_embeddings,\n",
    "del K,\n",
    "del SB_texts_match_indices,\n",
    "del SB_texts_match_scores,\n",
    "del SB_texts_match_correct_total,\n",
    "del SB_extracted_match_indices,\n",
    "del SB_extracted_match_scores,\n",
    "del SB_extracted_match_correct_total,\n",
    "del SB_texts_k1_match_indices,\n",
    "del SB_texts_k1_match_scores,\n",
    "del SB_texts_k1_match_correct_total,\n",
    "del SB_extracted_k1_match_indices,\n",
    "del SB_extracted_k1_match_scores,\n",
    "del SB_extracted_k1_match_correct_total,\n",
    "del SB_texts_mean_NN,\n",
    "del SB_extracted_text_mean_NN,\n",
    "del SB_emb_distance,\n",
    "del SB_emb_index,\n",
    "del SB_et_distance,\n",
    "del SB_et_index,\n",
    "del SBSMIS_texts_mean_NN,\n",
    "del SBSMIS_extracted_text_mean_NN,\n",
    "del SBSMIS_emb_distance,\n",
    "del SBSMIS_emb_index,\n",
    "del SBSMIS_et_distance,\n",
    "del SBSMIS_et_index,\n",
    "del SB_cls_KM,\n",
    "del SB_extracted_text_KM,\n",
    "del SB_cls,\n",
    "del SB_extracted_text_cls,\n",
    "del SB_cls_index,\n",
    "del SB_x_cls,\n",
    "del SB_et,\n",
    "del SBSMIS_cls_KM,\n",
    "del SBSMIS_extracted_text_KM,\n",
    "del SBSMIS_cls,\n",
    "del SBSMIS_extracted_text_cls,\n",
    "del SBSMIS_cls_index,\n",
    "del SBSMIS_x_cls,\n",
    "del SBSMIS_et,\n",
    "del OpenAIGPTModel,\n",
    "del get_GPT_embedding,\n",
    "del GPT_cls_embedding1,\n",
    "del GPT_mean_embedding1,\n",
    "del GPT_cls_embedding2,\n",
    "del GPT_mean_embedding2,\n",
    "del GPT_cls_embedding3,\n",
    "del GPT_mean_embedding3,\n",
    "del GPT_cls_embedding4,\n",
    "del GPT_mean_embedding4,\n",
    "del GPT_cls_embedding,\n",
    "del GPT_mean_embedding,\n",
    "del GPT_extracted_text_cls_embedding1,\n",
    "del GPT_extracted_text_mean_embedding1,\n",
    "del GPT_extracted_text_cls_embedding2,\n",
    "del GPT_extracted_text_mean_embedding2,\n",
    "del GPT_extracted_text_cls_embedding3,\n",
    "del GPT_extracted_text_mean_embedding3,\n",
    "del GPT_extracted_text_cls_embedding4,\n",
    "del GPT_extracted_text_mean_embedding4,\n",
    "del GPT_extracted_text_cls_embedding,\n",
    "del GPT_extracted_text_mean_embedding,\n",
    "del GPT_texts_match_indices,\n",
    "del GPT_texts_match_scores,\n",
    "del GPT_texts_match_correct_total,\n",
    "del GPT_texts_mean_match_indices,\n",
    "del GPT_texts_mean_match_scores,\n",
    "del GPT_texts_mean_match_correct_total,\n",
    "del GPT_extracted_text_match_indices,\n",
    "del GPT_extracted_text_match_scores,\n",
    "del GPTextracted_text_match_correct_total,\n",
    "del GPT_extracted_text_mean_match_indices,\n",
    "del GPT_extracted_text_mean_match_scores,\n",
    "del GPT_extracted_text_mean_match_correct_total,\n",
    "del GPT_texts_k1_match_indices,\n",
    "del GPT_texts_k1_match_scores,\n",
    "del GPT_texts_k1_match_correct_total,\n",
    "del GPT_texts_K1_mean_match_indices,\n",
    "del GPT_texts_k1_mean_match_scores,\n",
    "del GPT_texts_k1_mean_match_correct_total,\n",
    "del GPT_extracted_k1_text_match_indices,\n",
    "del GPT_extracted_k1_text_match_scores,\n",
    "del GPT_extracted_k1_text_match_correct_total,\n",
    "del GPT_extracted_k1_text_mean_match_indices,\n",
    "del GPT_extracted_k1_text_mean_match_scores,\n",
    "del GPT_extracted_text_k1_mean_match_correct_total,\n",
    "del GPT_texts_cls_NN,\n",
    "del GPT_texts_mean_NN,\n",
    "del GPT_extracted_text_cls_NN,\n",
    "del GPT_extracted_text_mean_NN,\n",
    "del CPT_texts_cls_NN_distance,\n",
    "del GPT_texts_cls_NN_indes,\n",
    "del GPT_texts_mean_NN_distance,\n",
    "del GPT_texts_mean_NN_indes,\n",
    "del GPT_extracted_text_cls_NN_distance,\n",
    "del GPT_extracted_text_cls_NN_indes,\n",
    "del GPT_extracted_text_mean_NN_distance,\n",
    "del GPT_extracted_text_mean_NN_indes,\n",
    "del GPTSMIS_texts_cls_NN,\n",
    "del GPTSMIS_texts_mean_NN,\n",
    "del GPTSMIS_extracted_text_cls_NN,\n",
    "del GPTSMIS_extracted_text_mean_NN,\n",
    "del CPTSMIS_texts_cls_NN_distance,\n",
    "del GPTSMIS_texts_cls_NN_indes,\n",
    "del GPTSMIS_texts_mean_NN_distance,\n",
    "del GPTSMIS_texts_mean_NN_indes,\n",
    "del GPTSMIS_extracted_text_cls_NN_distance,\n",
    "del GPTSMIS_extracted_text_cls_NN_indes,\n",
    "del GPTSMIS_extracted_text_mean_NN_distance,\n",
    "del GPTSMIS_extracted_text_mean_NN_indes,\n",
    "del GPT_cls_KM,\n",
    "del GPT_mean_KM,\n",
    "del GPT_extracted_text_cls_KM,\n",
    "del GPT_extracted_text_mean_KM,\n",
    "del GPT_cls_KM_index,\n",
    "del GPT_mean_KM_index,\n",
    "del GPT_extracted_text_cls_KM_index,\n",
    "del GPT_extracted_text_mean_KM_index,\n",
    "del GPT_cls_match_index,\n",
    "del GPT_mean_match_index,\n",
    "del GPT_et_cls_match_index,\n",
    "del GPT_et_mean_match_index,\n",
    "del GPT_x_cls,\n",
    "del GPT_x_mean,\n",
    "del GPT_x_et_cls,\n",
    "del GPT_x_et_mean,\n",
    "del GPTSMIS_cls_KM,\n",
    "del GPTSMIS_mean_KM,\n",
    "del GPTSMIS_extracted_text_cls_KM,\n",
    "del GPTSMIS_extracted_text_mean_KM,\n",
    "del GPTSMIS_cls_KM_index,\n",
    "del GPTSMIS_mean_KM_index,\n",
    "del GPTSMIS_extracted_text_cls_KM_index,\n",
    "del GPTSMIS_extracted_text_mean_KM_index,\n",
    "del GPTSMIS_cls_match_index,\n",
    "del GPTSMIS_mean_match_index,\n",
    "del GPTSMIS_et_cls_match_index,\n",
    "del GPTSMIS_et_mean_match_index,\n",
    "del GPTSMIS_x_cls,\n",
    "del GPTSMIS_x_mean,\n",
    "del GPTSMIS_x_et_cls,\n",
    "del GPTSMIS_x_et_mean,\n",
    "del Last_GPTembedding,\n",
    "del LastGPT_embeddings1,\n",
    "del LastGPT_embeddings2,\n",
    "del LastGPT_embeddings3,\n",
    "del LastGPT_embeddings4,\n",
    "del LastGPT_embeddings,\n",
    "del LastGPT_et_embeddings1,\n",
    "del LastGPT_et_embeddings2,\n",
    "del LastGPT_et_embeddings3,\n",
    "del LastGPT_et_embeddings4,\n",
    "del LastGPT_et_embeddings,\n",
    "del LastGPT_texts_match_indices,\n",
    "del LastGPT_texts_match_scores,\n",
    "del LastGPT_texts_match_correct_total,\n",
    "del LastGPT_extracted_text_match_indices,\n",
    "del LastGPT_extracted_text_match_scores,\n",
    "del LastGPT_extracted_text_match_correct_total,\n",
    "del LastGPT_texts_k1_match_indices,\n",
    "del LastGPT_extracted_k1_text_match_indices,\n",
    "del LastGPT_texts_cls_NN,\n",
    "del LastGPT_extracted_text_cls_NN,\n",
    "del LastGPT_texts_cls_NN_distance,\n",
    "del LastGPT_texts_cls_NN_indes,\n",
    "del LastGPT_extracted_text_cls_NN_distance,\n",
    "del LastGPT_extracted_text_cls_NN_indes,\n",
    "del LastGPT_cls_KM,\n",
    "del LastGPT_extracted_text_cls_KM,\n",
    "del LastGPT_cls_KM_index,\n",
    "del LastGPT_extracted_text_cls_KM_index,\n",
    "del LastGPT_cls_match_index,\n",
    "del LastGPT_et_cls_match_index,\n",
    "del LastGPT_x_cls,\n",
    "del LastGPT_x_et_cls,\n",
    "del LastGPTSMIS_texts_cls_NN,\n",
    "del LastGPTSMIS_extracted_text_cls_NN,\n",
    "del LastGPTSMIS_texts_cls_NN_distance,\n",
    "del LastGPTSMIS_texts_cls_NN_indes,\n",
    "del LastGPTSMIS_extracted_text_cls_NN_distance,\n",
    "del LastGPTSMIS_extracted_text_cls_NN_indes,\n",
    "del LastGPTSMIS_cls_KM,\n",
    "del LastGPTSMIS_extracted_text_cls_KM,\n",
    "del LastGPTSMIS_cls_KM_index,\n",
    "del LastGPTSMIS_extracted_text_cls_KM_index,\n",
    "del LastGPTSMIS_cls_match_index,\n",
    "del LastGPTSMIS_et_cls_match_index,\n",
    "del LastGPTSMIS_x_cls,\n",
    "del LastGPTSMIS_x_et_cls,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# \n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def Summarise_GPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT_embeddings=Summarise_GPTembedding(texts)\n",
    "# LastSGPT_et_embeddings=Summarise_GPTembedding(extracted_text)\n",
    "LastSGPT_embeddings1=Summarise_GPTembedding(texts[:1000])\n",
    "LastSGPT_embeddings2=Summarise_GPTembedding(texts[1000:2000])\n",
    "LastSGPT_embeddings3=Summarise_GPTembedding(texts[2000:3000])\n",
    "LastSGPT_embeddings4=Summarise_GPTembedding(texts[3000:])\n",
    "\n",
    "LastSGPT_embeddings=torch.cat((LastSGPT_embeddings1,LastSGPT_embeddings2,LastSGPT_embeddings3,LastSGPT_embeddings4))\n",
    "\n",
    "LastSGPT_et_embeddings1=Summarise_GPTembedding(extracted_text[:1000])\n",
    "LastSGPT_et_embeddings2=Summarise_GPTembedding(extracted_text[1000:2000])\n",
    "LastSGPT_et_embeddings3=Summarise_GPTembedding(extracted_text[2000:3000])\n",
    "LastSGPT_et_embeddings4=Summarise_GPTembedding(extracted_text[3000:])\n",
    "\n",
    "LastSGPT_et_embeddings=torch.cat((LastSGPT_et_embeddings1,LastSGPT_et_embeddings2,LastSGPT_et_embeddings3,LastSGPT_et_embeddings4))\n",
    "\n",
    "\n",
    "LastSGPT_embeddings.shape,LastSGPT_et_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT_embeddings,sim_embeddings.pooler_output),de_biased_cka(LastSGPT_et_embeddings,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(LastSGPT_embeddings,sim_embeddings.pooler_output.to(device)),CKA(LastSGPT_et_embeddings,sim_embeddings.pooler_output.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "\n",
    "LastSGPT_texts_match_indices, LastSGPT_texts_match_scores, LastSGPT_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastSGPT_texts_match_correct_total)\n",
    "LastSGPT_extracted_text_match_indices, LastSGPT_extracted_text_match_scores, LastSGPT_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastSGPT_extracted_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "\n",
    "LastSGPT_texts_k1_match_indices, LastSGPT_texts_match_scores, LastSGPT_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastSGPT_texts_match_correct_total)\n",
    "LastSGPT_extracted_k1_text_match_indices, LastSGPT_extracted_text_match_scores, LastSGPT_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastSGPT_extracted_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_texts_cls_NN=NN(embedding=LastSGPT_embeddings)\n",
    "LastSGPT_extracted_text_cls_NN=NN(embedding=LastSGPT_et_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_texts_cls_NN_distance,LastSGPT_texts_cls_NN_indes=LastSGPT_texts_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())\n",
    "\n",
    "LastSGPT_extracted_text_cls_NN_distance,LastSGPT_extracted_text_cls_NN_indes=LastSGPT_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastSGPT_texts_match_indices.cpu().numpy(), LastSGPT_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastSGPT_extracted_text_match_indices.cpu().numpy(), LastSGPT_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_cls_KM=KM(LastSGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastSGPT_extracted_text_cls_KM=KM(LastSGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_cls_KM_index=LastSGPT_cls_KM.predict(LastSGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastSGPT_extracted_text_cls_KM_index=LastSGPT_extracted_text_cls_KM.predict(LastSGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_cls_match_index=LastSGPT_cls_KM.predict(LastSGPT_embeddings[LastSGPT_texts_k1_match_indices].reshape(LastSGPT_embeddings.shape).cpu().numpy())\n",
    "LastSGPT_et_cls_match_index=LastSGPT_extracted_text_cls_KM.predict(LastSGPT_et_embeddings[LastSGPT_extracted_k1_text_match_indices].reshape(LastSGPT_et_embeddings.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT_x_cls=LastSGPT_cls_match_index==LastSGPT_cls_KM_index\n",
    "LastSGPT_x_et_cls=LastSGPT_extracted_text_cls_KM_index==LastSGPT_et_cls_match_index\n",
    "\n",
    "np.sum(LastSGPT_x_cls),np.sum(LastSGPT_x_et_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "LastSGPTSMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_texts_cls_NN_distance,LastSGPTSMIS_texts_cls_NN_indes=LastSGPTSMIS_texts_cls_NN.kneighbors(LastSGPT_embeddings.cpu().numpy())\n",
    "\n",
    "LastSGPTSMIS_extracted_text_cls_NN_distance,LastSGPTSMIS_extracted_text_cls_NN_indes=LastSGPTSMIS_extracted_text_cls_NN.kneighbors(LastSGPT_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastSGPT_texts_match_indices.cpu().numpy(), LastSGPTSMIS_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastSGPT_extracted_text_match_indices.cpu().numpy(), LastSGPTSMIS_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "LastSGPTSMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_cls_KM_index=LastSGPTSMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "LastSGPTSMIS_extracted_text_cls_KM_index=LastSGPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_cls_match_index=LastSGPTSMIS_cls_KM.predict(sim_embeddings.pooler_output[LastSGPT_texts_k1_match_indices].reshape(LastSGPT_embeddings.shape).cpu().numpy().astype(np.float32))\n",
    "LastSGPTSMIS_et_cls_match_index=LastSGPTSMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[LastSGPT_extracted_k1_text_match_indices].reshape(LastSGPT_et_embeddings.shape).cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPTSMIS_x_cls=LastSGPTSMIS_cls_match_index==LastSGPTSMIS_cls_KM_index\n",
    "LastSGPTSMIS_x_et_cls=LastSGPTSMIS_extracted_text_cls_KM_index==LastSGPTSMIS_et_cls_match_index\n",
    "\n",
    "np.sum(LastSGPTSMIS_x_cls),np.sum(LastSGPTSMIS_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del  AutoTokenizer,\n",
    "# del  AutoModel,\n",
    "# del  SCIBERT,\n",
    "# del  SCIB_cls_embedding,\n",
    "# del  SCIB_mean_embedding,\n",
    "# del  SCIB_extracted_text_cls_embedding,\n",
    "# del  SCIB_extracted_text_mean_embedding,\n",
    "# del  SCIB_texts_cls_NN,\n",
    "# del  SCIB_texts_mean_NN,\n",
    "# del  SCIB_extracted_text_cls_NN,\n",
    "# del  SCIB_extracted_text_mean_NN,\n",
    "# del  SCIB_texts_cls_NN_distance,\n",
    "# del  SCIB_texts_cls_NN_indes,\n",
    "# del  SCIB_texts_mean_NN_distance,\n",
    "# del  SCIB_texts_mean_NN_indes,\n",
    "# del  SCIB_extracted_text_cls_NN_distance,\n",
    "# del  SCIB_extracted_text_cls_NN_indes,\n",
    "# del  SCIB_extracted_text_mean_NN_distance,\n",
    "# del  SCIB_extracted_text_mean_NN_indes,\n",
    "# del  SCIB_texts_match_indices,\n",
    "# del  SCIB_texts_match_scores,\n",
    "# del  SCIB_texts_match_correct_total,\n",
    "# del  SCIB_texts_mean_match_indices,\n",
    "# del  SCIB_texts_mean_match_scores,\n",
    "# del  SCIB_texts_mean_match_correct_total,\n",
    "# del  SCIB_extracted_text_match_indices,\n",
    "# del  SCIB_extracted_text_match_scores,\n",
    "# del  SCIB_extracted_text_match_correct_total,\n",
    "# del  SCIB_extracted_text_mean_match_indices,\n",
    "# del  SCIB_extracted_text_mean_match_scores,\n",
    "# del  SCIB_extracted_text_mean_match_correct_total,\n",
    "# del  SCIB_texts_k1_match_indices,\n",
    "# del  SCIB_texts_k1_match_scores,\n",
    "# del  SCIB_texts_k1_match_correct_total,\n",
    "# del  SCIB_texts_K1_mean_match_indices,\n",
    "# del  SCIB_texts_k1_mean_match_scores,\n",
    "# del  SCIB_texts_k1_mean_match_correct_total,\n",
    "# del  SCIB_extracted_k1_text_match_indices,\n",
    "# del  SCIB_extracted_k1_text_match_scores,\n",
    "# del  SCIB_extracted_k1_text_match_correct_total,\n",
    "# del  SCIB_extracted_k1_text_mean_match_indices,\n",
    "# del  SCIB_extracted_k1_text_mean_match_scores,\n",
    "# del  SCIB_extracted_text_k1_mean_match_correct_total,\n",
    "# del  SCIBSMIS_texts_cls_NN,\n",
    "# del  SCIBSMIS_texts_mean_NN,\n",
    "# del  SCIBSMIS_extracted_text_cls_NN,\n",
    "# del  SCIBSMIS_extracted_text_mean_NN,\n",
    "# del  SCIBSMIS_texts_cls_NN_distance,\n",
    "# del  SCIBSMIS_texts_cls_NN_indes,\n",
    "# del  SCIBSMIS_texts_mean_NN_distance,\n",
    "# del  SCIBSMIS_texts_mean_NN_indes,\n",
    "# del  SCIBSMIS_extracted_text_cls_NN_distance,\n",
    "# del  SCIBSMIS_extracted_text_cls_NN_indes,\n",
    "# del  SCIBSMIS_extracted_text_mean_NN_distance,\n",
    "# del  SCIBSMIS_extracted_text_mean_NN_indes,\n",
    "# del  SCIB_cls_KM,\n",
    "# del  SCIB_mean_KM,\n",
    "# del  SCIB_extracted_text_cls_KM,\n",
    "# del  SCIB_extracted_text_mean_KM,\n",
    "# del  SCIB_cls_KM_index,\n",
    "# del  SCIB_mean_KM_index,\n",
    "# del  SCIB_extracted_text_cls_KM_index,\n",
    "# del  SCIB_extracted_text_mean_KM_index,\n",
    "# del  SCIB_cls_match_index,\n",
    "# del  SCIB_mean_match_index,\n",
    "# del  SCIB_et_cls_match_index,\n",
    "# del  SCIB_et_mean_match_index,\n",
    "# del  SCIB_x_cls,\n",
    "# del  SCIB_x_mean,\n",
    "# del  SCIB_x_et_cls,\n",
    "# del  SCIB_x_et_mean,\n",
    "# del  SCIBSMIS_cls_KM,\n",
    "# del  SCIBSMIS_mean_KM,\n",
    "# del  SCIBSMIS_extracted_text_cls_KM,\n",
    "# del  SCIBSMIS_extracted_text_mean_KM,\n",
    "# del  SCIBSMIS_cls_KM_index,\n",
    "# del  SCIBSMIS_mean_KM_index,\n",
    "# del  SCIBSMIS_extracted_text_cls_KM_index,\n",
    "# del  SCIBSMIS_extracted_text_mean_KM_index,\n",
    "# del  SCIBSMIS_cls_match_index,\n",
    "# del  SCIBSMIS_mean_match_index,\n",
    "# del  SCIBSMIS_et_cls_match_index,\n",
    "# del  SCIBSMIS_et_mean_match_index,\n",
    "# del  SCIBSMIS_x_cls,\n",
    "# del  SCIBSMIS_x_mean,\n",
    "# del  SCIBSMIS_x_et_cls,\n",
    "# del  SCIBSMIS_x_et_mean,\n",
    "# del  BertTokenizer,\n",
    "# del  BertModel,\n",
    "# del  pureBert,\n",
    "# del  PB_cls_embedding,\n",
    "# del  PB_mean_embedding,\n",
    "# del  PB_extracted_text_cls_embedding,\n",
    "# del  PB_extracted_text_mean_embedding,\n",
    "# del  PB_texts_match_indices,\n",
    "# del  PB_texts_match_scores,\n",
    "# del  PB_texts_match_correct_total,\n",
    "# del  PB_texts_mean_match_indices,\n",
    "# del  PB_texts_mean_match_scores,\n",
    "# del  PB_texts_mean_match_correct_total,\n",
    "# del  PB_extracted_text_match_indices,\n",
    "# del  PB_extracted_text_match_scores,\n",
    "# del  PB_extracted_text_match_correct_total,\n",
    "# del  PB_extracted_text_mean_match_indices,\n",
    "# del  PB_extracted_text_mean_match_scores,\n",
    "# del  PB_extracted_text_mean_match_correct_total,\n",
    "# del  PB_texts_k1_match_indices,\n",
    "# del  PB_texts_k1_match_scores,\n",
    "# del  PB_texts_k1_match_correct_total,\n",
    "# del  texts_K1_mean_match_indices,\n",
    "# del  texts_k1_mean_match_scores,\n",
    "# del  texts_k1_mean_match_correct_total,\n",
    "# del  PB_extracted_k1_text_match_indices,\n",
    "# del  PB_extracted_k1_text_match_scores,\n",
    "# del  PB_extracted_k1_text_match_correct_total,\n",
    "# del  PB_extracted_k1_text_mean_match_indices,\n",
    "# del  PB_extracted_k1_text_mean_match_scores,\n",
    "# del  PB_extracted_text_k1_mean_match_correct_total,\n",
    "# del  PB_texts_cls_NN,\n",
    "# del  PB_texts_mean_NN,\n",
    "# del  PB_extracted_text_cls_NN,\n",
    "# del  PB_extracted_text_mean_NN,\n",
    "# del  PB_texts_cls_NN_distance,\n",
    "# del  PB_texts_cls_NN_indes,\n",
    "# del  PB_,\n",
    "# del  PB_texts_mean_NN_indes,\n",
    "# del  PB_extracted_text_cls_NN_distance,\n",
    "# del  PB_extracted_text_cls_NN_indes,\n",
    "# del  PB_extracted_text_mean_NN_distance,\n",
    "# del  PB_extracted_text_mean_NN_indes,\n",
    "# del  PBSMIS_texts_cls_NN,\n",
    "# del  PBSMIS_texts_mean_NN,\n",
    "# del  PBSMIS_extracted_text_cls_NN,\n",
    "# del  PBSMIS_extracted_text_mean_NN,\n",
    "# del  PBSMIS_texts_cls_NN_distance,\n",
    "# del  PBSMIS_texts_cls_NN_indes,\n",
    "# del  PBSMIS_texts_mean_NN_distance,\n",
    "# del  PBSMIS_texts_mean_NN_indes,\n",
    "# del  PBSMIS_extracted_text_cls_NN_distance,\n",
    "# del  PBSMIS_extracted_text_cls_NN_indes,\n",
    "# del  PBSMIS_extracted_text_mean_NN_distance,\n",
    "# del  PBSMIS_extracted_text_mean_NN_indes,\n",
    "# del  PB_cls_KM,\n",
    "# del  PB_mean_KM,\n",
    "# del  PB_extracted_text_cls_KM,\n",
    "# del  PB_extracted_text_mean_KM,\n",
    "# del  PB_cls_KM_index,\n",
    "# del  PB_mean_KM_index,\n",
    "# del  PB_extracted_text_cls_KM_index,\n",
    "# del  PB_extracted_text_mean_KM_index,\n",
    "# del  PB_cls_match_index,\n",
    "# del  PB_mean_match_index,\n",
    "# del  PB_et_cls_match_index,\n",
    "# del  PB_et_mean_match_index,\n",
    "# del  PB_x_cls,\n",
    "# del  PB_x_mean,\n",
    "# del  PB_x_et_cls,\n",
    "# del  PB_x_et_mean,\n",
    "# del  PBSMIS_cls_KM,\n",
    "# del  PBSMIS_mean_KM,\n",
    "# del  PBSMIS_extracted_text_cls_KM,\n",
    "# del  PBSMIS_extracted_text_mean_KM,\n",
    "# del  PBSMIS_cls_KM_index,\n",
    "# del  PBSMIS_mean_KM_index,\n",
    "# del  PBSMIS_extracted_text_cls_KM_index,\n",
    "# del  PBSMIS_extracted_text_mean_KM_index,\n",
    "# del  PBSMIS_cls_match_index,\n",
    "# del  PBSMIS_mean_match_index,\n",
    "# del  PBSMIS_et_cls_match_index,\n",
    "# del  PBSMIS_et_mean_match_index,\n",
    "# del  PBSMIS_x_cls,\n",
    "# del  PBSMIS_x_mean,\n",
    "# del  PBSMIS_x_et_cls,\n",
    "# del  PBSMIS_x_et_mean,\n",
    "# del  SentenceTransformer,\n",
    "# del  SB_embeddings,\n",
    "# del  SB_extracted_embeddings,\n",
    "# del  K,\n",
    "# del  SB_texts_match_indices,\n",
    "# del  SB_texts_match_scores,\n",
    "# del  SB_texts_match_correct_total,\n",
    "# del  SB_extracted_match_indices,\n",
    "# del  SB_extracted_match_scores,\n",
    "# del  SB_extracted_match_correct_total,\n",
    "# del  SB_texts_k1_match_indices,\n",
    "# del  SB_texts_k1_match_scores,\n",
    "# del  SB_texts_k1_match_correct_total,\n",
    "# del  SB_extracted_k1_match_indices,\n",
    "# del  SB_extracted_k1_match_scores,\n",
    "# del  SB_extracted_k1_match_correct_total,\n",
    "# del  SB_texts_mean_NN,\n",
    "# del  SB_extracted_text_mean_NN,\n",
    "# del  SB_emb_distance,\n",
    "# del  SB_emb_index,\n",
    "# del  SB_et_distance,\n",
    "# del  SB_et_index,\n",
    "# del  SBSMIS_texts_mean_NN,\n",
    "# del  SBSMIS_extracted_text_mean_NN,\n",
    "# del  SBSMIS_emb_distance,\n",
    "# del  SBSMIS_emb_index,\n",
    "# del  SBSMIS_et_distance,\n",
    "# del  SBSMIS_et_index,\n",
    "# del  SB_cls_KM,\n",
    "# del  SB_extracted_text_KM,\n",
    "# del  SB_cls,\n",
    "# del  SB_extracted_text_cls,\n",
    "# del  SB_cls_index,\n",
    "# del  SB_x_cls,\n",
    "# del  SB_et,\n",
    "# del  SBSMIS_cls_KM,\n",
    "# del  SBSMIS_extracted_text_KM,\n",
    "# del  SBSMIS_cls,\n",
    "# del  SBSMIS_extracted_text_cls,\n",
    "# del  SBSMIS_cls_index,\n",
    "# del  SBSMIS_x_cls,\n",
    "# del  SBSMIS_et,\n",
    "# del  OpenAIGPTModel,\n",
    "# del  get_GPT_embedding,\n",
    "# del  GPT_cls_embedding1,\n",
    "# del  GPT_mean_embedding1,\n",
    "# del  GPT_cls_embedding2,\n",
    "# del  GPT_mean_embedding2,\n",
    "# del  GPT_cls_embedding3,\n",
    "# del  GPT_mean_embedding3,\n",
    "# del  GPT_cls_embedding4,\n",
    "# del  GPT_mean_embedding4,\n",
    "# del  GPT_cls_embedding,\n",
    "# del  GPT_mean_embedding,\n",
    "# del  GPT_extracted_text_cls_embedding1,\n",
    "# del  GPT_extracted_text_mean_embedding1,\n",
    "# del  GPT_extracted_text_cls_embedding2,\n",
    "# del  GPT_extracted_text_mean_embedding2,\n",
    "# del  GPT_extracted_text_cls_embedding3,\n",
    "# del  GPT_extracted_text_mean_embedding3,\n",
    "# del  GPT_extracted_text_cls_embedding4,\n",
    "# del  GPT_extracted_text_mean_embedding4,\n",
    "# del  GPT_extracted_text_cls_embedding,\n",
    "# del  GPT_extracted_text_mean_embedding,\n",
    "# del  GPT_texts_match_indices,\n",
    "# del  GPT_texts_match_scores,\n",
    "# del  GPT_texts_match_correct_total,\n",
    "# del  GPT_texts_mean_match_indices,\n",
    "# del  GPT_texts_mean_match_scores,\n",
    "# del  GPT_texts_mean_match_correct_total,\n",
    "# del  GPT_extracted_text_match_indices,\n",
    "# del  GPT_extracted_text_match_scores,\n",
    "# del  GPTextracted_text_match_correct_total,\n",
    "# del  GPT_extracted_text_mean_match_indices,\n",
    "# del  GPT_extracted_text_mean_match_scores,\n",
    "# del  GPT_extracted_text_mean_match_correct_total,\n",
    "# del  GPT_texts_k1_match_indices,\n",
    "# del  GPT_texts_k1_match_scores,\n",
    "# del  GPT_texts_k1_match_correct_total,\n",
    "# del  GPT_texts_K1_mean_match_indices,\n",
    "# del  GPT_texts_k1_mean_match_scores,\n",
    "# del  GPT_texts_k1_mean_match_correct_total,\n",
    "# del  GPT_extracted_k1_text_match_indices,\n",
    "# del  GPT_extracted_k1_text_match_scores,\n",
    "# del  GPT_extracted_k1_text_match_correct_total,\n",
    "# del  GPT_extracted_k1_text_mean_match_indices,\n",
    "# del  GPT_extracted_k1_text_mean_match_scores,\n",
    "# del  GPT_extracted_text_k1_mean_match_correct_total,\n",
    "# del  GPT_texts_cls_NN,\n",
    "# del  GPT_texts_mean_NN,\n",
    "# del  GPT_extracted_text_cls_NN,\n",
    "# del  GPT_extracted_text_mean_NN,\n",
    "# del  CPT_texts_cls_NN_distance,\n",
    "# del  GPT_texts_cls_NN_indes,\n",
    "# del  GPT_texts_mean_NN_distance,\n",
    "# del  GPT_texts_mean_NN_indes,\n",
    "# del  GPT_extracted_text_cls_NN_distance,\n",
    "# del  GPT_extracted_text_cls_NN_indes,\n",
    "# del  GPT_extracted_text_mean_NN_distance,\n",
    "# del  GPT_extracted_text_mean_NN_indes,\n",
    "# del  GPTSMIS_texts_cls_NN,\n",
    "# del  GPTSMIS_texts_mean_NN,\n",
    "# del  GPTSMIS_extracted_text_cls_NN,\n",
    "# del  GPTSMIS_extracted_text_mean_NN,\n",
    "# del  CPTSMIS_texts_cls_NN_distance,\n",
    "# del  GPTSMIS_texts_cls_NN_indes,\n",
    "# del  GPTSMIS_texts_mean_NN_distance,\n",
    "# del  GPTSMIS_texts_mean_NN_indes,\n",
    "# del  GPTSMIS_extracted_text_cls_NN_distance,\n",
    "# del  GPTSMIS_extracted_text_cls_NN_indes,\n",
    "# del  GPTSMIS_extracted_text_mean_NN_distance,\n",
    "# del  GPTSMIS_extracted_text_mean_NN_indes,\n",
    "# del  GPT_cls_KM,\n",
    "# del  GPT_mean_KM,\n",
    "# del  GPT_extracted_text_cls_KM,\n",
    "# del  GPT_extracted_text_mean_KM,\n",
    "# del  GPT_cls_KM_index,\n",
    "# del  GPT_mean_KM_index,\n",
    "# del  GPT_extracted_text_cls_KM_index,\n",
    "# del  GPT_extracted_text_mean_KM_index,\n",
    "# del  GPT_cls_match_index,\n",
    "# del  GPT_mean_match_index,\n",
    "# del  GPT_et_cls_match_index,\n",
    "# del  GPT_et_mean_match_index,\n",
    "# del  GPT_x_cls,\n",
    "# del  GPT_x_mean,\n",
    "# del  GPT_x_et_cls,\n",
    "# del  GPT_x_et_mean,\n",
    "# del  GPTSMIS_cls_KM,\n",
    "# del  GPTSMIS_mean_KM,\n",
    "# del  GPTSMIS_extracted_text_cls_KM,\n",
    "# del  GPTSMIS_extracted_text_mean_KM,\n",
    "# del  GPTSMIS_cls_KM_index,\n",
    "# del  GPTSMIS_mean_KM_index,\n",
    "# del  GPTSMIS_extracted_text_cls_KM_index,\n",
    "# del  GPTSMIS_extracted_text_mean_KM_index,\n",
    "# del  GPTSMIS_cls_match_index,\n",
    "# del  GPTSMIS_mean_match_index,\n",
    "# del  GPTSMIS_et_cls_match_index,\n",
    "# del  GPTSMIS_et_mean_match_index,\n",
    "# del  GPTSMIS_x_cls,\n",
    "# del  GPTSMIS_x_mean,\n",
    "# del  GPTSMIS_x_et_cls,\n",
    "# del  GPTSMIS_x_et_mean,\n",
    "# del  Last_GPTembedding,\n",
    "# del  LastGPT_embeddings1,\n",
    "# del  LastGPT_embeddings2,\n",
    "# del  LastGPT_embeddings3,\n",
    "# del  LastGPT_embeddings4,\n",
    "# del  LastGPT_embeddings,\n",
    "# del  LastGPT_et_embeddings1,\n",
    "# del  LastGPT_et_embeddings2,\n",
    "# del  LastGPT_et_embeddings3,\n",
    "# del  LastGPT_et_embeddings4,\n",
    "# del  LastGPT_et_embeddings,\n",
    "# del  LastGPT_texts_match_indices,\n",
    "# del  LastGPT_texts_match_scores,\n",
    "# del  LastGPT_texts_match_correct_total,\n",
    "# del  LastGPT_extracted_text_match_indices,\n",
    "# del  LastGPT_extracted_text_match_scores,\n",
    "# del  LastGPT_extracted_text_match_correct_total,\n",
    "# del  LastGPT_texts_k1_match_indices,\n",
    "# del  LastGPT_extracted_k1_text_match_indices,\n",
    "# del  LastGPT_texts_cls_NN,\n",
    "# del  LastGPT_extracted_text_cls_NN,\n",
    "# del  LastGPT_texts_cls_NN_distance,\n",
    "# del  LastGPT_texts_cls_NN_indes,\n",
    "# del  LastGPT_extracted_text_cls_NN_distance,\n",
    "# del  LastGPT_extracted_text_cls_NN_indes,\n",
    "# del  LastGPT_cls_KM,\n",
    "# del  LastGPT_extracted_text_cls_KM,\n",
    "# del  LastGPT_cls_KM_index,\n",
    "# del  LastGPT_extracted_text_cls_KM_index,\n",
    "# del  LastGPT_cls_match_index,\n",
    "# del  LastGPT_et_cls_match_index,\n",
    "# del  LastGPT_x_cls,\n",
    "# del  LastGPT_x_et_cls,\n",
    "# del  LastGPTSMIS_texts_cls_NN,\n",
    "# del  LastGPTSMIS_extracted_text_cls_NN,\n",
    "# del  LastGPTSMIS_texts_cls_NN_distance,\n",
    "# del  LastGPTSMIS_texts_cls_NN_indes,\n",
    "# del  LastGPTSMIS_extracted_text_cls_NN_distance,\n",
    "# del  LastGPTSMIS_extracted_text_cls_NN_indes,\n",
    "# del  LastGPTSMIS_cls_KM,\n",
    "# del  LastGPTSMIS_extracted_text_cls_KM,\n",
    "# del  LastGPTSMIS_cls_KM_index,\n",
    "# del  LastGPTSMIS_extracted_text_cls_KM_index,\n",
    "# del  LastGPTSMIS_cls_match_index,\n",
    "# del  LastGPTSMIS_et_cls_match_index,\n",
    "# del  LastGPTSMIS_x_cls,\n",
    "# del  LastGPTSMIS_x_et_cls,\n",
    "# del  Summarise_GPTembedding,\n",
    "# del  LastSGPT_embeddings1,\n",
    "# del  LastSGPT_embeddings2,\n",
    "# del  LastSGPT_embeddings3,\n",
    "# del  LastSGPT_embeddings4,\n",
    "# del  LastSGPT_embeddings,\n",
    "# del  LastSGPT_et_embeddings1,\n",
    "# del  LastSGPT_et_embeddings2,\n",
    "# del  LastSGPT_et_embeddings3,\n",
    "# del  LastSGPT_et_embeddings4,\n",
    "# del  LastSGPT_et_embeddings,\n",
    "# del  LastSGPT_texts_match_indices,\n",
    "# del  LastSGPT_texts_match_scores,\n",
    "# del  LastSGPT_texts_match_correct_total,\n",
    "# del  LastSGPT_extracted_text_match_indices,\n",
    "# del  LastSGPT_extracted_text_match_scores,\n",
    "# del  LastSGPT_extracted_text_match_correct_total,\n",
    "# del  LastSGPT_texts_k1_match_indices,\n",
    "# del  LastSGPT_extracted_k1_text_match_indices,\n",
    "# del  LastSGPT_texts_cls_NN,\n",
    "# del  LastSGPT_extracted_text_cls_NN,\n",
    "# del  LastSGPT_texts_cls_NN_distance,\n",
    "# del  LastSGPT_texts_cls_NN_indes,\n",
    "# del  LastSGPT_extracted_text_cls_NN_distance,\n",
    "# del  LastSGPT_extracted_text_cls_NN_indes,\n",
    "# del  LastSGPT_cls_KM,\n",
    "# del  LastSGPT_extracted_text_cls_KM,\n",
    "# del  LastSGPT_cls_KM_index,\n",
    "# del  LastSGPT_extracted_text_cls_KM_index,\n",
    "# del  LastSGPT_cls_match_index,\n",
    "# del  LastSGPT_et_cls_match_index,\n",
    "# del  LastSGPT_x_cls,\n",
    "# del  LastSGPT_x_et_cls,\n",
    "# del  LastSGPTSMIS_texts_cls_NN,\n",
    "# del  LastSGPTSMIS_extracted_text_cls_NN,\n",
    "# del  LastSGPTSMIS_texts_cls_NN_distance,\n",
    "# del  LastSGPTSMIS_texts_cls_NN_indes,\n",
    "# del  LastSGPTSMIS_extracted_text_cls_NN_distance,\n",
    "# del  LastSGPTSMIS_extracted_text_cls_NN_indes,\n",
    "# del  LastSGPTSMIS_cls_KM,\n",
    "# del  LastSGPTSMIS_extracted_text_cls_KM,\n",
    "# del  LastSGPTSMIS_cls_KM_index,\n",
    "# del  LastSGPTSMIS_extracted_text_cls_KM_index,\n",
    "# del  LastSGPTSMIS_cls_match_index,\n",
    "# del  LastSGPTSMIS_et_cls_match_index,\n",
    "# del  LastSGPTSMIS_x_cls,\n",
    "# del  LastSGPTSMIS_x_et_cls,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AutoTokenizer,\n",
    "del OpenAIGPTModel,\n",
    "del Summarise_GPTembedding,\n",
    "del LastSGPT_embeddings1,\n",
    "del LastSGPT_embeddings2,\n",
    "del LastSGPT_embeddings3,\n",
    "del LastSGPT_embeddings4,\n",
    "del LastSGPT_embeddings,\n",
    "del LastSGPT_et_embeddings1,\n",
    "del LastSGPT_et_embeddings2,\n",
    "del LastSGPT_et_embeddings3,\n",
    "del LastSGPT_et_embeddings4,\n",
    "del LastSGPT_et_embeddings,\n",
    "del K,\n",
    "del LastSGPT_texts_match_indices,\n",
    "del LastSGPT_texts_match_scores,\n",
    "del LastSGPT_texts_match_correct_total,\n",
    "del LastSGPT_extracted_text_match_indices,\n",
    "del LastSGPT_extracted_text_match_scores,\n",
    "del LastSGPT_extracted_text_match_correct_total,\n",
    "del LastSGPT_texts_k1_match_indices,\n",
    "del LastSGPT_extracted_k1_text_match_indices,\n",
    "del LastSGPT_texts_cls_NN,\n",
    "del LastSGPT_extracted_text_cls_NN,\n",
    "del LastSGPT_texts_cls_NN_distance,\n",
    "del LastSGPT_texts_cls_NN_indes,\n",
    "del LastSGPT_extracted_text_cls_NN_distance,\n",
    "del LastSGPT_extracted_text_cls_NN_indes,\n",
    "del LastSGPT_cls_KM,\n",
    "del LastSGPT_extracted_text_cls_KM,\n",
    "del LastSGPT_cls_KM_index,\n",
    "del LastSGPT_extracted_text_cls_KM_index,\n",
    "del LastSGPT_cls_match_index,\n",
    "del LastSGPT_et_cls_match_index,\n",
    "del LastSGPT_x_cls,\n",
    "del LastSGPT_x_et_cls,\n",
    "del LastSGPTSMIS_texts_cls_NN,\n",
    "del LastSGPTSMIS_extracted_text_cls_NN,\n",
    "del LastSGPTSMIS_texts_cls_NN_distance,\n",
    "del LastSGPTSMIS_texts_cls_NN_indes,\n",
    "del LastSGPTSMIS_extracted_text_cls_NN_distance,\n",
    "del LastSGPTSMIS_extracted_text_cls_NN_indes,\n",
    "del LastSGPTSMIS_cls_KM,\n",
    "del LastSGPTSMIS_extracted_text_cls_KM,\n",
    "del LastSGPTSMIS_cls_KM_index,\n",
    "del LastSGPTSMIS_extracted_text_cls_KM_index,\n",
    "del LastSGPTSMIS_cls_match_index,\n",
    "del LastSGPTSMIS_et_cls_match_index,\n",
    "del LastSGPTSMIS_x_cls,\n",
    "del LastSGPTSMIS_x_et_cls,\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "    input_ids = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "\n",
    "    # Get the hidden states (outputs from each layer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # # Optionally, move the result back to the CPU if needed\n",
    "    # sentence_embedding = sentence_embedding.cpu().detach()\n",
    "\n",
    "    # print(sentence_embedding.shape)\n",
    "    # return sentence_embedding\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2_cls_embedding,GPT2_mean_embedding=GPT2embedding(texts)\n",
    "# GPT2_extracted_text_cls_embedding,GPT2_extracted_text_mean_embedding=GPT2embedding(extracted_text)\n",
    "GPT2_cls_embedding1,GPT2_mean_embedding1=GPT2embedding(texts[:1000])\n",
    "GPT2_cls_embedding2,GPT2_mean_embedding2=GPT2embedding(texts[1000:2000])\n",
    "GPT2_cls_embedding3,GPT2_mean_embedding3=GPT2embedding(texts[2000:3000])\n",
    "GPT2_cls_embedding4,GPT2_mean_embedding4=GPT2embedding(texts[3000:])\n",
    "\n",
    "GPT2_cls_embedding=torch.cat((GPT2_cls_embedding1,GPT2_cls_embedding2,GPT2_cls_embedding3,GPT2_cls_embedding4))\n",
    "GPT2_mean_embedding=torch.cat((GPT2_mean_embedding1,GPT2_mean_embedding2,GPT2_mean_embedding3,GPT2_mean_embedding4))\n",
    "\n",
    "GPT2_extracted_text_cls_embedding1,GPT2_extracted_text_mean_embedding1=GPT2embedding(extracted_text[:1000])\n",
    "GPT2_extracted_text_cls_embedding2,GPT2_extracted_text_mean_embedding2=GPT2embedding(extracted_text[1000:2000])\n",
    "GPT2_extracted_text_cls_embedding3,GPT2_extracted_text_mean_embedding3=GPT2embedding(extracted_text[2000:3000])\n",
    "GPT2_extracted_text_cls_embedding4,GPT2_extracted_text_mean_embedding4=GPT2embedding(extracted_text[3000:])\n",
    "\n",
    "GPT2_extracted_text_cls_embedding=torch.cat((GPT2_extracted_text_cls_embedding1,GPT2_extracted_text_cls_embedding2,GPT2_extracted_text_cls_embedding3,GPT2_extracted_text_cls_embedding4))\n",
    "GPT2_extracted_text_mean_embedding=torch.cat((GPT2_extracted_text_mean_embedding1,GPT2_extracted_text_mean_embedding2,GPT2_extracted_text_mean_embedding3,GPT2_extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(GPT2_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT2_mean_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT2_extracted_text_cls_embedding,sim_embeddings.pooler_output),de_biased_cka(GPT2_extracted_text_mean_embedding,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(GPT2_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT2_mean_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT2_extracted_text_cls_embedding.cpu(),sim_embeddings.pooler_output),CKA(GPT2_extracted_text_mean_embedding.cpu(),sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we have cls mean embedding of texts and extracted_text\n",
    "cls_embedding,\n",
    "\n",
    "mean_embedding,\n",
    "\n",
    "extracted_text_cls_embedding,\n",
    "\n",
    "extracted_text_mean_embedding,\n",
    "\n",
    "#### exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_texts_match_indices, GPT2_texts_match_scores, GPT2_texts_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT2_cls_embedding.cpu().detach(), K=10)\n",
    "GPT2_texts_mean_match_indices, GPT2_texts_mean_match_scores, GPT2_texts_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT2_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", GPT2_texts_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT2_texts_mean_match_correct_total)\n",
    "GPT2_extracted_text_match_indices, GPT2_extracted_text_match_scores, GPT2_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT2_extracted_text_cls_embedding.cpu().detach(), K=10)\n",
    "GPT2_extracted_text_mean_match_indices, GPT2_extracted_text_mean_match_scores, GPT2_extracted_text_mean_match_correct_total = retrieve(sim_embeddings.pooler_output, GPT2_extracted_text_mean_embedding.cpu().detach(), K=10)\n",
    "\n",
    "print(\"正确匹配的条目数：\", GPT2_extracted_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT2_extracted_text_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_texts_k1_match_indices, GPT2_texts_k1_match_scores, GPT2_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT2_cls_embedding.cpu().detach(), K=1)\n",
    "GPT2_texts_K1_mean_match_indices, GPT2_texts_k1_mean_match_scores, GPT2_texts_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT2_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", GPT2_texts_k1_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", GPT2_texts_k1_mean_match_correct_total)\n",
    "\n",
    "\n",
    "extracted_k1_text_match_indices, extracted_k1_text_match_scores, extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT2_extracted_text_cls_embedding.cpu().detach(), K=1)\n",
    "extracted_k1_text_mean_match_indices, extracted_k1_text_mean_match_scores, extracted_text_k1_mean_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), GPT2_extracted_text_mean_embedding.cpu().detach(), K=1)\n",
    "\n",
    "print(\"正确匹配的条目数：\", extracted_k1_text_match_correct_total)\n",
    "print(\"正确匹配的条目数：\", extracted_text_k1_mean_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_texts_cls_NN=NN(embedding=GPT2_cls_embedding)\n",
    "GPT2_texts_mean_NN=NN(embedding=GPT2_mean_embedding)\n",
    "GPT2_extracted_text_cls_NN=NN(embedding=GPT2_extracted_text_cls_embedding)\n",
    "GPT2_extracted_text_mean_NN=NN(embedding=GPT2_extracted_text_mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_texts_cls_NN_distance,GPT2_texts_cls_NN_indes=GPT2_texts_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "GPT2_texts_mean_NN_distance,GPT2_texts_mean_NN_indes=GPT2_texts_mean_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "\n",
    "GPT2_extracted_text_cls_NN_distance,GPT2_extracted_text_cls_NN_indes=GPT2_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output)\n",
    "GPT2_extracted_text_mean_NN_distance,GPT2_extracted_text_mean_NN_indes=GPT2_extracted_text_mean_NN.kneighbors(sim_embeddings.pooler_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(GPT2_texts_match_indices.cpu().numpy(), GPT2_texts_cls_NN_indes)\n",
    "\n",
    "overleap(GPT2_texts_mean_match_indices.cpu().numpy(), GPT2_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT2_texts_match_indices.cpu().numpy(), GPT2_texts_mean_NN_indes)\n",
    "overleap(GPT2_texts_mean_match_indices.cpu().numpy(), GPT2_texts_cls_NN_indes)\n",
    "print('='*10)\n",
    "overleap(GPT2_extracted_text_match_indices.cpu().numpy(), GPT2_extracted_text_cls_NN_indes)\n",
    "overleap(GPT2_extracted_text_mean_match_indices.cpu().numpy(), GPT2_extracted_text_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT2_extracted_text_mean_match_indices.cpu().numpy(), GPT2_extracted_text_cls_NN_indes)\n",
    "overleap(GPT2_extracted_text_match_indices.cpu().numpy(), GPT2_extracted_text_mean_NN_indes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPT2SMIS_texts_mean_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPT2SMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "GPT2SMIS_extracted_text_mean_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_texts_cls_NN_distance,GPT2SMIS_texts_cls_NN_indes=GPT2SMIS_texts_cls_NN.kneighbors(GPT2_cls_embedding.cpu().numpy())\n",
    "GPT2SMIS_texts_mean_NN_distance,GPT2SMIS_texts_mean_NN_indes=GPT2SMIS_texts_mean_NN.kneighbors(GPT2_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPT2SMIS_extracted_text_cls_NN_distance,GPT2SMIS_extracted_text_cls_NN_indes=GPT2SMIS_extracted_text_cls_NN.kneighbors(GPT2_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPT2SMIS_extracted_text_mean_NN_distance,GPT2SMIS_extracted_text_mean_NN_indes=GPT2SMIS_extracted_text_mean_NN.kneighbors(GPT2_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_texts_match_indices.shape,GPT2SMIS_texts_cls_NN_indes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(GPT2_texts_match_indices.cpu().numpy(), GPT2SMIS_texts_cls_NN_indes)\n",
    "overleap(GPT2_texts_mean_match_indices.cpu().numpy(), GPT2SMIS_texts_mean_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(GPT2_texts_match_indices.cpu().numpy(), GPT2SMIS_texts_mean_NN_indes)\n",
    "overleap(GPT2_texts_mean_match_indices.cpu().numpy(), GPT2SMIS_texts_cls_NN_indes)\n",
    "print('='*20)\n",
    "overleap(GPT2_extracted_text_match_indices.cpu().numpy(), GPT2SMIS_extracted_text_cls_NN_indes)\n",
    "overleap(GPT2_extracted_text_mean_match_indices.cpu().numpy(), GPT2SMIS_extracted_text_mean_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "overleap(GPT2_extracted_text_mean_match_indices.cpu().numpy(), GPT2SMIS_extracted_text_cls_NN_indes)\n",
    "overleap(GPT2_extracted_text_match_indices.cpu().numpy(), GPT2SMIS_extracted_text_mean_NN_indes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_cls_KM=KM(GPT2_cls_embedding.cpu().numpy())\n",
    "GPT2_mean_KM=KM(GPT2_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPT2_extracted_text_cls_KM=KM(GPT2_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPT2_extracted_text_mean_KM=KM(GPT2_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_cls_KM_index=GPT2_cls_KM.predict(GPT2_cls_embedding.cpu().numpy())\n",
    "GPT2_mean_KM_index=GPT2_mean_KM.predict(GPT2_mean_embedding.cpu().numpy())\n",
    "\n",
    "GPT2_extracted_text_cls_KM_index=GPT2_extracted_text_cls_KM.predict(GPT2_extracted_text_cls_embedding.cpu().numpy())\n",
    "GPT2_extracted_text_mean_KM_index=GPT2_extracted_text_mean_KM.predict(GPT2_extracted_text_mean_embedding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_cls_match_index=GPT2_cls_KM.predict(GPT2_cls_embedding[GPT2_texts_k1_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2_mean_match_index=GPT2_mean_KM.predict(GPT2_mean_embedding[GPT2_texts_K1_mean_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2_et_cls_match_index=GPT2_extracted_text_cls_KM.predict(GPT2_extracted_text_cls_embedding[extracted_k1_text_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2_et_mean_match_index=GPT2_extracted_text_mean_KM.predict(GPT2_extracted_text_mean_embedding[extracted_k1_text_mean_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_x_cls=GPT2_cls_match_index==GPT2_cls_KM_index\n",
    "GPT2_x_mean=GPT2_mean_KM_index==GPT2_mean_match_index\n",
    "GPT2_x_et_cls=GPT2_extracted_text_cls_KM_index==GPT2_et_cls_match_index\n",
    "GPT2_x_et_mean=GPT2_et_mean_match_index==GPT2_extracted_text_mean_KM_index\n",
    "np.sum(GPT2_x_cls),np.sum(GPT2_x_mean),np.sum(GPT2_x_et_cls),np.sum(GPT2_x_et_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smis K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPT2SMIS_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "GPT2SMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPT2SMIS_extracted_text_mean_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_cls_KM_index=GPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPT2SMIS_mean_KM_index=GPT2SMIS_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "\n",
    "GPT2SMIS_extracted_text_cls_KM_index=GPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "GPT2SMIS_extracted_text_mean_KM_index=GPT2SMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_cls_match_index=GPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output[GPT2_texts_k1_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2SMIS_mean_match_index=GPT2SMIS_mean_KM.predict(sim_embeddings.pooler_output[GPT2_texts_K1_mean_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2SMIS_et_cls_match_index=GPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[extracted_k1_text_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())\n",
    "GPT2SMIS_et_mean_match_index=GPT2SMIS_extracted_text_mean_KM.predict(sim_embeddings.pooler_output[extracted_k1_text_mean_match_indices].reshape(GPT2_cls_embedding.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2SMIS_x_cls=GPT2SMIS_cls_match_index==GPT2SMIS_cls_KM_index\n",
    "GPT2SMIS_x_mean=GPT2SMIS_mean_KM_index==GPT2SMIS_mean_match_index\n",
    "GPT2SMIS_x_et_cls=GPT2SMIS_extracted_text_cls_KM_index==GPT2SMIS_et_cls_match_index\n",
    "GPT2SMIS_x_et_mean=GPT2SMIS_et_mean_match_index==GPT2SMIS_extracted_text_mean_KM_index\n",
    "np.sum(GPT2SMIS_x_cls),np.sum(GPT2SMIS_x_mean),np.sum(GPT2SMIS_x_et_cls),np.sum(GPT2SMIS_x_et_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(globals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del GPT2embedding,\n",
    "del GPT2_cls_embedding1,\n",
    "del GPT2_mean_embedding1,\n",
    "del GPT2_cls_embedding2,\n",
    "del GPT2_mean_embedding2,\n",
    "del GPT2_cls_embedding3,\n",
    "del GPT2_mean_embedding3,\n",
    "del GPT2_cls_embedding4,\n",
    "del GPT2_mean_embedding4,\n",
    "del GPT2_cls_embedding,\n",
    "del GPT2_mean_embedding,\n",
    "del GPT2_extracted_text_cls_embedding1,\n",
    "del GPT2_extracted_text_mean_embedding1,\n",
    "del GPT2_extracted_text_cls_embedding2,\n",
    "del GPT2_extracted_text_mean_embedding2,\n",
    "del GPT2_extracted_text_cls_embedding3,\n",
    "del GPT2_extracted_text_mean_embedding3,\n",
    "del GPT2_extracted_text_cls_embedding4,\n",
    "del GPT2_extracted_text_mean_embedding4,\n",
    "del GPT2_extracted_text_cls_embedding,\n",
    "del GPT2_extracted_text_mean_embedding,\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def LastGPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT2_embeddings=LastGPT2embedding(texts)\n",
    "LastGPT2_embeddings1=LastGPT2embedding(texts[:1000])\n",
    "LastGPT2_embeddings2=LastGPT2embedding(texts[1000:2000])\n",
    "LastGPT2_embeddings3=LastGPT2embedding(texts[2000:3000])\n",
    "LastGPT2_embeddings4=LastGPT2embedding(texts[3000:])\n",
    "\n",
    "LastGPT2_embeddings=torch.cat((LastGPT2_embeddings1,LastGPT2_embeddings2,LastGPT2_embeddings3,LastGPT2_embeddings4))\n",
    "LastGPT2_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastGPT2_et_embeddings=LastGPT2embedding(extracted_text)\n",
    "LastGPT2_et_embeddings1=LastGPT2embedding(extracted_text[:1000])\n",
    "LastGPT2_et_embeddings2=LastGPT2embedding(extracted_text[1000:2000])\n",
    "LastGPT2_et_embeddings3=LastGPT2embedding(extracted_text[2000:3000])\n",
    "LastGPT2_et_embeddings4=LastGPT2embedding(extracted_text[3000:])\n",
    "\n",
    "LastGPT2_et_embeddings=torch.cat((LastGPT2_et_embeddings1,LastGPT2_et_embeddings2,LastGPT2_et_embeddings3,LastGPT2_et_embeddings4))\n",
    "\n",
    "LastGPT2_et_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_embeddings.shape,LastGPT2_et_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastGPT2_embeddings,sim_embeddings.pooler_output),de_biased_cka(LastGPT2_et_embeddings,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(LastGPT2_embeddings,sim_embeddings.pooler_output.to(device)),CKA(LastGPT2_et_embeddings,sim_embeddings.pooler_output.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "\n",
    "LastGPT2_texts_match_indices, LastGPT2_texts_match_scores, LastGPT2_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT2_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastGPT2_texts_match_correct_total)\n",
    "LastGPT2_extracted_text_match_indices, LastGPT2_extracted_text_match_scores, LastGPT2_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT2_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastGPT2_extracted_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "\n",
    "LastGPT2_texts_k1_match_indices, LastGPT2_texts_match_scores, LastGPT2_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT2_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastGPT2_texts_k1_match_correct_total)\n",
    "LastGPT2_extracted_k1_text_match_indices, LastGPT2_extracted_text_match_scores, LastGPT2_extracted_text_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastGPT2_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastGPT2_extracted_text_k1_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_texts_cls_NN=NN(embedding=LastGPT2_embeddings)\n",
    "LastGPT2_extracted_text_cls_NN=NN(embedding=LastGPT2_et_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_texts_cls_NN_distance,LastGPT2_texts_cls_NN_indes=LastGPT2_texts_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())\n",
    "\n",
    "LastGPT2_extracted_text_cls_NN_distance,LastGPT2_extracted_text_cls_NN_indes=LastGPT2_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastGPT2_texts_match_indices.cpu().numpy(), LastGPT2_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastGPT2_extracted_text_match_indices.cpu().numpy(), LastGPT2_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_cls_KM=KM(LastGPT2_embeddings.cpu().numpy())\n",
    "LastGPT2_extracted_text_cls_KM=KM(LastGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_cls_KM_index=LastGPT2_cls_KM.predict(LastGPT2_embeddings.cpu().numpy())\n",
    "LastGPT2_extracted_text_cls_KM_index=LastGPT2_extracted_text_cls_KM.predict(LastGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_cls_match_index=LastGPT2_cls_KM.predict(LastGPT2_embeddings[LastGPT2_texts_k1_match_indices].reshape(LastGPT2_embeddings.shape).cpu().numpy())\n",
    "LastGPT2_et_cls_match_index=LastGPT2_extracted_text_cls_KM.predict(LastGPT2_et_embeddings[LastGPT2_extracted_k1_text_match_indices].reshape(LastGPT2_et_embeddings.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2_x_cls=LastGPT2_cls_match_index==LastGPT2_cls_KM_index\n",
    "LastGPT2_x_et_cls=LastGPT2_extracted_text_cls_KM_index==LastGPT2_et_cls_match_index\n",
    "\n",
    "np.sum(LastGPT2_x_cls),np.sum(LastGPT2_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(x_cls),np.sum(x_et_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "LastGPT2SMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_texts_cls_NN_distance,LastGPT2SMIS_texts_cls_NN_indes=LastGPT2SMIS_texts_cls_NN.kneighbors(LastGPT2_embeddings.cpu().numpy())\n",
    "\n",
    "LastGPT2SMIS_extracted_text_cls_NN_distance,LastGPT2SMIS_extracted_text_cls_NN_indes=LastGPT2SMIS_extracted_text_cls_NN.kneighbors(LastGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastGPT2_texts_match_indices.cpu().numpy(), LastGPT2SMIS_texts_cls_NN_indes)\n",
    "\n",
    "#_____________cross should get low score_____________\n",
    "\n",
    "overleap(LastGPT2_extracted_text_match_indices.cpu().numpy(), LastGPT2SMIS_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "LastGPT2SMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_cls_KM_index=LastGPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "LastGPT2SMIS_extracted_text_cls_KM_index=LastGPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_cls_match_index=LastGPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output[LastGPT2_texts_k1_match_indices].reshape(LastGPT2_embeddings.shape).cpu().numpy().astype(np.float32))\n",
    "LastGPT2SMIS_et_cls_match_index=LastGPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[LastGPT2_extracted_k1_text_match_indices].reshape(LastGPT2_et_embeddings.shape).cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastGPT2SMIS_x_cls=LastGPT2SMIS_cls_match_index==LastGPT2SMIS_cls_KM_index\n",
    "LastGPT2SMIS_x_et_cls=LastGPT2SMIS_extracted_text_cls_KM_index==LastGPT2SMIS_et_cls_match_index\n",
    "\n",
    "np.sum(LastGPT2SMIS_x_cls),np.sum(LastGPT2SMIS_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(x_cls),np.sum(x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastGPT2embedding,\n",
    "del LastGPT2_embeddings1,\n",
    "del LastGPT2_embeddings2,\n",
    "del LastGPT2_embeddings3,\n",
    "del LastGPT2_embeddings4,\n",
    "del LastGPT2_embeddings,\n",
    "del LastGPT2_et_embeddings1,\n",
    "del LastGPT2_et_embeddings2,\n",
    "del LastGPT2_et_embeddings3,\n",
    "del LastGPT2_et_embeddings4,\n",
    "del LastGPT2_et_embeddings,\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(globals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del GPT2Model,\n",
    "del GPT2Tokenizer,\n",
    "del GPT2_texts_match_indices,\n",
    "del GPT2_texts_match_scores,\n",
    "del GPT2_texts_match_correct_total,\n",
    "del GPT2_texts_mean_match_indices,\n",
    "del GPT2_texts_mean_match_scores,\n",
    "del GPT2_texts_mean_match_correct_total,\n",
    "del GPT2_extracted_text_match_indices,\n",
    "del GPT2_extracted_text_match_scores,\n",
    "del GPT2_extracted_text_match_correct_total,\n",
    "del GPT2_extracted_text_mean_match_indices,\n",
    "del GPT2_extracted_text_mean_match_scores,\n",
    "del GPT2_extracted_text_mean_match_correct_total,\n",
    "del GPT2_texts_k1_match_indices,\n",
    "del GPT2_texts_k1_match_scores,\n",
    "del GPT2_texts_k1_match_correct_total,\n",
    "del GPT2_texts_K1_mean_match_indices,\n",
    "del GPT2_texts_k1_mean_match_scores,\n",
    "del GPT2_texts_k1_mean_match_correct_total,\n",
    "del extracted_k1_text_match_indices,\n",
    "del extracted_k1_text_match_scores,\n",
    "del extracted_k1_text_match_correct_total,\n",
    "del extracted_k1_text_mean_match_indices,\n",
    "del extracted_k1_text_mean_match_scores,\n",
    "del extracted_text_k1_mean_match_correct_total,\n",
    "del GPT2_texts_cls_NN,\n",
    "del GPT2_texts_mean_NN,\n",
    "del GPT2_extracted_text_cls_NN,\n",
    "del GPT2_extracted_text_mean_NN,\n",
    "del GPT2_texts_cls_NN_distance,\n",
    "del GPT2_texts_cls_NN_indes,\n",
    "del GPT2_texts_mean_NN_distance,\n",
    "del GPT2_texts_mean_NN_indes,\n",
    "del GPT2_extracted_text_cls_NN_distance,\n",
    "del GPT2_extracted_text_cls_NN_indes,\n",
    "del GPT2_extracted_text_mean_NN_distance,\n",
    "del GPT2_extracted_text_mean_NN_indes,\n",
    "del GPT2SMIS_texts_cls_NN,\n",
    "del GPT2SMIS_texts_mean_NN,\n",
    "del GPT2SMIS_extracted_text_cls_NN,\n",
    "del GPT2SMIS_extracted_text_mean_NN,\n",
    "del GPT2SMIS_texts_cls_NN_distance,\n",
    "del GPT2SMIS_texts_cls_NN_indes,\n",
    "del GPT2SMIS_texts_mean_NN_distance,\n",
    "del GPT2SMIS_texts_mean_NN_indes,\n",
    "del GPT2SMIS_extracted_text_cls_NN_distance,\n",
    "del GPT2SMIS_extracted_text_cls_NN_indes,\n",
    "del GPT2SMIS_extracted_text_mean_NN_distance,\n",
    "del GPT2SMIS_extracted_text_mean_NN_indes,\n",
    "del GPT2_cls_KM,\n",
    "del GPT2_mean_KM,\n",
    "del GPT2_extracted_text_cls_KM,\n",
    "del GPT2_extracted_text_mean_KM,\n",
    "del GPT2_cls_KM_index,\n",
    "del GPT2_mean_KM_index,\n",
    "del GPT2_extracted_text_cls_KM_index,\n",
    "del GPT2_extracted_text_mean_KM_index,\n",
    "del GPT2_cls_match_index,\n",
    "del GPT2_mean_match_index,\n",
    "del GPT2_et_cls_match_index,\n",
    "del GPT2_et_mean_match_index,\n",
    "del GPT2_x_cls,\n",
    "del GPT2_x_mean,\n",
    "del GPT2_x_et_cls,\n",
    "del GPT2_x_et_mean,\n",
    "del GPT2SMIS_cls_KM,\n",
    "del GPT2SMIS_mean_KM,\n",
    "del GPT2SMIS_extracted_text_cls_KM,\n",
    "del GPT2SMIS_extracted_text_mean_KM,\n",
    "del GPT2SMIS_cls_KM_index,\n",
    "del GPT2SMIS_mean_KM_index,\n",
    "del GPT2SMIS_extracted_text_cls_KM_index,\n",
    "del GPT2SMIS_extracted_text_mean_KM_index,\n",
    "del GPT2SMIS_cls_match_index,\n",
    "del GPT2SMIS_mean_match_index,\n",
    "del GPT2SMIS_et_cls_match_index,\n",
    "del GPT2SMIS_et_mean_match_index,\n",
    "del GPT2SMIS_x_cls,\n",
    "del GPT2SMIS_x_mean,\n",
    "del GPT2SMIS_x_et_cls,\n",
    "del GPT2SMIS_x_et_mean,\n",
    "del K,\n",
    "del LastGPT2_texts_match_indices,\n",
    "del LastGPT2_texts_match_scores,\n",
    "del LastGPT2_texts_match_correct_total,\n",
    "del LastGPT2_extracted_text_match_indices,\n",
    "del LastGPT2_extracted_text_match_scores,\n",
    "del LastGPT2_extracted_text_match_correct_total,\n",
    "del LastGPT2_texts_k1_match_indices,\n",
    "del LastGPT2_texts_k1_match_correct_total,\n",
    "del LastGPT2_extracted_k1_text_match_indices,\n",
    "del LastGPT2_extracted_text_k1_match_correct_total,\n",
    "del LastGPT2_texts_cls_NN,\n",
    "del LastGPT2_extracted_text_cls_NN,\n",
    "del LastGPT2_texts_cls_NN_distance,\n",
    "del LastGPT2_texts_cls_NN_indes,\n",
    "del LastGPT2_extracted_text_cls_NN_distance,\n",
    "del LastGPT2_extracted_text_cls_NN_indes,\n",
    "del LastGPT2_cls_KM,\n",
    "del LastGPT2_extracted_text_cls_KM,\n",
    "del LastGPT2_cls_KM_index,\n",
    "del LastGPT2_extracted_text_cls_KM_index,\n",
    "del LastGPT2_cls_match_index,\n",
    "del LastGPT2_et_cls_match_index,\n",
    "del LastGPT2_x_cls,\n",
    "del LastGPT2_x_et_cls,\n",
    "del LastGPT2SMIS_texts_cls_NN,\n",
    "del LastGPT2SMIS_extracted_text_cls_NN,\n",
    "del LastGPT2SMIS_texts_cls_NN_distance,\n",
    "del LastGPT2SMIS_texts_cls_NN_indes,\n",
    "del LastGPT2SMIS_extracted_text_cls_NN_distance,\n",
    "del LastGPT2SMIS_extracted_text_cls_NN_indes,\n",
    "del LastGPT2SMIS_cls_KM,\n",
    "del LastGPT2SMIS_extracted_text_cls_KM,\n",
    "del LastGPT2SMIS_cls_KM_index,\n",
    "del LastGPT2SMIS_extracted_text_cls_KM_index,\n",
    "del LastGPT2SMIS_cls_match_index,\n",
    "del LastGPT2SMIS_et_cls_match_index,\n",
    "del LastGPT2SMIS_x_cls,\n",
    "del LastGPT2SMIS_x_et_cls,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def Summarise_GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT2_embeddings=Summarise_GPT2embedding(texts)\n",
    "LastSGPT2_embeddings1=Summarise_GPT2embedding(texts[:1000])\n",
    "LastSGPT2_embeddings2=Summarise_GPT2embedding(texts[1000:2000])\n",
    "LastSGPT2_embeddings3=Summarise_GPT2embedding(texts[2000:3000])\n",
    "LastSGPT2_embeddings4=Summarise_GPT2embedding(texts[3000:])\n",
    "\n",
    "LastSGPT2_embeddings=torch.cat((LastSGPT2_embeddings1,LastSGPT2_embeddings2,LastSGPT2_embeddings3,LastSGPT2_embeddings4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LastSGPT2_et_embeddings=Summarise_GPT2embedding(extracted_text)\n",
    "LastSGPT2_et_embeddings1=Summarise_GPT2embedding(extracted_text[:1000])\n",
    "LastSGPT2_et_embeddings2=Summarise_GPT2embedding(extracted_text[1000:2000])\n",
    "LastSGPT2_et_embeddings3=Summarise_GPT2embedding(extracted_text[2000:3000])\n",
    "LastSGPT2_et_embeddings4=Summarise_GPT2embedding(extracted_text[3000:])\n",
    "\n",
    "LastSGPT2_et_embeddings=torch.cat((LastSGPT2_et_embeddings1,LastSGPT2_et_embeddings2,LastSGPT2_et_embeddings3,LastSGPT2_et_embeddings4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_embeddings.shape,LastSGPT2_et_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(LastSGPT2_embeddings,sim_embeddings.pooler_output),de_biased_cka(LastSGPT2_et_embeddings,sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKA(LastSGPT2_embeddings,sim_embeddings.pooler_output.to(device)),CKA(LastSGPT2_et_embeddings,sim_embeddings.pooler_output.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "\n",
    "LastSGPT2_texts_match_indices, LastSGPT2_texts_match_scores, LastSGPT2_texts_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT2_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastSGPT2_texts_match_correct_total)\n",
    "LastSGPT2_extracted_text_match_indices, LastSGPT2_extracted_text_match_scores, LastSGPT2_extracted_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT2_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastSGPT2_extracted_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "\n",
    "LastSGPT2_texts_k1_match_indices, LastSGPT2_texts_k1_match_scores, LastSGPT2_texts_k1_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT2_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "# 打印结果\n",
    "# print(\"Top K 索引：\", indices)\n",
    "# print(\"Top K 分数：\", scores)\n",
    "print(\"正确匹配的条目数：\", LastSGPT2_texts_k1_match_correct_total)\n",
    "LastSGPT2_extracted_k1_text_match_indices, LastSGPT2_extracted_k1_text_match_scores, LastSGPT2_extracted_k1_text_match_correct_total = retrieve(sim_embeddings.pooler_output.cpu().detach(), LastSGPT2_et_embeddings.cpu().detach(), K=K)\n",
    "\n",
    "print(\"正确匹配的条目数：\", LastSGPT2_extracted_k1_text_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_texts_cls_NN=NN(embedding=LastSGPT2_embeddings)\n",
    "LastSGPT2_extracted_text_cls_NN=NN(embedding=LastSGPT2_et_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_texts_cls_NN_distance,LastSGPT2_texts_cls_NN_indes=LastSGPT2_texts_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())\n",
    "LastSGPT2_extracted_text_cls_NN_distance,LastSGPT2_extracted_text_cls_NN_indes=LastSGPT2_extracted_text_cls_NN.kneighbors(sim_embeddings.pooler_output.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastSGPT2_texts_match_indices.cpu().numpy(), LastSGPT2_texts_cls_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "overleap(LastSGPT2_extracted_text_match_indices.cpu().numpy(), LastSGPT2_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_cls_KM=KM(LastSGPT2_embeddings.cpu().numpy())\n",
    "LastSGPT2_extracted_text_cls_KM=KM(LastSGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_cls_KM_index=LastSGPT2_cls_KM.predict(LastSGPT2_embeddings.cpu().numpy())\n",
    "LastSGPT2_extracted_text_cls_KM_index=LastSGPT2_extracted_text_cls_KM.predict(LastSGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_cls_match_index=LastSGPT2_cls_KM.predict(LastSGPT2_embeddings[LastSGPT2_texts_k1_match_indices].reshape(LastSGPT2_embeddings.shape).cpu().numpy())\n",
    "LastSGPT2_et_cls_match_index=LastSGPT2_extracted_text_cls_KM.predict(LastSGPT2_et_embeddings[LastSGPT2_extracted_k1_text_match_indices].reshape(LastSGPT2_et_embeddings.shape).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2_x_cls=LastSGPT2_cls_match_index==LastSGPT2_cls_KM_index\n",
    "LastSGPT2_x_et_cls=LastSGPT2_extracted_text_cls_KM_index==LastSGPT2_et_cls_match_index\n",
    "\n",
    "np.sum(LastSGPT2_x_cls),np.sum(LastSGPT2_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(x_cls),np.sum(x_et_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_texts_cls_NN=NN(embedding=sim_embeddings.pooler_output)\n",
    "LastSGPT2SMIS_extracted_text_cls_NN=NN(embedding=sim_embeddings.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_texts_cls_NN_distance,LastSGPT2SMIS_texts_cls_NN_indes=LastSGPT2SMIS_texts_cls_NN.kneighbors(LastSGPT2_embeddings.cpu().numpy())\n",
    "LastSGPT2SMIS_extracted_text_cls_NN_distance,LastSGPT2SMIS_extracted_text_cls_NN_indes=LastSGPT2SMIS_extracted_text_cls_NN.kneighbors(LastSGPT2_et_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(LastSGPT2_texts_match_indices.cpu().numpy(), LastSGPT2SMIS_texts_cls_NN_indes)\n",
    "#_____________cross should get low score_____________\n",
    "overleap(LastSGPT2_extracted_text_match_indices.cpu().numpy(), LastSGPT2SMIS_extracted_text_cls_NN_indes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "LastSGPT2SMIS_extracted_text_cls_KM=KM(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_cls_KM_index=LastSGPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))\n",
    "LastSGPT2SMIS_extracted_text_cls_KM_index=LastSGPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output.cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_cls_match_index=LastSGPT2SMIS_cls_KM.predict(sim_embeddings.pooler_output[LastSGPT2_texts_k1_match_indices].reshape(LastSGPT2_embeddings.shape).cpu().numpy().astype(np.float32))\n",
    "LastSGPT2SMIS_et_cls_match_index=LastSGPT2SMIS_extracted_text_cls_KM.predict(sim_embeddings.pooler_output[LastSGPT2_extracted_k1_text_match_indices].reshape(LastSGPT2_et_embeddings.shape).cpu().numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastSGPT2SMIS_x_cls=LastSGPT2SMIS_cls_match_index==LastSGPT2SMIS_cls_KM_index\n",
    "LastSGPT2SMIS_x_et_cls=LastSGPT2SMIS_extracted_text_cls_KM_index==LastSGPT2SMIS_et_cls_match_index\n",
    "\n",
    "np.sum(LastSGPT2SMIS_x_cls),np.sum(LastSGPT2SMIS_x_et_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LastSGPT2_embeddings1,\n",
    "del LastSGPT2_embeddings2,\n",
    "del LastSGPT2_embeddings3,\n",
    "del LastSGPT2_embeddings4,\n",
    "del LastSGPT2_embeddings,\n",
    "del LastSGPT2_et_embeddings1,\n",
    "del LastSGPT2_et_embeddings2,\n",
    "del LastSGPT2_et_embeddings3,\n",
    "del LastSGPT2_et_embeddings4,\n",
    "del LastSGPT2_et_embeddings,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MolCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_embeddings=torch.load('gcn_embeddings.pt')\n",
    "# gcn_out_embeddings=torch.load('gcn_out_embeddings.pt')\n",
    "# gin_embeddings=torch.load('gin_embeddings.pt')\n",
    "# gin_out_embeddings=torch.load('gin_out_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcn_embeddings.shape,gcn_out_embeddings.shape,gin_embeddings.shape,gin_out_embeddings.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from MolCLR.models.ginet_molclr import GINet\n",
    "from MolCLR.models.gcn_molclr import GCN\n",
    "\n",
    "# Convert a SMILES string into a graph representation\n",
    "def smiles_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    Chem.SanitizeMol(mol)\n",
    "    \n",
    "    # Extract atom features and edge features\n",
    "    atom_features = torch.tensor([[atom.GetAtomicNum(), atom.GetChiralTag()] for atom in mol.GetAtoms()], dtype=torch.long)\n",
    "    bond_features = torch.tensor([[bond.GetBondTypeAsDouble(), bond.GetStereo()] for bond in mol.GetBonds()], dtype=torch.long)\n",
    "    \n",
    "    # Build edge_index (source, target) pairs for the molecular graph\n",
    "    edge_index = torch.tensor([[bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()] for bond in mol.GetBonds()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create a batch (for a single molecule, the batch index will be all 0s)\n",
    "    data = Data(x=atom_features, edge_index=edge_index, edge_attr=bond_features, batch=torch.tensor([0] * atom_features.size(0)))\n",
    "    return data\n",
    "\n",
    "# Load the GINet model based on the configuration from config.yaml\n",
    "def load_model(path='',type='gin'):\n",
    "    # Initialize the GINet model with the exact settings from config.yaml\n",
    "    if type=='gin':\n",
    "        model = GINet(num_layer=5, emb_dim=300, feat_dim=512, drop_ratio=0, pool='mean')\n",
    "    elif type=='gcn':\n",
    "        model=GCN(num_layer=5, emb_dim=300, feat_dim=512, drop_ratio=0, pool='mean')\n",
    "    \n",
    "    # Optionally load pre-trained weights from the \"pretrained_gin\" model\n",
    "    pretrained_model_path = path  # Ensure this path exists and is correct\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(pretrained_model_path,map_location='cuda:0'))\n",
    "        print(\"Loaded pre-trained model from:\", pretrained_model_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained weights not found, training from scratch.\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "\n",
    "gin_model = load_model(path='./../MolCLR/ckpt/pretrained_gzin/checkpoints/model.pth',type='gin')\n",
    "gcn_model = load_model(path='./../MolCLR/ckpt/pretrained_gcn/checkpoints/model.pth',type='gcn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_h_list=[]\n",
    "gcn_out_list=[]\n",
    "gin_h_list=[]\n",
    "gin_out_list=[]\n",
    "text_list=[]\n",
    "et_list=[]\n",
    "pass_list=[]\n",
    "compute_list=[]\n",
    "for index,i in enumerate(smis) :\n",
    "    # print(index)\n",
    "    # print(i)\n",
    "    graph_data = smiles_to_graph(i)\n",
    "    # print(graph_data)\n",
    "    try:\n",
    "        gin_h, gin_out = gin_model(graph_data)\n",
    "        gin_h_list.append(gin_h)\n",
    "        gin_out_list.append(gin_out)\n",
    "        gcn_h, gcn_out = gcn_model(graph_data)\n",
    "        gcn_h_list.append(gcn_h)\n",
    "        gcn_out_list.append(gcn_out)\n",
    "        text_list.append(texts[index])\n",
    "        et_list.append(extracted_text[index])\n",
    "        compute_list.append(index)\n",
    "    except IndexError:\n",
    "        # print(\"PASS:\",index)\n",
    "        pass_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(compute_list),len(pass_list),len(gin_h_list),len(gin_out_list),len(gcn_h_list),len(gcn_out_list),len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('compute_list.txt','w') as f:\n",
    "#     for i in compute_list:\n",
    "#         f.writelines(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pass_list.txt','w') as f:\n",
    "#     for i in pass_list:\n",
    "#         f.writelines(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_embeddings = torch.cat(gcn_h_list, dim=0)\n",
    "gcn_out_embeddings= torch.cat(gcn_out_list, dim=0)\n",
    "gin_embeddings = torch.cat(gin_h_list, dim=0)\n",
    "gin_out_embeddings= torch.cat(gin_out_list, dim=0)\n",
    "gcn_embeddings.shape,gin_embeddings.shape,gcn_out_embeddings.shape,gin_out_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCI-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "def SCIBERT(texts=texts):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embedding,mean_embedding=SCIBERT(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text_cls_embedding,extracted_text_mean_embedding=SCIBERT(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(mean_embedding[compute_list],gcn_embeddings),de_biased_cka(mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(mean_embedding[compute_list],gin_embeddings),de_biased_cka(mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del SCIBERT,\n",
    "del cls_embedding,\n",
    "del mean_embedding,\n",
    "del extracted_text_cls_embedding,\n",
    "del extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\").to(device)\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(text_list)\n",
    "print('embeddings',embeddings.shape)\n",
    "\n",
    "extracted_embeddings = model.encode(et_list)\n",
    "print('extracted_embeddings',extracted_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(torch.from_numpy(embeddings)[compute_list],gcn_embeddings),de_biased_cka(torch.from_numpy(extracted_embeddings)[compute_list],gcn_embeddings),de_biased_cka(torch.from_numpy(embeddings)[compute_list],gcn_out_embeddings),de_biased_cka(torch.from_numpy(extracted_embeddings)[compute_list],gcn_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(torch.from_numpy(embeddings)[compute_list],gin_embeddings),de_biased_cka(torch.from_numpy(extracted_embeddings)[compute_list],gin_embeddings),de_biased_cka(torch.from_numpy(embeddings)[compute_list],gin_out_embeddings),de_biased_cka(torch.from_numpy(extracted_embeddings)[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(torch.from_numpy(embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(embeddings)[compute_list],gcn_out_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gcn_out_embeddings))\n",
    "print(CKA(torch.from_numpy(embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(embeddings)[compute_list],gin_out_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gin_out_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(torch.from_numpy(embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gcn_embeddings),CKA(torch.from_numpy(embeddings)[compute_list],gcn_out_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gcn_out_embeddings))\n",
    "print(CKA(torch.from_numpy(embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gin_embeddings),CKA(torch.from_numpy(embeddings)[compute_list],gin_out_embeddings),CKA(torch.from_numpy(extracted_embeddings)[compute_list],gin_out_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "# GCN\n",
    "gcn_texts_match_indices, texts_match_scores, texts_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(embeddings).cpu().detach()[compute_list], K=10)\n",
    "gcn_extracted_match_indices, extracted_match_scores, extracted_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(extracted_embeddings).cpu().detach()[compute_list], K=10)\n",
    "\n",
    "print(texts_match_correct_total,extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "# GIN\n",
    "gin_texts_match_indices, texts_match_scores, texts_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(embeddings).cpu().detach()[compute_list], K=10)\n",
    "gin_extracted_match_indices, extracted_match_scores, extracted_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(extracted_embeddings).cpu().detach()[compute_list], K=10)\n",
    "\n",
    "print(texts_match_correct_total,extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "# GCN\n",
    "gcn_texts_k1_match_indices, texts_k1_match_scores, texts_k1_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(embeddings).cpu().detach()[compute_list], K=K)\n",
    "gcn_extracte_k1_match_indices, extracted_k1_match_scores, extracted_k1_match_correct_total = retrieve(gcn_embeddings.cpu(), torch.from_numpy(extracted_embeddings).cpu().detach()[compute_list], K=K)\n",
    "\n",
    "print(texts_k1_match_correct_total,extracted_k1_match_correct_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "# GIN\n",
    "gin_texts_k1_match_indices, texts_match_scores, texts_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(embeddings).cpu().detach()[compute_list], K=K)\n",
    "gin_extracted_k1_match_indices, extracted_match_scores, extracted_match_correct_total = retrieve(gin_embeddings.cpu(), torch.from_numpy(extracted_embeddings).cpu().detach()[compute_list], K=K)\n",
    "\n",
    "print(texts_match_correct_total,extracted_match_correct_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_mean_NN=NN(embedding=torch.from_numpy(embeddings)[compute_list])\n",
    "extracted_text_mean_NN=NN(embedding=torch.from_numpy(extracted_embeddings)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_distance,gcn_emb_index=texts_mean_NN.kneighbors(embeddings[compute_list])\n",
    "et_distance,gcn_et_index=extracted_text_mean_NN.kneighbors(extracted_embeddings[compute_list])\n",
    "\n",
    "emb_distance,gin_emb_index=texts_mean_NN.kneighbors(embeddings[compute_list])\n",
    "et_distance,gin_et_index=extracted_text_mean_NN.kneighbors(extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(gcn_texts_match_indices.cpu().numpy(), gcn_emb_index)\n",
    "\n",
    "overleap(gcn_extracted_match_indices.cpu().numpy(), gcn_et_index)\n",
    "\n",
    "overleap(gin_texts_match_indices.cpu().numpy(), gin_emb_index)\n",
    "\n",
    "overleap(gin_extracted_match_indices.cpu().numpy(), gin_et_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMILES NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_NN=NN(embedding=gcn_embeddings[compute_list])\n",
    "GIN_NN=NN(embedding=gin_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_distance,gcn_emb_index=GCN_NN.kneighbors(gcn_embeddings.detach().numpy()[compute_list])\n",
    "et_distance,gcn_et_index=GCN_NN.kneighbors(gcn_embeddings.detach().numpy()[compute_list])\n",
    "\n",
    "emb_distance,gin_emb_index=GIN_NN.kneighbors(gin_embeddings.detach().numpy()[compute_list])\n",
    "et_distance,gin_et_index=GIN_NN.kneighbors(gin_embeddings.detach().numpy()[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overleap(gcn_texts_match_indices.cpu().numpy(), gcn_emb_index)\n",
    "\n",
    "overleap(gcn_extracted_match_indices.cpu().numpy(), gcn_et_index)\n",
    "\n",
    "overleap(gin_texts_match_indices.cpu().numpy(), gin_emb_index)\n",
    "\n",
    "overleap(gin_extracted_match_indices.cpu().numpy(), gin_et_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_KM=KM(embeddings[compute_list])\n",
    "\n",
    "extracted_text_KM=KM(extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=cls_KM.predict(embeddings[compute_list])\n",
    "extracted_text_cls=extracted_text_KM.predict(extracted_embeddings[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index=cls_KM.predict(embeddings[gcn_texts_k1_match_indices].reshape(embeddings.shape)[compute_list])\n",
    "et_index=extracted_text_KM.predict(extracted_embeddings[gcn_extracte_k1_match_indices].reshape(extracted_embeddings.shape)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cls=cls_index==cls\n",
    "et=et_index==extracted_text_cls\n",
    "np.sum(x_cls),np.sum(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index=cls_KM.predict(embeddings[gin_texts_k1_match_indices].reshape(embeddings.shape)[compute_list])\n",
    "et_index=extracted_text_KM.predict(extracted_embeddings[gin_extracted_k1_match_indices].reshape(extracted_embeddings.shape)[compute_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cls=cls_index==cls\n",
    "et=et_index==extracted_text_cls\n",
    "np.sum(x_cls),np.sum(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMIS K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_KM=KM(gcn_embeddings.detach().numpy())\n",
    "\n",
    "extracted_text_KM=KM(gcn_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=cls_KM.predict(gcn_embeddings.detach().numpy())\n",
    "extracted_text_cls=extracted_text_KM.predict(gcn_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index=cls_KM.predict(gcn_embeddings[gcn_texts_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy() )\n",
    "et_index=extracted_text_KM.predict(gcn_embeddings[gcn_extracte_k1_match_indices].reshape(gcn_embeddings.shape).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cls=cls_index==cls\n",
    "et=et_index==extracted_text_cls\n",
    "np.sum(x_cls),np.sum(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_KM=KM(gin_embeddings.detach().numpy())\n",
    "\n",
    "extracted_text_KM=KM(gin_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=cls_KM.predict(gin_embeddings.detach().numpy())\n",
    "extracted_text_cls=extracted_text_KM.predict(gin_embeddings.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_index=cls_KM.predict(gin_embeddings[gin_texts_k1_match_indices].reshape(gin_embeddings.shape).detach().numpy())\n",
    "et_index=extracted_text_KM.predict(gin_embeddings[gin_extracted_k1_match_indices].reshape(gin_embeddings.shape).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cls=cls_index==cls\n",
    "et=et_index==extracted_text_cls\n",
    "np.sum(x_cls),np.sum(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings\n",
    "del extracted_embeddings\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "def pureBert(texts=texts):\n",
    "    # 检查是否有可用的 GPU\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cuda:1')\n",
    "    # 加载预训练的BERT模型和tokenizer\n",
    "    model_name = 'bert-base-uncased'  # 你可以使用其他的BERT模型，例如 'bert-large-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # 将模型转移到 GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 将句子 token 化，并将输入转移到 GPU\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # 转移到 GPU\n",
    "\n",
    "    # 获取BERT的输出\n",
    "    with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # BERT的最后隐藏层的输出 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 取 [CLS] token 的 embedding（句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding,mean_embedding=pureBert(texts)\n",
    "cls_embedding1,mean_embedding1=pureBert(texts[:1000])\n",
    "cls_embedding2,mean_embedding2=pureBert(texts[1000:2000])\n",
    "cls_embedding3,mean_embedding3=pureBert(texts[2000:3000])\n",
    "cls_embedding4,mean_embedding4=pureBert(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))\n",
    "mean_embedding=torch.cat((mean_embedding1,mean_embedding2,mean_embedding3,mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding,extracted_text_mean_embedding=pureBert(extracted_text)\n",
    "extracted_text_cls_embedding1,extracted_text_mean_embedding1=pureBert(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2,extracted_text_mean_embedding2=pureBert(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3,extracted_text_mean_embedding3=pureBert(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4,extracted_text_mean_embedding4=pureBert(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))\n",
    "extracted_text_mean_embedding=torch.cat((extracted_text_mean_embedding1,extracted_text_mean_embedding2,extracted_text_mean_embedding3,extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(mean_embedding[compute_list],gcn_embeddings),de_biased_cka(mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(mean_embedding[compute_list],gin_embeddings),de_biased_cka(mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_list),len(et_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,mean_embedding1,\n",
    "del cls_embedding2,mean_embedding2,\n",
    "del cls_embedding3,mean_embedding3,\n",
    "del cls_embedding4,mean_embedding4,\n",
    "del cls_embedding,\n",
    "del mean_embedding,\n",
    "del extracted_text_cls_embedding1,extracted_text_mean_embedding1,\n",
    "del extracted_text_cls_embedding2,extracted_text_mean_embedding2,\n",
    "del extracted_text_cls_embedding3,extracted_text_mean_embedding3,\n",
    "del extracted_text_cls_embedding4,extracted_text_mean_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "del extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "def get_embedding(texts=texts):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "    # Set the padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Run model to get hidden states\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding,mean_embedding=get_embedding(texts)\n",
    "cls_embedding1,mean_embedding1=get_embedding(texts[:1000])\n",
    "cls_embedding2,mean_embedding2=get_embedding(texts[1000:2000])\n",
    "cls_embedding3,mean_embedding3=get_embedding(texts[2000:3000])\n",
    "cls_embedding4,mean_embedding4=get_embedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))\n",
    "mean_embedding=torch.cat((mean_embedding1,mean_embedding2,mean_embedding3,mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embedding.shape,mean_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding,extracted_text_mean_embedding=get_embedding(extracted_text)\n",
    "extracted_text_cls_embedding1,extracted_text_mean_embedding1=get_embedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2,extracted_text_mean_embedding2=get_embedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3,extracted_text_mean_embedding3=get_embedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4,extracted_text_mean_embedding4=get_embedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))\n",
    "extracted_text_mean_embedding=torch.cat((extracted_text_mean_embedding1,extracted_text_mean_embedding2,extracted_text_mean_embedding3,extracted_text_mean_embedding4))\n",
    "extracted_text_cls_embedding.shape,extracted_text_mean_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(mean_embedding[compute_list],gcn_embeddings),de_biased_cka(mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(mean_embedding[compute_list],gin_embeddings),de_biased_cka(mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del mean_embedding1,\n",
    "del cls_embedding2,\n",
    "del mean_embedding2,\n",
    "del cls_embedding3,\n",
    "del mean_embedding3,\n",
    "del cls_embedding4,\n",
    "del mean_embedding4,\n",
    "del cls_embedding,\n",
    "del mean_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_mean_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_mean_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_mean_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_mean_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "del extracted_text_mean_embedding,\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def GPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding=GPTembedding(texts)\n",
    "cls_embedding1=GPTembedding(texts[:1000])\n",
    "cls_embedding2=GPTembedding(texts[1000:2000])\n",
    "cls_embedding3=GPTembedding(texts[2000:3000])\n",
    "cls_embedding4=GPTembedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding=GPTembedding(extracted_text)\n",
    "extracted_text_cls_embedding1=GPTembedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2=GPTembedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3=GPTembedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4=GPTembedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del cls_embedding2,\n",
    "del cls_embedding3,\n",
    "del cls_embedding4,\n",
    "del cls_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "\n",
    "def GPTembedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "    model = OpenAIGPTModel.from_pretrained(\"openai-community/openai-gpt\").to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, truncation=True,  max_length=512,return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding=GPTembedding(texts)\n",
    "cls_embedding1=GPTembedding(texts[:1000])\n",
    "cls_embedding2=GPTembedding(texts[1000:2000])\n",
    "cls_embedding3=GPTembedding(texts[2000:3000])\n",
    "cls_embedding4=GPTembedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding=GPTembedding(extracted_text)\n",
    "# extracted_text_cls_embedding=GPTembedding(extracted_text)\n",
    "extracted_text_cls_embedding1=GPTembedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2=GPTembedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3=GPTembedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4=GPTembedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del cls_embedding2,\n",
    "del cls_embedding3,\n",
    "del cls_embedding4,\n",
    "del cls_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "    input_ids = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "\n",
    "    # Get the hidden states (outputs from each layer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    # # Optionally, move the result back to the CPU if needed\n",
    "    # sentence_embedding = sentence_embedding.cpu().detach()\n",
    "\n",
    "    # print(sentence_embedding.shape)\n",
    "    # return sentence_embedding\n",
    "\n",
    "    # Access last hidden states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    print(last_hidden_states.shape)\n",
    "\n",
    "    # 获取最后的隐藏状态 [batch_size, seq_len, hidden_size]\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取第一个 token (<s> token) 的 embedding（相当于 CLS token 的句子嵌入）\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "    # 也可以取所有 token 的平均值作为句子嵌入\n",
    "    mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "    print(f\"CLS token embedding size: {cls_embedding.shape}\")\n",
    "    print(f\"Mean token embedding size: {mean_embedding.shape}\")\n",
    "    return cls_embedding,mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding,mean_embedding=GPT2embedding(texts)\n",
    "cls_embedding1,mean_embedding1=GPT2embedding(texts[:1000])\n",
    "cls_embedding2,mean_embedding2=GPT2embedding(texts[1000:2000])\n",
    "cls_embedding3,mean_embedding3=GPT2embedding(texts[2000:3000])\n",
    "cls_embedding4,mean_embedding4=GPT2embedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))\n",
    "mean_embedding=torch.cat((mean_embedding1,mean_embedding2,mean_embedding3,mean_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding,extracted_text_mean_embedding=GPT2embedding(extracted_text)\n",
    "extracted_text_cls_embedding1,extracted_text_mean_embedding1=GPT2embedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2,extracted_text_mean_embedding2=GPT2embedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3,extracted_text_mean_embedding3=GPT2embedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4,extracted_text_mean_embedding4=GPT2embedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))\n",
    "extracted_text_mean_embedding=torch.cat((extracted_text_mean_embedding1,extracted_text_mean_embedding2,extracted_text_mean_embedding3,extracted_text_mean_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(mean_embedding[compute_list],gcn_embeddings),de_biased_cka(mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(mean_embedding[compute_list],gin_embeddings),de_biased_cka(mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_mean_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_mean_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(mean_embedding[compute_list],gcn_embeddings.to(device)),CKA(mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_embeddings.to(device)),CKA(mean_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_mean_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_mean_embedding[compute_list],gin_out_embeddings.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del mean_embedding1,\n",
    "del cls_embedding2,\n",
    "del mean_embedding2,\n",
    "del cls_embedding3,\n",
    "del mean_embedding3,\n",
    "del cls_embedding4,\n",
    "del mean_embedding4,\n",
    "del cls_embedding,\n",
    "del mean_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_mean_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_mean_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_mean_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_mean_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "del extracted_text_mean_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last word GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_means_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_means_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding=GPT2embedding(texts)\n",
    "cls_embedding1=GPT2embedding(texts[:1000])\n",
    "cls_embedding2=GPT2embedding(texts[1000:2000])\n",
    "cls_embedding3=GPT2embedding(texts[2000:3000])\n",
    "cls_embedding4=GPT2embedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding=GPT2embedding(extracted_text)\n",
    "extracted_text_cls_embedding1=GPT2embedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2=GPT2embedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3=GPT2embedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4=GPT2embedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del cls_embedding2,\n",
    "del cls_embedding3,\n",
    "del cls_embedding4,\n",
    "del cls_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise last word GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def GPT2embedding(texts):\n",
    "\n",
    "    # Check if GPU is available and set the device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load pre-trained model and tokenizer and move the model to the GPU\n",
    "    model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    # Input sentences\n",
    "    # texts = ['Your sentence here1', 'Your sentence here2']\n",
    "\n",
    "    # Tokenize input sentence and move input tensors to the GPU\n",
    "\n",
    "    template = 'This_sentence_:_\"The_molecule_is_a_nitrile_that_is_acetonitrile_where_one_of_the_methyl_hydrogens_is_substituted_by_a_2-methylphenyl_group.\"_summarise_in_one_word:\"Acetonitrile\".This_sentence_:_\"*sent_0*\"_summarise_in_one_word:\"'\n",
    "    inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors=\"pt\")['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]\n",
    "    print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_embedding=GPT2embedding(texts)\n",
    "cls_embedding1=GPT2embedding(texts[:1000])\n",
    "cls_embedding2=GPT2embedding(texts[1000:2000])\n",
    "cls_embedding3=GPT2embedding(texts[2000:3000])\n",
    "cls_embedding4=GPT2embedding(texts[3000:])\n",
    "\n",
    "cls_embedding=torch.cat((cls_embedding1,cls_embedding2,cls_embedding3,cls_embedding4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text_cls_embedding=GPT2embedding(extracted_text)\n",
    "extracted_text_cls_embedding1=GPT2embedding(extracted_text[:1000])\n",
    "extracted_text_cls_embedding2=GPT2embedding(extracted_text[1000:2000])\n",
    "extracted_text_cls_embedding3=GPT2embedding(extracted_text[2000:3000])\n",
    "extracted_text_cls_embedding4=GPT2embedding(extracted_text[3000:])\n",
    "\n",
    "extracted_text_cls_embedding=torch.cat((extracted_text_cls_embedding1,extracted_text_cls_embedding2,extracted_text_cls_embedding3,extracted_text_cls_embedding4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(cls_embedding[compute_list], gcn_embeddings),de_biased_cka(cls_embedding[compute_list], gcn_out_embeddings),de_biased_cka(cls_embedding[compute_list],gin_embeddings),de_biased_cka(cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_biased_cka(extracted_text_cls_embedding[compute_list], gcn_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gcn_out_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_embeddings),de_biased_cka(extracted_text_cls_embedding[compute_list],gin_out_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CKA(cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(cls_embedding[compute_list], gcn_out_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_embeddings.to(device)),CKA(cls_embedding[compute_list],gin_out_embeddings.to(device)))\n",
    "print(CKA(extracted_text_cls_embedding[compute_list], gcn_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gcn_out_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_embeddings.to(device)),CKA(extracted_text_cls_embedding[compute_list],gin_out_embeddings.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cls_embedding1,\n",
    "del cls_embedding2,\n",
    "del cls_embedding3,\n",
    "del cls_embedding4,\n",
    "del cls_embedding,\n",
    "del extracted_text_cls_embedding1,\n",
    "del extracted_text_cls_embedding2,\n",
    "del extracted_text_cls_embedding3,\n",
    "del extracted_text_cls_embedding4,\n",
    "del extracted_text_cls_embedding,\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenBioMed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
